{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ViMPaSuC6Cht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os \n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.parallel import DataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzDFqyLS6Chz"
   },
   "source": [
    "### Change the path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L-WVJtbi6Ch1"
   },
   "outputs": [],
   "source": [
    "training_filepath = '../../Data/BioASQ_data/BioASQ-training7b/trainining7b.json'\n",
    "test_directory = '../../Data/BioASQ_data/Task7BGoldenEnriched/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oAkv4Hzo6Ch2"
   },
   "outputs": [],
   "source": [
    "training_text = []\n",
    "test_text = []\n",
    "\n",
    "# Filter out the training data\n",
    "with open (training_filepath, \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "    texts = data['questions']\n",
    "    for text in texts:\n",
    "        if 'exact_answer' in text.keys():\n",
    "            if text['exact_answer'] == 'yes' or text['exact_answer'] == 'no':\n",
    "                training_text.append(text)\n",
    "\n",
    "# Filter out the text data\n",
    "directory = test_directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open (directory+filename, \"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        texts = data['questions']\n",
    "        for text in texts:\n",
    "            if 'exact_answer' in text.keys():\n",
    "                if text['exact_answer'] == 'yes' or text['exact_answer'] == 'no':\n",
    "                    test_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R2Sk43KZ6Ch3"
   },
   "outputs": [],
   "source": [
    "def process_data(texts):\n",
    "    question_list = []\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    for text in texts:\n",
    "        question_list.append(text['body'])\n",
    "        context_list.append(' '. join([x['text'] for x in text['snippets']]))\n",
    "        target_list.append(1 if text['exact_answer'] == 'yes' else 0)\n",
    "    df = pd.DataFrame(zip(question_list, context_list, target_list), columns=['question', 'context', 'target'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R2m6PFBk6Ch6"
   },
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(model_path)\n",
    "        self.out = nn.Linear(768, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self, question, context, target, tokenizer, max_len):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.target = target\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        question= str(self.question[item])\n",
    "        context = str(self.context[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        padding_len = self.max_len - len(ids)\n",
    "        \n",
    "        ids = ids[:self.max_len] + ([0] * padding_len) \n",
    "        token_type_ids = token_type_ids[:self.max_len] + ([0] * padding_len)\n",
    "        mask = mask[:self.max_len] + ([0] * padding_len)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.target[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def loss_fn(outputs, target):\n",
    "    return nn.CrossEntropyLoss()(outputs, target)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        target = d['target']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        target = target.to(device, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 20 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            \n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            ids = d['ids'].to(device, dtype=torch.long)\n",
    "            mask = d['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "            target = d['target'].to(device, dtype=torch.long)\n",
    "          \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "          \n",
    "            fin_targets.append(target.cpu().detach().numpy())\n",
    "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.hstack(fin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_8GB_model-trained-0-7063\n",
      "run_8GB_model-trained-8-63567\n",
      "run_8GB_model-trained-22-162449\n"
     ]
    }
   ],
   "source": [
    "for path in model_paths:\n",
    "    try:\n",
    "        print(path.split('/')[-2].split('.')[0])\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A56vzcaR6Ch8",
    "outputId": "48204d53-3848-4e25-9374-778cfaa95364"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.715508759021759\n",
      "bi=20, loss=0.3137582838535309\n",
      "bi=40, loss=0.43840402364730835\n",
      "bi=0, loss=0.6257165670394897\n",
      "bi=20, loss=0.43836620450019836\n",
      "bi=40, loss=0.625725507736206\n",
      "bi=0, loss=0.5632762908935547\n",
      "bi=20, loss=0.43833428621292114\n",
      "bi=40, loss=0.5632702708244324\n",
      "bi=0, loss=0.5008020401000977\n",
      "bi=20, loss=0.5007956624031067\n",
      "bi=40, loss=0.6257413625717163\n",
      "bi=0, loss=0.5007944107055664\n",
      "bi=20, loss=0.5007848739624023\n",
      "bi=40, loss=0.5632685422897339\n",
      "bi=0, loss=0.6257481575012207\n",
      "bi=20, loss=0.3758203983306885\n",
      "bi=40, loss=0.5632649064064026\n",
      "bi=0, loss=0.5007869601249695\n",
      "bi=20, loss=0.5007845163345337\n",
      "bi=40, loss=0.5007820129394531\n",
      "bi=0, loss=0.43829625844955444\n",
      "bi=20, loss=0.43830206990242004\n",
      "bi=40, loss=0.5632651448249817\n",
      "Final Accuracy for Round 1: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7130880355834961\n",
      "bi=20, loss=0.5009768605232239\n",
      "bi=40, loss=0.6256934404373169\n",
      "bi=0, loss=0.4384469985961914\n",
      "bi=20, loss=0.3759791851043701\n",
      "bi=40, loss=0.5008022785186768\n",
      "bi=0, loss=0.3759301006793976\n",
      "bi=20, loss=0.4383658468723297\n",
      "bi=40, loss=0.5008127689361572\n",
      "bi=0, loss=0.5632714629173279\n",
      "bi=20, loss=0.43834391236305237\n",
      "bi=40, loss=0.5632756352424622\n",
      "bi=0, loss=0.5632696747779846\n",
      "bi=20, loss=0.43832123279571533\n",
      "bi=40, loss=0.37585702538490295\n",
      "bi=0, loss=0.3133784830570221\n",
      "bi=20, loss=0.5007981061935425\n",
      "bi=40, loss=0.3758460283279419\n",
      "bi=0, loss=0.6882193684577942\n",
      "bi=20, loss=0.43831732869148254\n",
      "bi=40, loss=0.3758370578289032\n",
      "bi=0, loss=0.5007929801940918\n",
      "bi=20, loss=0.5007844567298889\n",
      "bi=40, loss=0.4383183717727661\n",
      "Final Accuracy for Round 2: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6897773146629333\n",
      "bi=20, loss=0.43863800168037415\n",
      "bi=40, loss=0.5008514523506165\n",
      "bi=0, loss=0.37593331933021545\n",
      "bi=20, loss=0.3758949637413025\n",
      "bi=40, loss=0.5008049607276917\n",
      "bi=0, loss=0.5008039474487305\n",
      "bi=20, loss=0.43832889199256897\n",
      "bi=40, loss=0.31337466835975647\n",
      "bi=0, loss=0.5632663369178772\n",
      "bi=20, loss=0.500789225101471\n",
      "bi=40, loss=0.4383050203323364\n",
      "bi=0, loss=0.31334930658340454\n",
      "bi=20, loss=0.5007799863815308\n",
      "bi=40, loss=0.7507157921791077\n",
      "bi=0, loss=0.3758212625980377\n",
      "bi=20, loss=0.5007763504981995\n",
      "bi=40, loss=0.6257506608963013\n",
      "bi=0, loss=0.3758150637149811\n",
      "bi=20, loss=0.625751256942749\n",
      "bi=40, loss=0.5007811188697815\n",
      "bi=0, loss=0.4382968842983246\n",
      "bi=20, loss=0.500778079032898\n",
      "bi=40, loss=0.4382920265197754\n",
      "Final Accuracy for Round 3: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7623088955879211\n",
      "bi=20, loss=0.37607282400131226\n",
      "bi=40, loss=0.5632805228233337\n",
      "bi=0, loss=0.4383757412433624\n",
      "bi=20, loss=0.5632743239402771\n",
      "bi=40, loss=0.5007973313331604\n",
      "bi=0, loss=0.37584778666496277\n",
      "bi=20, loss=0.500794529914856\n",
      "bi=40, loss=0.750708281993866\n",
      "bi=0, loss=0.6257479786872864\n",
      "bi=20, loss=0.31333762407302856\n",
      "bi=40, loss=0.3758159577846527\n",
      "bi=0, loss=0.6257517337799072\n",
      "bi=20, loss=0.4382968842983246\n",
      "bi=40, loss=0.4382939636707306\n",
      "bi=0, loss=0.5007811784744263\n",
      "bi=20, loss=0.5632664561271667\n",
      "bi=40, loss=0.5007774233818054\n",
      "bi=0, loss=0.5007784962654114\n",
      "bi=20, loss=0.5632658004760742\n",
      "bi=40, loss=0.4382913112640381\n",
      "bi=0, loss=0.5632657408714294\n",
      "bi=20, loss=0.37580373883247375\n",
      "bi=40, loss=0.6257514953613281\n",
      "Final Accuracy for Round 4: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-0-7063/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7159106731414795\n",
      "bi=20, loss=0.5010585188865662\n",
      "bi=40, loss=0.43852803111076355\n",
      "bi=0, loss=0.5008797645568848\n",
      "bi=20, loss=0.37600377202033997\n",
      "bi=40, loss=0.4383910894393921\n",
      "bi=0, loss=0.31349053978919983\n",
      "bi=20, loss=0.6257275938987732\n",
      "bi=40, loss=0.5008185505867004\n",
      "bi=0, loss=0.5008072853088379\n",
      "bi=20, loss=0.5008049011230469\n",
      "bi=40, loss=0.5632730722427368\n",
      "bi=0, loss=0.6257365942001343\n",
      "bi=20, loss=0.4383322596549988\n",
      "bi=40, loss=0.5632712244987488\n",
      "bi=0, loss=0.6257320642471313\n",
      "bi=20, loss=0.500796914100647\n",
      "bi=40, loss=0.5632730722427368\n",
      "bi=0, loss=0.5007941722869873\n",
      "bi=20, loss=0.5007693767547607\n",
      "bi=40, loss=0.5632704496383667\n",
      "bi=0, loss=0.4383139908313751\n",
      "bi=20, loss=0.3758389949798584\n",
      "bi=40, loss=0.625746488571167\n",
      "Final Accuracy for Round 5: 0.6714\n",
      "\n",
      "---------------------\n",
      "Logging model stats....\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7214868664741516\n",
      "bi=20, loss=0.4390156865119934\n",
      "bi=40, loss=0.5008606910705566\n",
      "bi=0, loss=0.43833643198013306\n",
      "bi=20, loss=0.43842020630836487\n",
      "bi=40, loss=0.5631966590881348\n",
      "bi=0, loss=0.6255940794944763\n",
      "bi=20, loss=0.50081866979599\n",
      "bi=40, loss=0.37591925263404846\n",
      "bi=0, loss=0.6257746815681458\n",
      "bi=20, loss=0.3758966028690338\n",
      "bi=40, loss=0.7505738139152527\n",
      "bi=0, loss=0.5631365776062012\n",
      "bi=20, loss=0.37595218420028687\n",
      "bi=40, loss=0.6882184743881226\n",
      "bi=0, loss=0.3134373426437378\n",
      "bi=20, loss=0.4383319020271301\n",
      "bi=40, loss=0.5632245540618896\n",
      "bi=0, loss=0.5008074045181274\n",
      "bi=20, loss=0.3758785128593445\n",
      "bi=40, loss=0.6253570914268494\n",
      "bi=0, loss=0.5632641315460205\n",
      "bi=20, loss=0.5007962584495544\n",
      "bi=40, loss=0.5007917881011963\n",
      "Final Accuracy for Round 1: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6988315582275391\n",
      "bi=20, loss=0.563424825668335\n",
      "bi=40, loss=0.688101053237915\n",
      "bi=0, loss=0.4384562075138092\n",
      "bi=20, loss=0.37604573369026184\n",
      "bi=40, loss=0.8130662441253662\n",
      "bi=0, loss=0.6257133483886719\n",
      "bi=20, loss=0.5008403062820435\n",
      "bi=40, loss=0.5632782578468323\n",
      "bi=0, loss=0.500827968120575\n",
      "bi=20, loss=0.4383563995361328\n",
      "bi=40, loss=0.6257315874099731\n",
      "bi=0, loss=0.6882010698318481\n",
      "bi=20, loss=0.43828850984573364\n",
      "bi=40, loss=0.43833234906196594\n",
      "bi=0, loss=0.3758920729160309\n",
      "bi=20, loss=0.31339558959007263\n",
      "bi=40, loss=0.4383356273174286\n",
      "bi=0, loss=0.5632814168930054\n",
      "bi=20, loss=0.5007321834564209\n",
      "bi=40, loss=0.4383406639099121\n",
      "bi=0, loss=0.31339937448501587\n",
      "bi=20, loss=0.43835484981536865\n",
      "bi=40, loss=0.3757883608341217\n",
      "Final Accuracy for Round 2: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6088639497756958\n",
      "bi=20, loss=0.3768012225627899\n",
      "bi=40, loss=0.5009221434593201\n",
      "bi=0, loss=0.43853166699409485\n",
      "bi=20, loss=0.5008024573326111\n",
      "bi=40, loss=0.5008887052536011\n",
      "bi=0, loss=0.6880475878715515\n",
      "bi=20, loss=0.438418447971344\n",
      "bi=40, loss=0.43841466307640076\n",
      "bi=0, loss=0.6257168650627136\n",
      "bi=20, loss=0.5008284449577332\n",
      "bi=40, loss=0.43831878900527954\n",
      "bi=0, loss=0.3759237229824066\n",
      "bi=20, loss=0.438292920589447\n",
      "bi=40, loss=0.563144862651825\n",
      "bi=0, loss=0.5008986592292786\n",
      "bi=20, loss=0.5632185935974121\n",
      "bi=40, loss=0.5008043646812439\n",
      "bi=0, loss=0.5632370710372925\n",
      "bi=20, loss=0.5007266402244568\n",
      "bi=40, loss=0.4383649230003357\n",
      "bi=0, loss=0.5008116960525513\n",
      "bi=20, loss=0.500824511051178\n",
      "bi=40, loss=0.3758912682533264\n",
      "Final Accuracy for Round 3: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6559893488883972\n",
      "bi=20, loss=0.5009816288948059\n",
      "bi=40, loss=0.4384993314743042\n",
      "bi=0, loss=0.4384741187095642\n",
      "bi=20, loss=0.43842440843582153\n",
      "bi=40, loss=0.37594345211982727\n",
      "bi=0, loss=0.4383932054042816\n",
      "bi=20, loss=0.5632808804512024\n",
      "bi=40, loss=0.43835875391960144\n",
      "bi=0, loss=0.37590593099594116\n",
      "bi=20, loss=0.6257306933403015\n",
      "bi=40, loss=0.5008207559585571\n",
      "bi=0, loss=0.6882166266441345\n",
      "bi=20, loss=0.3758782744407654\n",
      "bi=40, loss=0.43833643198013306\n",
      "bi=0, loss=0.3133945167064667\n",
      "bi=20, loss=0.6257383823394775\n",
      "bi=40, loss=0.5632663369178772\n",
      "bi=0, loss=0.3758598864078522\n",
      "bi=20, loss=0.5632699728012085\n",
      "bi=40, loss=0.3133797347545624\n",
      "bi=0, loss=0.6257155537605286\n",
      "bi=20, loss=0.43832385540008545\n",
      "bi=40, loss=0.6257337331771851\n",
      "Final Accuracy for Round 4: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-8-63567/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6916256546974182\n",
      "bi=20, loss=0.3142433166503906\n",
      "bi=40, loss=0.37620165944099426\n",
      "bi=0, loss=0.31368204951286316\n",
      "bi=20, loss=0.4384147524833679\n",
      "bi=40, loss=0.37596842646598816\n",
      "bi=0, loss=0.43841707706451416\n",
      "bi=20, loss=0.4383704662322998\n",
      "bi=40, loss=0.5008276104927063\n",
      "bi=0, loss=0.8130616545677185\n",
      "bi=20, loss=0.6257811188697815\n",
      "bi=40, loss=0.5008139610290527\n",
      "bi=0, loss=0.5007957816123962\n",
      "bi=20, loss=0.43835994601249695\n",
      "bi=40, loss=0.5008010864257812\n",
      "bi=0, loss=0.4383760094642639\n",
      "bi=20, loss=0.3758564591407776\n",
      "bi=40, loss=0.5632628202438354\n",
      "bi=0, loss=0.5007989406585693\n",
      "bi=20, loss=0.4383184611797333\n",
      "bi=40, loss=0.5632226467132568\n",
      "bi=0, loss=0.3758527636528015\n",
      "bi=20, loss=0.5008055567741394\n",
      "bi=40, loss=0.3758486807346344\n",
      "Final Accuracy for Round 5: 0.6714\n",
      "\n",
      "---------------------\n",
      "Logging model stats....\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.704915463924408\n",
      "bi=20, loss=0.439016729593277\n",
      "bi=40, loss=0.5633245706558228\n",
      "bi=0, loss=0.5009728074073792\n",
      "bi=20, loss=0.5009222626686096\n",
      "bi=40, loss=0.4384104311466217\n",
      "bi=0, loss=0.5632842183113098\n",
      "bi=20, loss=0.43836671113967896\n",
      "bi=40, loss=0.7505554556846619\n",
      "bi=0, loss=0.563163161277771\n",
      "bi=20, loss=0.625767707824707\n",
      "bi=40, loss=0.500857949256897\n",
      "bi=0, loss=0.3759307563304901\n",
      "bi=20, loss=0.5631797313690186\n",
      "bi=40, loss=0.5008669495582581\n",
      "bi=0, loss=0.6256673336029053\n",
      "bi=20, loss=0.4382793605327606\n",
      "bi=40, loss=0.4384063184261322\n",
      "bi=0, loss=0.3134635090827942\n",
      "bi=20, loss=0.500739574432373\n",
      "bi=40, loss=0.3759142756462097\n",
      "bi=0, loss=0.5006908178329468\n",
      "bi=20, loss=0.6257304549217224\n",
      "bi=40, loss=0.4383482038974762\n",
      "Final Accuracy for Round 1: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7105241417884827\n",
      "bi=20, loss=0.5011458396911621\n",
      "bi=40, loss=0.5009334683418274\n",
      "bi=0, loss=0.7504209876060486\n",
      "bi=20, loss=0.5633223652839661\n",
      "bi=40, loss=0.5008732676506042\n",
      "bi=0, loss=0.4385031759738922\n",
      "bi=20, loss=0.3135209381580353\n",
      "bi=40, loss=0.43838369846343994\n",
      "bi=0, loss=0.3759423792362213\n",
      "bi=20, loss=0.5007381439208984\n",
      "bi=40, loss=0.5632721781730652\n",
      "bi=0, loss=0.43835610151290894\n",
      "bi=20, loss=0.5008310675621033\n",
      "bi=40, loss=0.5007939338684082\n",
      "bi=0, loss=0.4383661150932312\n",
      "bi=20, loss=0.5008054375648499\n",
      "bi=40, loss=0.6257391571998596\n",
      "bi=0, loss=0.5008033514022827\n",
      "bi=20, loss=0.5007964968681335\n",
      "bi=40, loss=0.5632708668708801\n",
      "bi=0, loss=0.6882104277610779\n",
      "bi=20, loss=0.3758784234523773\n",
      "bi=40, loss=0.4383329451084137\n",
      "Final Accuracy for Round 2: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7555574774742126\n",
      "bi=20, loss=0.37812867760658264\n",
      "bi=40, loss=0.37665167450904846\n",
      "bi=0, loss=0.43889978528022766\n",
      "bi=20, loss=0.31388285756111145\n",
      "bi=40, loss=0.4385802149772644\n",
      "bi=0, loss=0.31378689408302307\n",
      "bi=20, loss=0.5009547472000122\n",
      "bi=40, loss=0.5632438063621521\n",
      "bi=0, loss=0.4382820129394531\n",
      "bi=20, loss=0.5008319616317749\n",
      "bi=40, loss=0.6257015466690063\n",
      "bi=0, loss=0.3759329319000244\n",
      "bi=20, loss=0.37595465779304504\n",
      "bi=40, loss=0.5633894205093384\n",
      "bi=0, loss=0.438289076089859\n",
      "bi=20, loss=0.3751663863658905\n",
      "bi=40, loss=0.4383544921875\n",
      "bi=0, loss=0.3811506927013397\n",
      "bi=20, loss=0.5319992899894714\n",
      "bi=40, loss=0.41580817103385925\n",
      "bi=0, loss=0.5009181499481201\n",
      "bi=20, loss=0.6666535139083862\n",
      "bi=40, loss=0.5196933150291443\n",
      "Final Accuracy for Round 3: 0.7357\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6158610582351685\n",
      "bi=20, loss=0.43901312351226807\n",
      "bi=40, loss=0.5009333491325378\n",
      "bi=0, loss=0.5004693269729614\n",
      "bi=20, loss=0.6257903575897217\n",
      "bi=40, loss=0.5632901787757874\n",
      "bi=0, loss=0.5007635354995728\n",
      "bi=20, loss=0.4384327530860901\n",
      "bi=40, loss=0.37599119544029236\n",
      "bi=0, loss=0.5008662939071655\n",
      "bi=20, loss=0.438281387090683\n",
      "bi=40, loss=0.5006917119026184\n",
      "bi=0, loss=0.43825563788414\n",
      "bi=20, loss=0.5630446672439575\n",
      "bi=40, loss=0.4383472800254822\n",
      "bi=0, loss=0.5632711052894592\n",
      "bi=20, loss=0.3758915662765503\n",
      "bi=40, loss=0.5008163452148438\n",
      "bi=0, loss=0.6257544159889221\n",
      "bi=20, loss=0.5008159875869751\n",
      "bi=40, loss=0.37587666511535645\n",
      "bi=0, loss=0.500870943069458\n",
      "bi=20, loss=0.375882089138031\n",
      "bi=40, loss=0.5008062124252319\n",
      "Final Accuracy for Round 4: 0.6714\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/8GB-checkpoints/run_8GB_model-trained-22-162449/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6855602264404297\n",
      "bi=20, loss=0.4389772117137909\n",
      "bi=40, loss=0.6880782842636108\n",
      "bi=0, loss=0.43871018290519714\n",
      "bi=20, loss=0.43851858377456665\n",
      "bi=40, loss=0.3136983811855316\n",
      "bi=0, loss=0.43845421075820923\n",
      "bi=20, loss=0.37621986865997314\n",
      "bi=40, loss=0.5008324384689331\n",
      "bi=0, loss=0.37800633907318115\n",
      "bi=20, loss=0.35351115465164185\n",
      "bi=40, loss=0.5044620037078857\n",
      "bi=0, loss=0.43866071105003357\n",
      "bi=20, loss=0.5009447932243347\n",
      "bi=40, loss=0.6228271722793579\n",
      "bi=0, loss=0.43299248814582825\n",
      "bi=20, loss=0.563065230846405\n",
      "bi=40, loss=0.4390726089477539\n",
      "bi=0, loss=0.563274621963501\n",
      "bi=20, loss=0.43844789266586304\n",
      "bi=40, loss=0.5008668303489685\n",
      "bi=0, loss=0.4383482038974762\n",
      "bi=20, loss=0.5011698603630066\n",
      "bi=40, loss=0.4106025695800781\n",
      "Final Accuracy for Round 5: 0.6714\n",
      "\n",
      "---------------------\n",
      "Logging model stats....\n",
      "\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 4\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 3e-5\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for model_path in model_paths:\n",
    "    tokenizer_path = ('bert-base-uncased' if model_path == 'bert-base-uncased'\\\n",
    "                      else '../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "    model_name = model_path if model_path == 'bert-base-uncased' else model_path.split('/')[-2].split('.')[0]\n",
    "    scores = []\n",
    "    model_stats = {'model_name':model_name,\n",
    "                   'seeds':[],\n",
    "                   'batch_size':TRAIN_BATCH_SIZE,\n",
    "                   'epochs':EPOCHS,\n",
    "                   'metric':'accuracy',\n",
    "                   'scores': [],\n",
    "                   'mean_score':0\n",
    "                    }\n",
    "    for num, seed in enumerate([42,43,44,45,46], 1):\n",
    "      \n",
    "        tokenizer = transformers.BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        model = BERTBaseUncased(model_path)\n",
    "        model = DataParallel(model)\n",
    "        model.to(device)\n",
    "\n",
    "        train_df = process_data(training_text)\n",
    "        test_df = process_data(test_text)\n",
    "\n",
    "        train_dataset = BERTDatasetTraining(\n",
    "            question=train_df.question.values,\n",
    "            context=train_df.context.values,\n",
    "            target=train_df.target.values,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=MAX_LEN\n",
    "        )\n",
    "        train_data_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        test_dataset = BERTDatasetTraining(\n",
    "            question=test_df.question.values,\n",
    "            context=test_df.context.values,\n",
    "            target=test_df.target.values,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=MAX_LEN\n",
    "        )\n",
    "        test_data_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=TEST_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        num_training_steps = int((len(train_dataset) / TRAIN_BATCH_SIZE) * EPOCHS)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "            \n",
    "        output, target = eval_loop_fn(test_data_loader, model, device)\n",
    "        acc = (output.argmax(1) == target).sum() / len(target)\n",
    "        print(f'Final Accuracy for Round {num}: {round(acc,4)}')\n",
    "        model_stats['scores'].append(round(acc, 6))\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(3)\n",
    "        print()\n",
    "        print('---------------------')\n",
    "    print('Logging model stats....')\n",
    "    print()\n",
    "    final_score = np.round(np.mean(model_stats['scores']), 4)\n",
    "    model_stats['mean_score'] = final_score\n",
    "    with open('logs/stats.txt', 'a') as f:\n",
    "        f.write(json.dumps(model_stats))\n",
    "        f.write('\\n')\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1sImHvP6-fmM"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning_QA_BioASQ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
