{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62030896-2adb-4358-90d2-f8868b541d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import torch\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#tokenizers and datasets\n",
    "import tokenizers\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import BertTokenizer\n",
    "from whole_word_masking import create_masked_lm_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af5d0f-2b5e-43ab-999b-fbd962d0581c",
   "metadata": {},
   "source": [
    "#### Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca09c24-e880-4af1-8a00-5e95624fee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xactest', 'xac', 'xab', 'xad', 'xaatest', 'xaa', 'xabtest']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_tok_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "vm_data = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/text/partials/'\n",
    "checkpoint_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/'\n",
    "files = [f for f in os.listdir(vm_data) if os.path.isfile(os.path.join(vm_data, f))]\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c7f68-7413-47b4-be37-b4bf0cfbf0f2",
   "metadata": {},
   "source": [
    "#### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55f7d5d-afa2-413d-97ba-8d6eeb507a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c6f2148-2a9f-4c2b-9f55-66855d60f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_seq_512(os.path.join(vm_data, 'xaatest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75e41d00-5381-496b-a553-fe11bb968269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b84e09-aa59-4af7-865d-1d5fb78c31f6",
   "metadata": {},
   "source": [
    "#### Load tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ecf5184-9eb6-43f9-a27a-f9aacded92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_from_file(vocab_path: str) -> BertWordPieceTokenizer:\n",
    "    tokenizer = BertWordPieceTokenizer(vocab_path, strip_accents=True, lowercase=True)\n",
    "    tokenizer.enable_truncation(max_length=512)\n",
    "    tokenizer.enable_padding()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "            (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "        ],\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "004a35b9-6a08-4292-bcbc-ce38c6625f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer_from_file('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a556b1-56ad-434f-a73c-389a8d481ae7",
   "metadata": {},
   "source": [
    "#### Batch encode raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3b79067-bdf4-433c-91bf-4981ef2f87f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(data)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99438542-073d-4bfd-a73c-e24abdbea137",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = batch[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3840947-2c1b-4ab8-9e14-0a65b500c972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'introduction',\n",
       " 'under',\n",
       " 'normal',\n",
       " 'physiological',\n",
       " 'conditions',\n",
       " ',',\n",
       " 'all',\n",
       " 'cells',\n",
       " 'in',\n",
       " 'the',\n",
       " 'body',\n",
       " 'are',\n",
       " 'exposed',\n",
       " 'chronically',\n",
       " 'to',\n",
       " 'oxidants',\n",
       " 'from',\n",
       " 'both',\n",
       " 'endogenous',\n",
       " 'and',\n",
       " 'exogenous',\n",
       " 'sources',\n",
       " ';',\n",
       " 'yet',\n",
       " 'the',\n",
       " 'intracellular',\n",
       " '“',\n",
       " 'redox',\n",
       " 'buffer',\n",
       " '”',\n",
       " 'mechanism',\n",
       " 'provides',\n",
       " 'significant',\n",
       " 'protection',\n",
       " 'mainly',\n",
       " 'by',\n",
       " 'the',\n",
       " 'antioxidant',\n",
       " 'network',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " '.',\n",
       " 'disturbance',\n",
       " 'in',\n",
       " 'the',\n",
       " 'pro',\n",
       " '##oxid',\n",
       " '##ant',\n",
       " '-',\n",
       " 'antioxidant',\n",
       " 'balance',\n",
       " 'in',\n",
       " 'favor',\n",
       " 'of',\n",
       " 'the',\n",
       " 'former',\n",
       " 'leads',\n",
       " 'to',\n",
       " 'what',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'oxidative',\n",
       " 'stress',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " '.',\n",
       " 'this',\n",
       " 'oxidative',\n",
       " 'stress',\n",
       " 'and',\n",
       " 'reactive',\n",
       " 'oxygen',\n",
       " 'species',\n",
       " '(',\n",
       " 'ros',\n",
       " ')',\n",
       " 'can',\n",
       " 'cause',\n",
       " 'damage',\n",
       " 'to',\n",
       " 'dna',\n",
       " ',',\n",
       " 'proteins',\n",
       " 'and',\n",
       " 'lipids',\n",
       " 'an',\n",
       " 'd',\n",
       " 'end',\n",
       " 'up',\n",
       " 'with',\n",
       " 'an',\n",
       " 'epidemic',\n",
       " 'of',\n",
       " 'non',\n",
       " 'communicable',\n",
       " 'chronic',\n",
       " 'human',\n",
       " 'diseases',\n",
       " '[',\n",
       " '3',\n",
       " '–',\n",
       " '5',\n",
       " ']',\n",
       " '.',\n",
       " 'the',\n",
       " 'prevalence',\n",
       " 'of',\n",
       " 'ncd',\n",
       " 'are',\n",
       " 'at',\n",
       " 'escal',\n",
       " '##ating',\n",
       " 'in',\n",
       " 'egypt',\n",
       " 'due',\n",
       " 'to',\n",
       " 'activation',\n",
       " 'of',\n",
       " '64',\n",
       " 'genes',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'inflammation',\n",
       " '[',\n",
       " '6',\n",
       " ',',\n",
       " '7',\n",
       " ']',\n",
       " 'and',\n",
       " 'other',\n",
       " 'modifiable',\n",
       " 'risk',\n",
       " 'factors',\n",
       " '[',\n",
       " '8',\n",
       " ']',\n",
       " '.',\n",
       " 'medical',\n",
       " 'and',\n",
       " 'pharmacologic',\n",
       " 'chemotherapeutic',\n",
       " 'agents',\n",
       " 'were',\n",
       " 'reported',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'cardio',\n",
       " 'vascular',\n",
       " 'mortality',\n",
       " 'among',\n",
       " 'individuals',\n",
       " 'at',\n",
       " 'risk',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'may',\n",
       " 'induce',\n",
       " 'oxidative',\n",
       " 'stress',\n",
       " ',',\n",
       " 'which',\n",
       " 'increases',\n",
       " 'to',\n",
       " 'an',\n",
       " 'invasive',\n",
       " 'stage',\n",
       " 'with',\n",
       " 'disease',\n",
       " 'progression',\n",
       " '[',\n",
       " '9',\n",
       " ']',\n",
       " '.',\n",
       " 'plant',\n",
       " 'polyphenols',\n",
       " 'possess',\n",
       " 'the',\n",
       " 'ideal',\n",
       " 'chemical',\n",
       " 'structure',\n",
       " 'for',\n",
       " 'free',\n",
       " 'radical',\n",
       " 'scavenging',\n",
       " 'activity',\n",
       " 'and',\n",
       " 'their',\n",
       " 'in',\n",
       " 'vitro',\n",
       " 'antioxidative',\n",
       " 'activities',\n",
       " 'are',\n",
       " 'more',\n",
       " 'effective',\n",
       " 'than',\n",
       " 'tocopherol',\n",
       " '##s',\n",
       " 'and',\n",
       " 'ascorbate',\n",
       " '[',\n",
       " '10',\n",
       " ']',\n",
       " '.',\n",
       " 'designing',\n",
       " 'effective',\n",
       " 'preventive',\n",
       " 'strategies',\n",
       " 'using',\n",
       " 'naturally',\n",
       " 'occurring',\n",
       " 'phyto',\n",
       " '##nutr',\n",
       " '##ients',\n",
       " 'aiming',\n",
       " 'at',\n",
       " 'reducing',\n",
       " 'oxidative',\n",
       " 'stress',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cost',\n",
       " '-',\n",
       " 'effective',\n",
       " 'strategies',\n",
       " 'to',\n",
       " 'move',\n",
       " 'people',\n",
       " '’',\n",
       " 's',\n",
       " 'lifestyle',\n",
       " 'toward',\n",
       " 'healthier',\n",
       " 'behaviors',\n",
       " '[',\n",
       " '11',\n",
       " ']',\n",
       " '.',\n",
       " 'pom',\n",
       " '##eg',\n",
       " '##ran',\n",
       " '##ate',\n",
       " 'is',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'fruit',\n",
       " 'grown',\n",
       " 'in',\n",
       " 'egypt',\n",
       " 'with',\n",
       " 'annual',\n",
       " 'production',\n",
       " 'of',\n",
       " 'approximately',\n",
       " '1300',\n",
       " '##00',\n",
       " 'tons',\n",
       " '[',\n",
       " '12',\n",
       " ']',\n",
       " '.',\n",
       " 'natural',\n",
       " 'unp',\n",
       " '##rocess',\n",
       " '##ed',\n",
       " 'pom',\n",
       " '##eg',\n",
       " '##ran',\n",
       " '##ate',\n",
       " 'juice',\n",
       " '(',\n",
       " 'pg',\n",
       " '##j',\n",
       " ')',\n",
       " 'is',\n",
       " 'superior',\n",
       " 'to',\n",
       " 'commercial',\n",
       " 'juice',\n",
       " '##s',\n",
       " 'in',\n",
       " 'their',\n",
       " 'polyphenol',\n",
       " '(',\n",
       " 'pp',\n",
       " ')',\n",
       " 'contents',\n",
       " 'with',\n",
       " 'mean',\n",
       " 'levels',\n",
       " 'of',\n",
       " '421',\n",
       " 'and',\n",
       " '382',\n",
       " 'mg',\n",
       " 'per',\n",
       " '100',\n",
       " 'ml',\n",
       " ',',\n",
       " 'respectively',\n",
       " '.',\n",
       " 'another',\n",
       " 'investigation',\n",
       " 'reported',\n",
       " 'respective',\n",
       " '(',\n",
       " 'pp',\n",
       " ')',\n",
       " 'concentrations',\n",
       " 'of',\n",
       " '139',\n",
       " 'mg',\n",
       " 'gallic',\n",
       " 'acid',\n",
       " 'equivalent',\n",
       " '(',\n",
       " 'ga',\n",
       " '##e',\n",
       " ')',\n",
       " 'per',\n",
       " '100',\n",
       " 'ml',\n",
       " 'juice',\n",
       " 'and',\n",
       " 'may',\n",
       " 'reach',\n",
       " 'over',\n",
       " '200',\n",
       " 'mg',\n",
       " 'ga',\n",
       " '##e',\n",
       " '/',\n",
       " '100',\n",
       " 'ml',\n",
       " ',',\n",
       " 'when',\n",
       " 'the',\n",
       " 'other',\n",
       " 'phenolic',\n",
       " 'compounds',\n",
       " ';',\n",
       " 'anth',\n",
       " '##ocy',\n",
       " '##amin',\n",
       " ',',\n",
       " 'ell',\n",
       " '##agi',\n",
       " '##tan',\n",
       " '##ni',\n",
       " '##ns',\n",
       " ',',\n",
       " 'and',\n",
       " 'tann',\n",
       " '##in',\n",
       " 'pun',\n",
       " '##ical',\n",
       " '##agin',\n",
       " 'were',\n",
       " 'included',\n",
       " '[',\n",
       " '13',\n",
       " ']',\n",
       " '.',\n",
       " 'dietary',\n",
       " 'pom',\n",
       " '##eg',\n",
       " '##ran',\n",
       " '##ate',\n",
       " 'intake',\n",
       " 'in',\n",
       " 'human',\n",
       " 'trials',\n",
       " 'elevated',\n",
       " 'urinary',\n",
       " 'excretion',\n",
       " 'of',\n",
       " 'the',\n",
       " 'above',\n",
       " 'mentioned',\n",
       " 'phenolic',\n",
       " 'metabolites',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'the',\n",
       " 'bioactive',\n",
       " 'constituents',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'more',\n",
       " 'than',\n",
       " '>',\n",
       " '50',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'antioxidative',\n",
       " 'capacity',\n",
       " 'activity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'juice',\n",
       " '[',\n",
       " '14',\n",
       " ',',\n",
       " '15',\n",
       " ']',\n",
       " 'and',\n",
       " 'are',\n",
       " 'the',\n",
       " 'biomarkers',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'health',\n",
       " 'promotion',\n",
       " '[',\n",
       " '16',\n",
       " ']',\n",
       " '.',\n",
       " 'measurement',\n",
       " 'of',\n",
       " 'gst',\n",
       " 'activity',\n",
       " 'had',\n",
       " 'been',\n",
       " 'recommended',\n",
       " 'for',\n",
       " 'the',\n",
       " 'evaluation',\n",
       " 'of',\n",
       " 'protective',\n",
       " 'treatment',\n",
       " 'in',\n",
       " 'trials',\n",
       " 'considering',\n",
       " 'antioxidant',\n",
       " 'strategies',\n",
       " '.',\n",
       " 'the',\n",
       " 'expression',\n",
       " 'of',\n",
       " 'the',\n",
       " 'phase',\n",
       " 'ii',\n",
       " 'hepatic',\n",
       " 'glutathione',\n",
       " 's',\n",
       " '-',\n",
       " 'transferase',\n",
       " 'was',\n",
       " 'activated',\n",
       " 'in',\n",
       " 'the',\n",
       " 'liver',\n",
       " 'cells',\n",
       " 'of',\n",
       " 'animals',\n",
       " 'following',\n",
       " 'feeding',\n",
       " 'pom',\n",
       " '##eg',\n",
       " '##ran',\n",
       " '##ate',\n",
       " 'anthocyanins',\n",
       " 'flavonoids',\n",
       " '.',\n",
       " 'the',\n",
       " 'molecular',\n",
       " 'mechanism',\n",
       " 'was',\n",
       " 'related',\n",
       " 'to',\n",
       " 'activation',\n",
       " 'of',\n",
       " 'antioxidant',\n",
       " 'response',\n",
       " 'element',\n",
       " '(',\n",
       " 'are',\n",
       " ')',\n",
       " 'upstream',\n",
       " 'of',\n",
       " 'genes',\n",
       " 'that',\n",
       " 'regulate',\n",
       " 'the',\n",
       " 'expression',\n",
       " 'of',\n",
       " 'gst',\n",
       " '[',\n",
       " '17',\n",
       " ']',\n",
       " '.',\n",
       " 'specific',\n",
       " 'strains',\n",
       " 'of',\n",
       " 'lactic',\n",
       " 'acid',\n",
       " 'bacteria',\n",
       " '(',\n",
       " 'lab',\n",
       " ')',\n",
       " 'possess',\n",
       " 'also',\n",
       " 'antioxidative',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cfb22-9dc1-4e99-ba27-ded5ecbd3f8e",
   "metadata": {},
   "source": [
    "#### Prep dataset with Masked tokens @ 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82a3fbea-b189-42e7-8bd7-8ea71532583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5e1e83a-40af-4390-a954-b7c94fb210d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054eb03362b3463babd9f6136167b9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd91372d59144087aad3e90eda1bf87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attention Mask:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb02dda8188c40eeada7ca78f0f480a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking Words:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodings = mlm_pipe(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c797e86f-d8dc-4b1b-96b1-cb0bae64c9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1491)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc051eef-e075-4ff3-a17e-eac96362c81e",
   "metadata": {},
   "source": [
    "#### Serialize encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fecc683b-fd48-49ff-9b61-d8bf11559076",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encodings, './encodings.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0091ba9a-df1a-490d-a1d8-6f02adccceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/encodings/encodings_395390_combined4Gb_1.txt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "77d2e572-d72d-4aae-bcab-fd9d1acad8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = test['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ac200b53-4953-4a93-86ab-42258fc28459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = tokens.detach().cpu().tolist()\n",
    "tokens = [tokenizer.id_to_token(tok) for tok in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3114fcad-e5f3-49f2-bd1b-cb6a26e3bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.token_to_id('##ate')\n",
    "testrun = ['pom','##eg', '##ran','##ate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6093eeb9-0ff4-4b4d-8f0c-d6899f1aede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(tokenizer.get_vocab().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2779a955-75b0-4e2c-b55f-e2e1a10e1755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b134390-e57f-4c0b-a94e-3c0a87c50d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = create_masked_lm_predictions(tokens, 0.15, 80, vocab, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "592ab0fa-d439-49e9-9080-0edf603be1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b571d9d-6f7f-40d7-9592-9f6b03095d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tokens.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9849de0f-6c1f-49bd-8c9d-4c3b4ec726b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenizer.id_to_token(tok) for tensor in tokens for tok in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "984bea07-39d4-4a2e-9349-5fe8843d7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ' '.join(samples).split('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "01a2a84e-4721-4f7c-a9a4-d7f7dc02cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tester(n=100):\n",
    "    percents = []\n",
    "    for _ in range(n):\n",
    "        ids, labels = create_masked_lm_predictions(tokens, 0.20, 80, vocab, rng)\n",
    "        count = 0\n",
    "        for label in ids:\n",
    "            if label == '[MASK]':\n",
    "                count += 1\n",
    "        percents.append(count/len(ids))\n",
    "    return np.mean(percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "def241a6-58a3-4d6f-8959-bbf1cf0999d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  2765,  2166,  ...,  1970, 26915,     3],\n",
       "        [    2,    21,  2765,  ...,    16,  1637,     3],\n",
       "        [    2,  2765,  6500,  ..., 17977,  1672,     3],\n",
       "        ...,\n",
       "        [    2,    21,    18,  ...,    18,  1784,     3],\n",
       "        [    2,  3680,  3337,  ...,    30,  1763,     3],\n",
       "        [    2,  2765, 20044,  ...,    13,    18,     3]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b235380c-bc1f-4ddd-a921-549315897373",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8255/2335880522.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, inputs, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encode_batch: `inputs` can't be `None`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "tokenizer.encode_batch([output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f06b2-1704-459e-958f-21730e347fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
