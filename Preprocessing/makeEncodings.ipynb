{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62030896-2adb-4358-90d2-f8868b541d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import torch\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#tokenizers and datasets\n",
    "import tokenizers\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af5d0f-2b5e-43ab-999b-fbd962d0581c",
   "metadata": {},
   "source": [
    "#### Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eca09c24-e880-4af1-8a00-5e95624fee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(90.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_tok_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "vm_data = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/'\n",
    "checkpoint_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/'\n",
    "files = [f for f in os.listdir(vm_data) if os.path.isfile(os.path.join(vm_data, f))]\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c7f68-7413-47b4-be37-b4bf0cfbf0f2",
   "metadata": {},
   "source": [
    "#### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55f7d5d-afa2-413d-97ba-8d6eeb507a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6f2148-2a9f-4c2b-9f55-66855d60f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_seq_512(os.path.join(vm_data, 'english_docs_aa.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b84e09-aa59-4af7-865d-1d5fb78c31f6",
   "metadata": {},
   "source": [
    "#### Load tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ecf5184-9eb6-43f9-a27a-f9aacded92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_from_file(vocab_path: str) -> BertWordPieceTokenizer:\n",
    "    tokenizer = BertWordPieceTokenizer(vocab_path, strip_accents=True, lowercase=True)\n",
    "    tokenizer.enable_truncation(max_length=512)\n",
    "    tokenizer.enable_padding()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "            (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "        ],\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004a35b9-6a08-4292-bcbc-ce38c6625f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer_from_file('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a556b1-56ad-434f-a73c-389a8d481ae7",
   "metadata": {},
   "source": [
    "#### Batch encode raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b79067-bdf4-433c-91bf-4981ef2f87f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.5 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(data)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578ec21f-3898-4f4a-930b-91114a431daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98862"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cfb22-9dc1-4e99-ba27-ded5ecbd3f8e",
   "metadata": {},
   "source": [
    "#### Prep dataset with Masked tokens @ 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82a3fbea-b189-42e7-8bd7-8ea71532583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5e1e83a-40af-4390-a954-b7c94fb210d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054eb03362b3463babd9f6136167b9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd91372d59144087aad3e90eda1bf87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attention Mask:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb02dda8188c40eeada7ca78f0f480a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking Words:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodings = mlm_pipe(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c797e86f-d8dc-4b1b-96b1-cb0bae64c9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1491)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc051eef-e075-4ff3-a17e-eac96362c81e",
   "metadata": {},
   "source": [
    "#### Serialize encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fecc683b-fd48-49ff-9b61-d8bf11559076",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encodings, './encodings.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091ba9a-df1a-490d-a1d8-6f02adccceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
