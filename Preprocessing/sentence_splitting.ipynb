{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0896a17-7fc9-452f-9f9b-2a6e985568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "import unicodedata\n",
    "import time\n",
    "import random\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from tqdm import tqdm\n",
    "# import nltk\n",
    "# from nltk import tokenize\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b887516f-e5a1-4219-a1b2-23c3bd721a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/english_docs_aa.txt') as f:\n",
    "    docs = [line for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e2cd270-b01e-4625-b313-84280ead858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = docs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f55ea401-cf72-4db8-9760-16be75bfeeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(docs):\n",
    "    sentences = []\n",
    "    count = 0\n",
    "    \n",
    "    s = time.perf_counter()\n",
    "    for doc in docs:\n",
    "        sents = split_into_sentences(unicodedata.normalize(\"NFKD\", doc))\n",
    "        sentences.append(sents)\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f'{count} completed')\n",
    "    e = time.perf_counter() - s\n",
    "    sents = [s for s in sentences if len(s.split()) > 4]\n",
    "    print(round(e, 2))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfdb2f0-20bc-4d3b-89f0-dd52345b2777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a bunhc of text.',\n",
       " 'And some more.',\n",
       " 'And maybe 5.2% more or could be 5.5% more.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences('a bunhc of text. And some more.  And maybe 5.2% more or could be 5. 5% more.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01539ee8-bc34-4580-b1e0-5ab5835c9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wordPiece_tokenizer_config_30500.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "066d07e2-2965-4ed2-b623-7ec31c4419ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 doc sentenceized\n",
      "20000 doc sentenceized\n",
      "30000 doc sentenceized\n",
      "40000 doc sentenceized\n",
      "50000 doc sentenceized\n",
      "60000 doc sentenceized\n",
      "70000 doc sentenceized\n",
      "80000 doc sentenceized\n",
      "90000 doc sentenceized\n",
      "Total time: 4.2 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "\n",
    "start = time.perf_counter()\n",
    "with open('../Data/english_docs_aa.txt') as f:\n",
    "    docs = [line for line in f.readlines()]\n",
    "    with open('../Data/test_sentences.txt', 'a') as w:\n",
    "        for doc in docs:\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'{count} doc sentenceized')\n",
    "            try:\n",
    "                sentences = split_into_sentences(unicodedata.normalize('NFKD', doc))\n",
    "                for sentence in sentences:\n",
    "                    if len(sentence.split()) > 4:\n",
    "                        w.write(sentence.strip())\n",
    "                        w.write('\\n')\n",
    "                w.write('\\n')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "end = time.perf_counter() - start\n",
    "print(f'Total time: {round(end/60, 1)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24918dbb-9cbc-43d1-8f71-fdde7d315ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6838a4c2-0c23-4e29-85ec-701d797b7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer(vocab_file='../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', tokenizer_file='../Preprocessing/Tokenization/wordPiece_tokenizer_config_30500.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27be977e-6dbc-47f0-9055-65dfe3fa943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = tok.batch_encode_plus(sentences, max_length=128, padding=True, truncation=True, return_token_type_ids=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5c451-19e0-49fe-89eb-85e71517b16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
