{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62030896-2adb-4358-90d2-f8868b541d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#standard imports\n",
    "import torch\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#tokenizers and datasets\n",
    "import tokenizers\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import BertTokenizer\n",
    "from whole_word_masking_ids import create_masked_lm_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af5d0f-2b5e-43ab-999b-fbd962d0581c",
   "metadata": {},
   "source": [
    "#### Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca09c24-e880-4af1-8a00-5e95624fee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['combined4Gb_1.txt', 'combined4Gb_2.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_tok_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "vm_data = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/text/partials/xaatest'\n",
    "checkpoint_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/'\n",
    "files = [f for f in os.listdir(vm_data) if os.path.isfile(os.path.join(vm_data, f)) and f.startswith('combined')]\n",
    "files = sorted(files)[:2]\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c7f68-7413-47b4-be37-b4bf0cfbf0f2",
   "metadata": {},
   "source": [
    "#### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c55f7d5d-afa2-413d-97ba-8d6eeb507a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c6f2148-2a9f-4c2b-9f55-66855d60f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_seq_512(os.path.join(vm_data, 'english_docs_aa.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75e41d00-5381-496b-a553-fe11bb968269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98862"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b84e09-aa59-4af7-865d-1d5fb78c31f6",
   "metadata": {},
   "source": [
    "#### Load tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecf5184-9eb6-43f9-a27a-f9aacded92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_from_file(vocab_path: str) -> BertWordPieceTokenizer:\n",
    "    tokenizer = BertWordPieceTokenizer(vocab_path, strip_accents=True, lowercase=True)\n",
    "    tokenizer.enable_truncation(max_length=512)\n",
    "    tokenizer.enable_padding()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "            (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "        ],\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004a35b9-6a08-4292-bcbc-ce38c6625f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer_from_file('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d83a5f31-e38a-411b-99a1-c032f5ad6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_ids(tokenizer):\n",
    "    id_list = [tokenizer.token_to_id(word) for word in tokenizer.get_vocab()]\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "531466d8-7662-41dd-8212-06f329f33376",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = create_source_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a556b1-56ad-434f-a73c-389a8d481ae7",
   "metadata": {},
   "source": [
    "#### Batch encode raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3b79067-bdf4-433c-91bf-4981ef2f87f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.01 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(data)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cfb22-9dc1-4e99-ba27-ded5ecbd3f8e",
   "metadata": {},
   "source": [
    "#### Prep dataset with Masked tokens @ 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82a3fbea-b189-42e7-8bd7-8ea71532583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], source_ids: list, tokenizer, mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = torch.tensor([create_masked_lm_ids(x.ids, source_ids, tokenizer) for x in tqdm(batch, 'Input Ids')])\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    \n",
    "    # rand = torch.rand(input_ids.shape)\n",
    "    # mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    # for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "    #     selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    #     input_ids[i, selection] = 4\n",
    "        \n",
    "   \n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f5e1e83a-40af-4390-a954-b7c94fb210d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e9f183a3ea4c6c8edd4dc592fa52b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c472ed5657247c2b89bfddc19da29a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attention Mask:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046f8316b1264e228e8ce60e4be69397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Input Ids:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodings = mlm_pipe(batch, id_list, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d32c4-c298-4613-a2ea-6932b52e8f97",
   "metadata": {},
   "source": [
    "#### Check to ensure percentage of masking is what you'd expect (should be @ 12%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c797e86f-d8dc-4b1b-96b1-cb0bae64c9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1203)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "167bcd16-f984-460d-9f75-ba0deab5e584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98862, 512])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc051eef-e075-4ff3-a17e-eac96362c81e",
   "metadata": {},
   "source": [
    "#### Serialize encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fecc683b-fd48-49ff-9b61-d8bf11559076",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encodings, './encodings.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0091ba9a-df1a-490d-a1d8-6f02adccceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/encodings/encodings_395390_combined4Gb_1.txt.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
