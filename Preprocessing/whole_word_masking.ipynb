{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92394b58-f3b2-42b9-8d19-ff747184b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import BertTokenizer\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30a080c-af8b-4a8e-9b28-46ea4f6248f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "text_data_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/Text/'\n",
    "encodings_data_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/Encodings/encodings_0_395390.pt'\n",
    "working_data =  'combined_4Gb.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1b58d0-1d28-4c2f-88f0-fe919a8f62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = namedtuple(\"MaskedLmInstance\",[\"index\", \"label\"])\n",
    "\n",
    "def create_masked_lm_predictions(tokens: list, vocab_words: list, masked_lm_prob: float=0.15, max_predictions_per_seq: int=77):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word. When a word has been split into\n",
    "    # WordPieces, the first token does not have any marker and any subsequence\n",
    "    # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "    # append it to the previous set of word indexes.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (len(cand_indexes) >= 1 and token.startswith(\"##\")):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "\n",
    "  random.shuffle(cand_indexes)\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for index_set in cand_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if random.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if random.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[random.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f22b8ca-d48e-495c-9f62-0ae56f241b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(tokenizer_path, strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a14c98ec-d59f-4bd1-8a4c-f09d70a049b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This city of Hoboken is known for snuffleupagus, histrionics, missspellled words, such as acetylcholinesterase and dopaminergic effects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d318e059-f122-4a12-beee-13b9b66f2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ec656a7-e8de-4152-aedf-bb83703fcf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]',\n",
       "  'this',\n",
       "  'city',\n",
       "  'of',\n",
       "  'hob',\n",
       "  '##oken',\n",
       "  'is',\n",
       "  'known',\n",
       "  '[MASK]',\n",
       "  'sn',\n",
       "  '##uff',\n",
       "  '##le',\n",
       "  '##up',\n",
       "  '##agus',\n",
       "  ',',\n",
       "  'hist',\n",
       "  '##rion',\n",
       "  '##ics',\n",
       "  ',',\n",
       "  'miss',\n",
       "  '##sp',\n",
       "  '##ell',\n",
       "  '##led',\n",
       "  'words',\n",
       "  ',',\n",
       "  'such',\n",
       "  'as',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  'and',\n",
       "  'dopaminergic',\n",
       "  'effects',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " [8, 27, 28, 29, 30],\n",
       " ['for', 'acetylcholine', '##st', '##era', '##se'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masked_lm_predictions(tokens.tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e406623d-790a-4cfb-a964-e0e02345b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(tokenizer.get_vocab().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474837d-1be5-4000-b22d-447a971de59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
