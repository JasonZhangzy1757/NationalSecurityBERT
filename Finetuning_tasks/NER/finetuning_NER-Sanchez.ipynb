{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VphU63SBtdXW",
    "outputId": "41a590dd-6e0c-4cf3-b084-15f1cd9f9d79"
   },
   "outputs": [],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NYUqKiOZdR1H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.nn.parallel import DataParallel\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "from torch import cuda\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "sdqhHeAqcLnO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def def_value():\n",
    "    return 'O'\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        sent_dict = {}\n",
    "        label_dict = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line.isspace():\n",
    "                continue\n",
    "            if '|' in line and len(line.split('|')) == 3 and (line.split('|')[1] == 'a' or line.split('|')[1] == 't'):\n",
    "                idx, _, sentence = line.split('|')\n",
    "                sent_dict[idx] = sent_dict.get(idx, '') + ' ' + sentence\n",
    "            else:\n",
    "                idx, start_pos, end_pos, word, label, _ = line.split('\\t')\n",
    "                if idx not in label_dict:\n",
    "                    label_dict[idx] = defaultdict(def_value)\n",
    "                    for i in range(int(start_pos), int(end_pos)):\n",
    "                        label_dict[idx][i] = label  \n",
    "                else:\n",
    "                    for i in range(int(start_pos), int(end_pos)):\n",
    "                        label_dict[idx][i] = label\n",
    "                        \n",
    "    idx_col, word_col, label_col = [], [], []\n",
    "    for idx in sent_dict:\n",
    "        sentence = sent_dict[idx].replace('\\n', '')\n",
    "        \n",
    "        char_seq = 0\n",
    "        for word in sentence.split(' ')[1:]:\n",
    "            label = label_dict[idx][char_seq]\n",
    "            if word and word[0] == '(':\n",
    "                label = label_dict[idx][char_seq + 1]\n",
    "            char_seq += len(word) + 1\n",
    "            \n",
    "            idx_col.append(idx)\n",
    "            word_col.append(word)\n",
    "            label_col.append(label)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(idx_col, word_col, label_col)),\n",
    "               columns =['sentence_id', 'word', 'label'])\n",
    "    return df\n",
    "\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda x: [(w, t) for w, t in zip(x[\"word\"].values.tolist(),\n",
    "                                                        x[\"label\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            sentence = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return sentence\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "# Creating new lists and dicts that will be used at a later stage for reference and processing\n",
    "def get_data(df, label_vals):\n",
    "    getter = SentenceGetter(df)\n",
    "    label2idx = {value: key for key, value in enumerate(label_vals)}\n",
    "    sentences = [' '.join([s[0] for s in sentence]) for sentence in getter.sentences]\n",
    "    labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
    "    labels = [[label2idx.get(l) for l in label] for label in labels]\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8aQ6WCk2a-Vd"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentence = str(self.sentences[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        label = self.labels[index]\n",
    "        label.extend([4]*200)\n",
    "        label=label[:200]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'tags': torch.tensor(label, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert = transformers.BertForTokenClassification.from_pretrained(model_path, \n",
    "                                                                            num_labels=18,\n",
    "                                                                            )\n",
    "\n",
    "    \n",
    "    def forward(self, ids, mask, labels):\n",
    "        output = self.bert(ids, mask, labels = labels)\n",
    "\n",
    "        return output\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for step, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['tags'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss = model(ids, mask, labels = targets)[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 5==0:\n",
    "            print(f'Epoch: {epoch}  Step: {step}  Loss: {loss.sum()}')\n",
    "            \n",
    "def valid(model, testing_loader, label_vals):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    predictions , true_labels = [], []\n",
    "    nb_eval_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['tags'].to(device, dtype = torch.long)\n",
    "\n",
    "            output = model(ids, mask, labels=targets)\n",
    "            loss, logits = output[:2]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = targets.to('cpu').numpy()\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "            true_labels.append(label_ids)\n",
    "            eval_loss += loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "        eval_loss = eval_loss/nb_eval_steps\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "        pred_tags = [label_vals[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [label_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "        print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags, average='micro')))\n",
    "    return pred_tags, valid_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "mEJxhvwhtSs1"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "df_train = read_data('./NCBI-disease/NCBItrainset_corpus.txt')\n",
    "df_valid = read_data('./NCBI-disease/NCBIdevelopset_corpus.txt')\n",
    "\n",
    "label_vals = list(df_train[\"label\"].value_counts().keys())\n",
    "label2idx = {value: key for key, value in enumerate(label_vals)}\n",
    "\n",
    "train_sentences, train_labels = get_data(df_train, label_vals)\n",
    "valid_sentences, valid_labels = get_data(df_valid, label_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50oMGTe0bvl0",
    "outputId": "c6f7bade-d303-476e-ecc2-ec8178895dbf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 6\n",
    "LEARNING_RATE = 5e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "\n",
    "training_set = CustomDataset(tokenizer, train_sentences, train_labels, MAX_LEN)\n",
    "valid_set = CustomDataset(tokenizer, valid_sentences, valid_labels, MAX_LEN)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.617720603942871\n",
      "Epoch: 0  Step: 5  Loss: 1.8589186668395996\n",
      "Epoch: 0  Step: 10  Loss: 1.5983073711395264\n",
      "Epoch: 0  Step: 15  Loss: 1.4750643968582153\n",
      "Epoch: 1  Step: 0  Loss: 1.3763785362243652\n",
      "Epoch: 1  Step: 5  Loss: 1.1906495094299316\n",
      "Epoch: 1  Step: 10  Loss: 1.222059965133667\n",
      "Epoch: 1  Step: 15  Loss: 1.1399253606796265\n",
      "Epoch: 2  Step: 0  Loss: 1.0465832948684692\n",
      "Epoch: 2  Step: 5  Loss: 1.012534499168396\n",
      "Epoch: 2  Step: 10  Loss: 1.1112782955169678\n",
      "Epoch: 2  Step: 15  Loss: 0.898760974407196\n",
      "Epoch: 3  Step: 0  Loss: 0.8962532877922058\n",
      "Epoch: 3  Step: 5  Loss: 0.9761052131652832\n",
      "Epoch: 3  Step: 10  Loss: 1.060213327407837\n",
      "Epoch: 3  Step: 15  Loss: 0.9729636907577515\n",
      "Epoch: 4  Step: 0  Loss: 0.824504554271698\n",
      "Epoch: 4  Step: 5  Loss: 1.0439521074295044\n",
      "Epoch: 4  Step: 10  Loss: 0.9217320084571838\n",
      "Epoch: 4  Step: 15  Loss: 0.7990142107009888\n",
      "Epoch: 5  Step: 0  Loss: 0.7839782238006592\n",
      "Epoch: 5  Step: 5  Loss: 0.8049127459526062\n",
      "Epoch: 5  Step: 10  Loss: 0.6893328428268433\n",
      "Epoch: 5  Step: 15  Loss: 0.7793339490890503\n",
      "Validation loss: 0.5275888752937317\n",
      "F1-Score: 0.85925\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "pred_tags, valid_tags = valid(model, valid_loader, label_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'../../Modeling/checkpoints/model-trained-36-130647.pt/'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
