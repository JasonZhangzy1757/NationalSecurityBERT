{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VphU63SBtdXW",
    "outputId": "41a590dd-6e0c-4cf3-b084-15f1cd9f9d79"
   },
   "outputs": [],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  2 17:21:13 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    36W / 250W |      0MiB / 16384MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NYUqKiOZdR1H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from torch.nn.parallel import DataParallel\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sdqhHeAqcLnO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def def_value():\n",
    "    return 'O'\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        sent_dict = {}\n",
    "        label_dict = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line.isspace():\n",
    "                continue\n",
    "            if '|' in line and len(line.split('|')) == 3 and (line.split('|')[1] == 'a' or line.split('|')[1] == 't'):\n",
    "                idx, _, sentence = line.split('|')\n",
    "                sent_dict[idx] = sent_dict.get(idx, '') + ' ' + sentence\n",
    "            else:\n",
    "                idx, start_pos, end_pos, word, label, _ = line.split('\\t')\n",
    "                if idx not in label_dict:\n",
    "                    label_dict[idx] = defaultdict(def_value)\n",
    "                    for i in range(int(start_pos), int(end_pos)):\n",
    "                        label_dict[idx][i] = label  \n",
    "                else:\n",
    "                    for i in range(int(start_pos), int(end_pos)):\n",
    "                        label_dict[idx][i] = label\n",
    "                        \n",
    "    idx_col, word_col, label_col = [], [], []\n",
    "    for idx in sent_dict:\n",
    "        sentence = sent_dict[idx].replace('\\n', '')\n",
    "        \n",
    "        char_seq = 0\n",
    "        for word in sentence.split(' ')[1:]:\n",
    "            label = label_dict[idx][char_seq]\n",
    "            if word and word[0] == '(':\n",
    "                label = label_dict[idx][char_seq + 1]\n",
    "            char_seq += len(word) + 1\n",
    "            \n",
    "            idx_col.append(idx)\n",
    "            word_col.append(word)\n",
    "            label_col.append(label)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(idx_col, word_col, label_col)),\n",
    "               columns =['sentence_id', 'word', 'label'])\n",
    "    return df\n",
    "\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda x: [(w, t) for w, t in zip(x[\"word\"].values.tolist(),\n",
    "                                                        x[\"label\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            sentence = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return sentence\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "# Creating new lists and dicts that will be used at a later stage for reference and processing\n",
    "def get_data(df, label_vals):\n",
    "    getter = SentenceGetter(df)\n",
    "    label2idx = {value: key for key, value in enumerate(label_vals)}\n",
    "    sentences = [' '.join([s[0] for s in sentence]) for sentence in getter.sentences]\n",
    "    labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
    "    labels = [[label2idx.get(l) for l in label] for label in labels]\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8aQ6WCk2a-Vd"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentence = str(self.sentences[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        label = self.labels[index]\n",
    "        label.extend([4]*200)\n",
    "        label=label[:200]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'tags': torch.tensor(label, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert = transformers.BertForTokenClassification.from_pretrained(model_path, \n",
    "                                                                            num_labels=18,\n",
    "                                                                            )\n",
    "    def forward(self, ids, mask, labels):\n",
    "        output = self.bert(ids, mask, labels = labels)\n",
    "\n",
    "        return output\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for step, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['tags'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss = model(ids, mask, labels = targets)[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10==0:\n",
    "            print(f'Epoch: {epoch}  Step: {step}  Loss: {loss.sum()}')\n",
    "            \n",
    "def valid(model, testing_loader, label_vals):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    predictions , true_labels = [], []\n",
    "    nb_eval_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['tags'].to(device, dtype = torch.long)\n",
    "\n",
    "            output = model(ids, mask, labels=targets)\n",
    "            loss, logits = output[:2]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = targets.to('cpu').numpy()\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "            true_labels.append(label_ids)\n",
    "            eval_loss += loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "        eval_loss = eval_loss/nb_eval_steps\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "        pred_tags = [label_vals[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [label_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "        score = f1_score(pred_tags, valid_tags, average='micro')\n",
    "        print(\"F1-Score: {}\".format(round(score, 4)))\n",
    "    return pred_tags, valid_tags, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mEJxhvwhtSs1"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "df_train = read_data('./NCBI-disease/NCBItrainset_corpus.txt')\n",
    "df_valid = read_data('./NCBI-disease/NCBIdevelopset_corpus.txt')\n",
    "df_test = read_data('./NCBI-disease/NCBItestset_corpus.txt')\n",
    "\n",
    "label_vals = list(df_train[\"label\"].value_counts().keys())\n",
    "label2idx = {value: key for key, value in enumerate(label_vals)}\n",
    "\n",
    "train_sentences, train_labels = get_data(df_train, label_vals)\n",
    "valid_sentences, valid_labels = get_data(df_valid, label_vals)\n",
    "test_sentences, test_labels = get_data(df_test, label_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['bert-base-uncased',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50oMGTe0bvl0",
    "outputId": "c6f7bade-d303-476e-ecc2-ec8178895dbf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.687420845031738\n",
      "Epoch: 0  Step: 10  Loss: 1.7327806949615479\n",
      "Epoch: 1  Step: 0  Loss: 1.3148291110992432\n",
      "Epoch: 1  Step: 10  Loss: 1.149648666381836\n",
      "Epoch: 2  Step: 0  Loss: 1.107922911643982\n",
      "Epoch: 2  Step: 10  Loss: 0.9656139612197876\n",
      "Epoch: 3  Step: 0  Loss: 0.8845291137695312\n",
      "Epoch: 3  Step: 10  Loss: 1.067922830581665\n",
      "Validation loss: 0.5210551851987839\n",
      "F1-Score: 0.8458\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.509517669677734\n",
      "Epoch: 0  Step: 10  Loss: 1.5289348363876343\n",
      "Epoch: 1  Step: 0  Loss: 1.3666505813598633\n",
      "Epoch: 1  Step: 10  Loss: 1.3123486042022705\n",
      "Epoch: 2  Step: 0  Loss: 1.0871326923370361\n",
      "Epoch: 2  Step: 10  Loss: 1.1183075904846191\n",
      "Epoch: 3  Step: 0  Loss: 0.8956232070922852\n",
      "Epoch: 3  Step: 10  Loss: 0.8684400320053101\n",
      "Validation loss: 0.52109020113945\n",
      "F1-Score: 0.847\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.263585090637207\n",
      "Epoch: 0  Step: 10  Loss: 1.6403026580810547\n",
      "Epoch: 1  Step: 0  Loss: 1.3779706954956055\n",
      "Epoch: 1  Step: 10  Loss: 1.4398229122161865\n",
      "Epoch: 2  Step: 0  Loss: 1.1944172382354736\n",
      "Epoch: 2  Step: 10  Loss: 1.0910563468933105\n",
      "Epoch: 3  Step: 0  Loss: 1.0459060668945312\n",
      "Epoch: 3  Step: 10  Loss: 0.9561759233474731\n",
      "Validation loss: 0.5206994640827179\n",
      "F1-Score: 0.8394\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.705776214599609\n",
      "Epoch: 0  Step: 10  Loss: 1.4801115989685059\n",
      "Epoch: 1  Step: 0  Loss: 1.3735594749450684\n",
      "Epoch: 1  Step: 10  Loss: 1.0405910015106201\n",
      "Epoch: 2  Step: 0  Loss: 0.9718479514122009\n",
      "Epoch: 2  Step: 10  Loss: 0.992163360118866\n",
      "Epoch: 3  Step: 0  Loss: 0.965697169303894\n",
      "Epoch: 3  Step: 10  Loss: 0.9491474628448486\n",
      "Validation loss: 0.5049722898006439\n",
      "F1-Score: 0.8472\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.083489418029785\n",
      "Epoch: 0  Step: 10  Loss: 1.4887809753417969\n",
      "Epoch: 1  Step: 0  Loss: 1.48544442653656\n",
      "Epoch: 1  Step: 10  Loss: 1.3792879581451416\n",
      "Epoch: 2  Step: 0  Loss: 1.107362985610962\n",
      "Epoch: 2  Step: 10  Loss: 0.985801100730896\n",
      "Epoch: 3  Step: 0  Loss: 0.940099835395813\n",
      "Epoch: 3  Step: 10  Loss: 0.9363573789596558\n",
      "Validation loss: 0.5169443321228028\n",
      "F1-Score: 0.8356\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-3531-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.176520824432373\n",
      "Epoch: 0  Step: 10  Loss: 1.0837037563323975\n",
      "Epoch: 1  Step: 0  Loss: 1.0709788799285889\n",
      "Epoch: 1  Step: 10  Loss: 1.0175144672393799\n",
      "Epoch: 2  Step: 0  Loss: 0.899137020111084\n",
      "Epoch: 2  Step: 10  Loss: 1.0591070652008057\n",
      "Epoch: 3  Step: 0  Loss: 0.8320268392562866\n",
      "Epoch: 3  Step: 10  Loss: 0.8605939149856567\n",
      "Validation loss: 0.4625757098197937\n",
      "F1-Score: 0.8678\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-3531-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 7.144320487976074\n",
      "Epoch: 0  Step: 10  Loss: 1.1516921520233154\n",
      "Epoch: 1  Step: 0  Loss: 1.0236661434173584\n",
      "Epoch: 1  Step: 10  Loss: 1.0558438301086426\n",
      "Epoch: 2  Step: 0  Loss: 0.7569860219955444\n",
      "Epoch: 2  Step: 10  Loss: 1.0348730087280273\n",
      "Epoch: 3  Step: 0  Loss: 0.7869874238967896\n",
      "Epoch: 3  Step: 10  Loss: 0.8950877785682678\n",
      "Validation loss: 0.44284523963928224\n",
      "F1-Score: 0.8737\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-3531-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.038453102111816\n",
      "Epoch: 0  Step: 10  Loss: 0.9828178882598877\n",
      "Epoch: 1  Step: 0  Loss: 1.003914475440979\n",
      "Epoch: 1  Step: 10  Loss: 1.045363187789917\n",
      "Epoch: 2  Step: 0  Loss: 0.9457418918609619\n",
      "Epoch: 2  Step: 10  Loss: 0.9361658096313477\n",
      "Epoch: 3  Step: 0  Loss: 0.7678549289703369\n",
      "Epoch: 3  Step: 10  Loss: 0.7457766532897949\n",
      "Validation loss: 0.4402932631969452\n",
      "F1-Score: 0.869\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-3531-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.123995304107666\n",
      "Epoch: 0  Step: 10  Loss: 1.089279294013977\n",
      "Epoch: 1  Step: 0  Loss: 1.0557209253311157\n",
      "Epoch: 1  Step: 10  Loss: 0.9836486577987671\n",
      "Epoch: 2  Step: 0  Loss: 0.8785923719406128\n",
      "Epoch: 2  Step: 10  Loss: 0.9308173656463623\n",
      "Epoch: 3  Step: 0  Loss: 0.872560441493988\n",
      "Epoch: 3  Step: 10  Loss: 0.876224160194397\n",
      "Validation loss: 0.49477944493293763\n",
      "F1-Score: 0.8666\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-3531-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 7.0498199462890625\n",
      "Epoch: 0  Step: 10  Loss: 1.1392158269882202\n",
      "Epoch: 1  Step: 0  Loss: 1.0418723821640015\n",
      "Epoch: 1  Step: 10  Loss: 0.8802346587181091\n",
      "Epoch: 2  Step: 0  Loss: 0.8278154134750366\n",
      "Epoch: 2  Step: 10  Loss: 0.8145420551300049\n",
      "Epoch: 3  Step: 0  Loss: 0.9953644275665283\n",
      "Epoch: 3  Step: 10  Loss: 0.8801654577255249\n",
      "Validation loss: 0.45299997806549075\n",
      "F1-Score: 0.8706\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.761045455932617\n",
      "Epoch: 0  Step: 10  Loss: 1.2753090858459473\n",
      "Epoch: 1  Step: 0  Loss: 1.1585495471954346\n",
      "Epoch: 1  Step: 10  Loss: 0.9526417255401611\n",
      "Epoch: 2  Step: 0  Loss: 0.8694533109664917\n",
      "Epoch: 2  Step: 10  Loss: 0.7739043831825256\n",
      "Epoch: 3  Step: 0  Loss: 0.7288327217102051\n",
      "Epoch: 3  Step: 10  Loss: 0.7251293659210205\n",
      "Validation loss: 0.44182080507278443\n",
      "F1-Score: 0.8795\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.752429962158203\n",
      "Epoch: 0  Step: 10  Loss: 1.2748037576675415\n",
      "Epoch: 1  Step: 0  Loss: 1.2442469596862793\n",
      "Epoch: 1  Step: 10  Loss: 1.0283081531524658\n",
      "Epoch: 2  Step: 0  Loss: 0.9913196563720703\n",
      "Epoch: 2  Step: 10  Loss: 0.774103045463562\n",
      "Epoch: 3  Step: 0  Loss: 0.9567851424217224\n",
      "Epoch: 3  Step: 10  Loss: 0.6703081130981445\n",
      "Validation loss: 0.44857399106025697\n",
      "F1-Score: 0.8728\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.941545486450195\n",
      "Epoch: 0  Step: 10  Loss: 1.1746726036071777\n",
      "Epoch: 1  Step: 0  Loss: 1.1387711763381958\n",
      "Epoch: 1  Step: 10  Loss: 0.9998651742935181\n",
      "Epoch: 2  Step: 0  Loss: 0.7578157782554626\n",
      "Epoch: 2  Step: 10  Loss: 0.800089955329895\n",
      "Epoch: 3  Step: 0  Loss: 0.6854862570762634\n",
      "Epoch: 3  Step: 10  Loss: 0.6026679873466492\n",
      "Validation loss: 0.4542917287349701\n",
      "F1-Score: 0.8786\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.060126781463623\n",
      "Epoch: 0  Step: 10  Loss: 1.2926037311553955\n",
      "Epoch: 1  Step: 0  Loss: 1.065826177597046\n",
      "Epoch: 1  Step: 10  Loss: 0.9457899332046509\n",
      "Epoch: 2  Step: 0  Loss: 0.7496519088745117\n",
      "Epoch: 2  Step: 10  Loss: 0.7237745523452759\n",
      "Epoch: 3  Step: 0  Loss: 0.7779240608215332\n",
      "Epoch: 3  Step: 10  Loss: 0.8325852155685425\n",
      "Validation loss: 0.44782627940177916\n",
      "F1-Score: 0.8758\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.031376361846924\n",
      "Epoch: 0  Step: 10  Loss: 1.3102623224258423\n",
      "Epoch: 1  Step: 0  Loss: 1.0752805471420288\n",
      "Epoch: 1  Step: 10  Loss: 0.8903945684432983\n",
      "Epoch: 2  Step: 0  Loss: 0.9251581430435181\n",
      "Epoch: 2  Step: 10  Loss: 0.8307806849479675\n",
      "Epoch: 3  Step: 0  Loss: 0.6949150562286377\n",
      "Epoch: 3  Step: 10  Loss: 0.6901861429214478\n",
      "Validation loss: 0.4597280889749527\n",
      "F1-Score: 0.8786\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.253313064575195\n",
      "Epoch: 0  Step: 10  Loss: 1.4128987789154053\n",
      "Epoch: 1  Step: 0  Loss: 0.9794008135795593\n",
      "Epoch: 1  Step: 10  Loss: 1.0067834854125977\n",
      "Epoch: 2  Step: 0  Loss: 0.7918365597724915\n",
      "Epoch: 2  Step: 10  Loss: 0.7996829748153687\n",
      "Epoch: 3  Step: 0  Loss: 0.681629478931427\n",
      "Epoch: 3  Step: 10  Loss: 0.7882630825042725\n",
      "Validation loss: 0.48265872716903685\n",
      "F1-Score: 0.857\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.729779243469238\n",
      "Epoch: 0  Step: 10  Loss: 1.1355373859405518\n",
      "Epoch: 1  Step: 0  Loss: 1.0580153465270996\n",
      "Epoch: 1  Step: 10  Loss: 0.8669025897979736\n",
      "Epoch: 2  Step: 0  Loss: 0.803413987159729\n",
      "Epoch: 2  Step: 10  Loss: 0.852226734161377\n",
      "Epoch: 3  Step: 0  Loss: 0.6994971036911011\n",
      "Epoch: 3  Step: 10  Loss: 0.6726678609848022\n",
      "Validation loss: 0.4481345200538635\n",
      "F1-Score: 0.8723\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.405369281768799\n",
      "Epoch: 0  Step: 10  Loss: 1.364654779434204\n",
      "Epoch: 1  Step: 0  Loss: 1.0936577320098877\n",
      "Epoch: 1  Step: 10  Loss: 1.1336878538131714\n",
      "Epoch: 2  Step: 0  Loss: 0.9382357001304626\n",
      "Epoch: 2  Step: 10  Loss: 0.883196234703064\n",
      "Epoch: 3  Step: 0  Loss: 0.6918751001358032\n",
      "Epoch: 3  Step: 10  Loss: 0.7894693613052368\n",
      "Validation loss: 0.4704786694049835\n",
      "F1-Score: 0.8759\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.940945625305176\n",
      "Epoch: 0  Step: 10  Loss: 1.2618517875671387\n",
      "Epoch: 1  Step: 0  Loss: 1.1855392456054688\n",
      "Epoch: 1  Step: 10  Loss: 0.9962645769119263\n",
      "Epoch: 2  Step: 0  Loss: 0.8198496699333191\n",
      "Epoch: 2  Step: 10  Loss: 0.8521735668182373\n",
      "Epoch: 3  Step: 0  Loss: 0.6587133407592773\n",
      "Epoch: 3  Step: 10  Loss: 0.7292495965957642\n",
      "Validation loss: 0.4568700981140137\n",
      "F1-Score: 0.8682\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.495365142822266\n",
      "Epoch: 0  Step: 10  Loss: 1.1687381267547607\n",
      "Epoch: 1  Step: 0  Loss: 1.1701271533966064\n",
      "Epoch: 1  Step: 10  Loss: 1.022147297859192\n",
      "Epoch: 2  Step: 0  Loss: 0.8440830707550049\n",
      "Epoch: 2  Step: 10  Loss: 0.8157178163528442\n",
      "Epoch: 3  Step: 0  Loss: 0.6536878347396851\n",
      "Epoch: 3  Step: 10  Loss: 0.7143787741661072\n",
      "Validation loss: 0.4679556852579117\n",
      "F1-Score: 0.876\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.937840461730957\n",
      "Epoch: 0  Step: 10  Loss: 1.1916557550430298\n",
      "Epoch: 1  Step: 0  Loss: 1.0318058729171753\n",
      "Epoch: 1  Step: 10  Loss: 0.8718205094337463\n",
      "Epoch: 2  Step: 0  Loss: 0.8274091482162476\n",
      "Epoch: 2  Step: 10  Loss: 0.8227959871292114\n",
      "Epoch: 3  Step: 0  Loss: 0.8565163612365723\n",
      "Epoch: 3  Step: 10  Loss: 0.77791827917099\n",
      "Validation loss: 0.44042434215545656\n",
      "F1-Score: 0.8796\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.226255416870117\n",
      "Epoch: 0  Step: 10  Loss: 1.1775641441345215\n",
      "Epoch: 1  Step: 0  Loss: 1.0140107870101929\n",
      "Epoch: 1  Step: 10  Loss: 0.8634536266326904\n",
      "Epoch: 2  Step: 0  Loss: 1.069753885269165\n",
      "Epoch: 2  Step: 10  Loss: 0.7539669275283813\n",
      "Epoch: 3  Step: 0  Loss: 0.7988015413284302\n",
      "Epoch: 3  Step: 10  Loss: 0.8262899518013\n",
      "Validation loss: 0.4349671411514282\n",
      "F1-Score: 0.8778\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.292889595031738\n",
      "Epoch: 0  Step: 10  Loss: 1.2103261947631836\n",
      "Epoch: 1  Step: 0  Loss: 1.041152834892273\n",
      "Epoch: 1  Step: 10  Loss: 0.8123900890350342\n",
      "Epoch: 2  Step: 0  Loss: 0.7839919328689575\n",
      "Epoch: 2  Step: 10  Loss: 0.749350368976593\n",
      "Epoch: 3  Step: 0  Loss: 0.7184507846832275\n",
      "Epoch: 3  Step: 10  Loss: 0.7082673907279968\n",
      "Validation loss: 0.4390988880395889\n",
      "F1-Score: 0.8768\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.310847282409668\n",
      "Epoch: 0  Step: 10  Loss: 1.0801191329956055\n",
      "Epoch: 1  Step: 0  Loss: 1.0795520544052124\n",
      "Epoch: 1  Step: 10  Loss: 0.9931679368019104\n",
      "Epoch: 2  Step: 0  Loss: 0.9597789645195007\n",
      "Epoch: 2  Step: 10  Loss: 0.7461528778076172\n",
      "Epoch: 3  Step: 0  Loss: 0.8485317826271057\n",
      "Epoch: 3  Step: 10  Loss: 0.6940948963165283\n",
      "Validation loss: 0.4332398724555969\n",
      "F1-Score: 0.8774\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.118152618408203\n",
      "Epoch: 0  Step: 10  Loss: 1.1412197351455688\n",
      "Epoch: 1  Step: 0  Loss: 0.988243579864502\n",
      "Epoch: 1  Step: 10  Loss: 0.9707333445549011\n",
      "Epoch: 2  Step: 0  Loss: 0.9084277749061584\n",
      "Epoch: 2  Step: 10  Loss: 0.9630613327026367\n",
      "Epoch: 3  Step: 0  Loss: 0.8323900699615479\n",
      "Epoch: 3  Step: 10  Loss: 0.8386963605880737\n",
      "Validation loss: 0.43899447679519654\n",
      "F1-Score: 0.8744\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.389832496643066\n",
      "Epoch: 0  Step: 10  Loss: 1.3396880626678467\n",
      "Epoch: 1  Step: 0  Loss: 1.113393783569336\n",
      "Epoch: 1  Step: 10  Loss: 0.8390498757362366\n",
      "Epoch: 2  Step: 0  Loss: 0.8431789875030518\n",
      "Epoch: 2  Step: 10  Loss: 0.9121683239936829\n",
      "Epoch: 3  Step: 0  Loss: 0.7702758312225342\n",
      "Epoch: 3  Step: 10  Loss: 0.7575777769088745\n",
      "Validation loss: 0.4201903903484345\n",
      "F1-Score: 0.8798\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.924657344818115\n",
      "Epoch: 0  Step: 10  Loss: 1.3602311611175537\n",
      "Epoch: 1  Step: 0  Loss: 1.0277396440505981\n",
      "Epoch: 1  Step: 10  Loss: 0.956778883934021\n",
      "Epoch: 2  Step: 0  Loss: 0.8131201863288879\n",
      "Epoch: 2  Step: 10  Loss: 0.7568610906600952\n",
      "Epoch: 3  Step: 0  Loss: 0.7396928071975708\n",
      "Epoch: 3  Step: 10  Loss: 0.7435135841369629\n",
      "Validation loss: 0.4272695505619049\n",
      "F1-Score: 0.8806\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.583527088165283\n",
      "Epoch: 0  Step: 10  Loss: 1.2721060514450073\n",
      "Epoch: 1  Step: 0  Loss: 1.1240758895874023\n",
      "Epoch: 1  Step: 10  Loss: 1.0284817218780518\n",
      "Epoch: 2  Step: 0  Loss: 0.8109520673751831\n",
      "Epoch: 2  Step: 10  Loss: 0.8094910383224487\n",
      "Epoch: 3  Step: 0  Loss: 0.6855038404464722\n",
      "Epoch: 3  Step: 10  Loss: 0.7719545364379883\n",
      "Validation loss: 0.446859050989151\n",
      "F1-Score: 0.8806\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.776584625244141\n",
      "Epoch: 0  Step: 10  Loss: 1.2082741260528564\n",
      "Epoch: 1  Step: 0  Loss: 0.9836190938949585\n",
      "Epoch: 1  Step: 10  Loss: 0.9006124138832092\n",
      "Epoch: 2  Step: 0  Loss: 0.808418869972229\n",
      "Epoch: 2  Step: 10  Loss: 0.8538410067558289\n",
      "Epoch: 3  Step: 0  Loss: 0.7500112056732178\n",
      "Epoch: 3  Step: 10  Loss: 0.8128361701965332\n",
      "Validation loss: 0.42175318717956545\n",
      "F1-Score: 0.8794\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.797215461730957\n",
      "Epoch: 0  Step: 10  Loss: 1.2746272087097168\n",
      "Epoch: 1  Step: 0  Loss: 1.0525991916656494\n",
      "Epoch: 1  Step: 10  Loss: 0.9829685688018799\n",
      "Epoch: 2  Step: 0  Loss: 0.8864978551864624\n",
      "Epoch: 2  Step: 10  Loss: 0.8328107595443726\n",
      "Epoch: 3  Step: 0  Loss: 0.8353455066680908\n",
      "Epoch: 3  Step: 10  Loss: 0.7419294118881226\n",
      "Validation loss: 0.43624906837940214\n",
      "F1-Score: 0.881\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 5.730953216552734\n",
      "Epoch: 0  Step: 10  Loss: 1.270866870880127\n",
      "Epoch: 1  Step: 0  Loss: 1.082711935043335\n",
      "Epoch: 1  Step: 10  Loss: 0.9416139721870422\n",
      "Epoch: 2  Step: 0  Loss: 0.8713295459747314\n",
      "Epoch: 2  Step: 10  Loss: 0.8191078901290894\n",
      "Epoch: 3  Step: 0  Loss: 0.8366246819496155\n",
      "Epoch: 3  Step: 10  Loss: 0.7673459053039551\n",
      "Validation loss: 0.44756336450576784\n",
      "F1-Score: 0.8754\n",
      "Training run 1 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.3316192626953125\n",
      "Epoch: 0  Step: 10  Loss: 1.38814115524292\n",
      "Epoch: 1  Step: 0  Loss: 1.0534669160842896\n",
      "Epoch: 1  Step: 10  Loss: 0.890581488609314\n",
      "Epoch: 2  Step: 0  Loss: 0.788931131362915\n",
      "Epoch: 2  Step: 10  Loss: 0.8796412348747253\n",
      "Epoch: 3  Step: 0  Loss: 0.7575341463088989\n",
      "Epoch: 3  Step: 10  Loss: 0.670474648475647\n",
      "Validation loss: 0.4485188257694244\n",
      "F1-Score: 0.881\n",
      "Training run 2 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.096582412719727\n",
      "Epoch: 0  Step: 10  Loss: 1.3446786403656006\n",
      "Epoch: 1  Step: 0  Loss: 1.010474681854248\n",
      "Epoch: 1  Step: 10  Loss: 0.8628054857254028\n",
      "Epoch: 2  Step: 0  Loss: 0.8140055537223816\n",
      "Epoch: 2  Step: 10  Loss: 0.736637532711029\n",
      "Epoch: 3  Step: 0  Loss: 0.7375506162643433\n",
      "Epoch: 3  Step: 10  Loss: 0.7885117530822754\n",
      "Validation loss: 0.4440004098415375\n",
      "F1-Score: 0.8782\n",
      "Training run 3 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.4991278648376465\n",
      "Epoch: 0  Step: 10  Loss: 1.124345064163208\n",
      "Epoch: 1  Step: 0  Loss: 1.088498830795288\n",
      "Epoch: 1  Step: 10  Loss: 0.8322250843048096\n",
      "Epoch: 2  Step: 0  Loss: 1.0093187093734741\n",
      "Epoch: 2  Step: 10  Loss: 0.6772608757019043\n",
      "Epoch: 3  Step: 0  Loss: 0.8110606074333191\n",
      "Epoch: 3  Step: 10  Loss: 0.6481634378433228\n",
      "Validation loss: 0.4550692629814148\n",
      "F1-Score: 0.8684\n",
      "Training run 4 completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 6.207277297973633\n",
      "Epoch: 0  Step: 10  Loss: 1.1326942443847656\n",
      "Epoch: 1  Step: 0  Loss: 1.0481551885604858\n",
      "Epoch: 1  Step: 10  Loss: 0.9503096342086792\n",
      "Epoch: 2  Step: 0  Loss: 0.9141451120376587\n",
      "Epoch: 2  Step: 10  Loss: 0.9123660326004028\n",
      "Epoch: 3  Step: 0  Loss: 0.6639115810394287\n",
      "Epoch: 3  Step: 10  Loss: 0.7573761343955994\n",
      "Validation loss: 0.43077939689159395\n",
      "F1-Score: 0.876\n",
      "Training run 5 completed.\n",
      "\n",
      "Logging model stats....\n",
      "\n",
      "Total Training/Eval time: 1061.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 5e-05\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for model_path in model_paths:\n",
    "    \n",
    "    tokenizer_path = ('bert-base-uncased' if model_path == 'bert-base-uncased'\\\n",
    "                  else '../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    model_name = model_path if model_path == 'bert-base-uncased' else model_path.split('/')[-2].split('.')[0]\n",
    "    model_stats = {'model_name':model_name,\n",
    "                   'seeds':[],\n",
    "                   'batch_size':TRAIN_BATCH_SIZE,\n",
    "                   'epochs':EPOCHS,\n",
    "                   'metric':'f1-score (micro)',\n",
    "                   'scores': [],\n",
    "                   'mean_score':0\n",
    "                    }\n",
    "    for num in range(1,6):\n",
    "        \n",
    "        training_set = CustomDataset(tokenizer, train_sentences, train_labels, MAX_LEN)\n",
    "        #valid_set = CustomDataset(tokenizer, valid_sentences, valid_labels, MAX_LEN)\n",
    "        test_set = CustomDataset(tokenizer, test_sentences, test_labels, MAX_LEN)\n",
    "        \n",
    "        training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "        #valid_loader = DataLoader(valid_set, batch_size=VALID_BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "        test_loader = DataLoader(test_set, batch_size=VALID_BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "        \n",
    "        model = BERTClass(model_path)\n",
    "        model = DataParallel(model)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        print(f'Using model {model_name}, with tokenizer {tokenizer_path}')\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            train(epoch)\n",
    "        pred_tags, test_tags, score = valid(model, test_loader, label_vals)\n",
    "        model_stats['scores'].append(round(score, 6))\n",
    "        #pred_tags, valid_tags = valid(model, valid_loader, label_vals)\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(3)\n",
    "        print(f'Training run {num} completed.')\n",
    "        print()\n",
    "        \n",
    "    print('Logging model stats....')\n",
    "    print()\n",
    "    final_score = np.round(np.mean(model_stats['scores']), 4)\n",
    "    model_stats['mean_score'] = final_score\n",
    "    with open('logs/NER_stats.txt', 'a') as f:\n",
    "        f.write(json.dumps(model_stats))\n",
    "        f.write('\\n')\n",
    "        \n",
    "end = time.perf_counter() - start\n",
    "print(f'Total Training/Eval time: {round(end, 2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
