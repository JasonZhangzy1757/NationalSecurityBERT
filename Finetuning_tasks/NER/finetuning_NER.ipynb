{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QoB2SD6tWjM",
    "outputId": "da7f454a-ed25-493e-dd38-552e13377a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8-Br7SNtbkC",
    "outputId": "fcb8b4e5-5bc7-4312-e587-312e5315c5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 4.3 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 4.8 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 44.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 25.7 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 47.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T21:07:26.875722Z",
     "iopub.status.busy": "2022-03-26T21:07:26.875480Z",
     "iopub.status.idle": "2022-03-26T21:07:28.146258Z",
     "shell.execute_reply": "2022-03-26T21:07:28.144442Z",
     "shell.execute_reply.started": "2022-03-26T21:07:26.875694Z"
    },
    "id": "NYUqKiOZdR1H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T21:10:02.170262Z",
     "iopub.status.busy": "2022-03-26T21:10:02.169957Z",
     "iopub.status.idle": "2022-03-26T21:10:06.615648Z",
     "shell.execute_reply": "2022-03-26T21:10:06.614490Z",
     "shell.execute_reply.started": "2022-03-26T21:10:02.170228Z"
    },
    "id": "sdqhHeAqcLnO"
   },
   "outputs": [],
   "source": [
    "# Process the data\n",
    "def def_value():\n",
    "    return 'O'\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        sent_dict = {}\n",
    "        label_dict = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line.isspace():\n",
    "                continue\n",
    "            if '|' in line and len(line.split('|')) == 3 and (line.split('|')[1] == 'a' or line.split('|')[1] == 't'):\n",
    "                idx, _, sentence = line.split('|')\n",
    "                sent_dict[idx] = sent_dict.get(idx, '') + ' ' + sentence\n",
    "            else:\n",
    "                idx, start_pos, end_pos, word, label, _ = line.split('\\t')\n",
    "                if idx not in label_dict:\n",
    "                    label_dict[idx] = defaultdict(def_value)\n",
    "                    for i in range(int(start_pos), int(end_pos)):\n",
    "                        label_dict[idx][i] = label\n",
    "                else:\n",
    "                    for i in range(int(start_pos), int(end_pos)):\n",
    "                        label_dict[idx][i] = label\n",
    "\n",
    "    idx_col, word_col, label_col = [], [], []\n",
    "    for idx in sent_dict:\n",
    "        sentence = sent_dict[idx].replace('\\n', '')\n",
    "\n",
    "        char_seq = 0\n",
    "        for word in sentence.split(' ')[1:]:\n",
    "            label = label_dict[idx][char_seq]\n",
    "            if word and word[0] == '(':\n",
    "                label = label_dict[idx][char_seq + 1]\n",
    "            char_seq += len(word) + 1\n",
    "\n",
    "            idx_col.append(idx)\n",
    "            word_col.append(word)\n",
    "            label_col.append(label)\n",
    "\n",
    "    df = pd.DataFrame(list(zip(idx_col, word_col, label_col)),\n",
    "               columns =['sentence_id', 'word', 'label'])\n",
    "    return df\n",
    "\n",
    "\n",
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda x: [(w, t) for w, t in zip(x[\"word\"].values.tolist(),\n",
    "                                                        x[\"label\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            sentence = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return sentence\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "# Creating new lists and dicts that will be used at a later stage for reference and processing\n",
    "def get_data(df, label_vals):\n",
    "    getter = SentenceGetter(df)\n",
    "    label2idx = {value: key for key, value in enumerate(label_vals)}\n",
    "    sentences = [' '.join([s[0] for s in sentence]) for sentence in getter.sentences]\n",
    "    labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
    "    labels = [[label2idx.get(l) for l in label] for label in labels]\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "8aQ6WCk2a-Vd"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len):\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = str(self.sentences[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        label = self.labels[index]\n",
    "        label.extend([4]*200)\n",
    "        label=label[:200]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'tags': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert = transformers.BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask, labels):\n",
    "        output = self.bert(ids, mask, labels = labels)\n",
    "\n",
    "        return output\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for step, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['tags'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss = model(ids, mask, labels = targets)[0]\n",
    "\n",
    "        if step % 5==0:\n",
    "            print(f'Epoch: {epoch}  Step: {step}  Loss: {loss.item()}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def valid(model, testing_loader, label_vals):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    predictions , true_labels = [], []\n",
    "    nb_eval_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['tags'].to(device, dtype = torch.long)\n",
    "\n",
    "            output = model(ids, mask, labels=targets)\n",
    "            loss, logits = output[:2]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = targets.to('cpu').numpy()\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "            true_labels.append(label_ids)\n",
    "            eval_loss += loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "        eval_loss = eval_loss/nb_eval_steps\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "        pred_tags = [label_vals[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [label_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "        print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags, average='micro')))\n",
    "    return pred_tags, valid_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "mEJxhvwhtSs1"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "df_train = read_data('./drive/MyDrive/qa_finetuning/NCBI-disease/NCBItrainset_corpus.txt')\n",
    "df_valid = read_data('./drive/MyDrive/qa_finetuning/NCBI-disease/NCBIdevelopset_corpus.txt')\n",
    "\n",
    "label_vals = list(df_train[\"label\"].value_counts().keys())\n",
    "label2idx = {value: key for key, value in enumerate(label_vals)}\n",
    "\n",
    "train_sentences, train_labels = get_data(df_train, label_vals)\n",
    "valid_sentences, valid_labels = get_data(df_valid, label_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50oMGTe0bvl0",
    "outputId": "c6f7bade-d303-476e-ecc2-ec8178895dbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step: 0  Loss: 3.1176812648773193\n",
      "Epoch: 0  Step: 5  Loss: 0.9472585320472717\n",
      "Epoch: 0  Step: 10  Loss: 0.9107089042663574\n",
      "Epoch: 0  Step: 15  Loss: 0.8591963052749634\n",
      "Epoch: 1  Step: 0  Loss: 0.6417899131774902\n",
      "Epoch: 1  Step: 5  Loss: 0.624437153339386\n",
      "Epoch: 1  Step: 10  Loss: 0.5715211033821106\n",
      "Epoch: 1  Step: 15  Loss: 0.6045364141464233\n",
      "Epoch: 2  Step: 0  Loss: 0.4701229929924011\n",
      "Epoch: 2  Step: 5  Loss: 0.5876104831695557\n",
      "Epoch: 2  Step: 10  Loss: 0.5174559950828552\n",
      "Epoch: 2  Step: 15  Loss: 0.4936816692352295\n",
      "Validation loss: 0.4946334731578827\n",
      "F1-Score: 0.85655\n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_set = CustomDataset(tokenizer, train_sentences, train_labels, MAX_LEN)\n",
    "valid_set = CustomDataset(tokenizer, valid_sentences, valid_labels, MAX_LEN)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = BERTClass().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "pred_tags, valid_tags = valid(model, valid_loader, label_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGmXhswimTG3",
    "outputId": "2ed38463-f39f-43fa-91d1-2e7007cd4dfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4798848079265775\n",
      "F1-Score: 0.8671537162162162\n"
     ]
    }
   ],
   "source": [
    "df_test = read_data('./drive/MyDrive/qa_finetuning/NCBI-disease/NCBItestset_corpus.txt')\n",
    "test_sentences, test_labels = get_data(df_train, label_vals)\n",
    "test_set = CustomDataset(tokenizer, test_sentences, test_labels, MAX_LEN)\n",
    "test_loader = DataLoader(test_set, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "_, _ = valid(model, test_loader, label_vals)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
