{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCFsXkJ66PA-",
    "outputId": "d4889b3f-fd4a-4957-9167-afb027f11d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NaK6P0W6XN2",
    "outputId": "99b89e67-a1cb-4713-9646-5717bc027a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 15.8 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 55.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Downloading tokenizers-0.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 48.3 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 6.4 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 60.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.0 transformers-4.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ViMPaSuC6Cht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzDFqyLS6Chz"
   },
   "source": [
    "### Change the path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L-WVJtbi6Ch1"
   },
   "outputs": [],
   "source": [
    "training_filepath = '../../Data/BioASQ_data/BioASQ-training7b/trainining7b.json'\n",
    "test_directory = '../../Data/BioASQ_data/Task7BGoldenEnriched/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oAkv4Hzo6Ch2"
   },
   "outputs": [],
   "source": [
    "training_text = []\n",
    "test_text = []\n",
    "\n",
    "# Filter out the training data\n",
    "with open (training_filepath, \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "    texts = data['questions']\n",
    "    for text in texts:\n",
    "        if 'exact_answer' in text.keys():\n",
    "            if text['exact_answer'] == 'yes' or text['exact_answer'] == 'no':\n",
    "                training_text.append(text)\n",
    "\n",
    "# Filter out the text data\n",
    "directory = test_directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open (directory+filename, \"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        texts = data['questions']\n",
    "        for text in texts:\n",
    "            if 'exact_answer' in text.keys():\n",
    "                if text['exact_answer'] == 'yes' or text['exact_answer'] == 'no':\n",
    "                    test_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R2Sk43KZ6Ch3"
   },
   "outputs": [],
   "source": [
    "def process_data(texts):\n",
    "    question_list = []\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    for text in texts:\n",
    "        question_list.append(text['body'])\n",
    "        context_list.append(' '. join([x['text'] for x in text['snippets']]))\n",
    "        target_list.append(1 if text['exact_answer'] == 'yes' else 0)\n",
    "    df = pd.DataFrame(zip(question_list, context_list, target_list), columns=['question', 'context', 'target'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R2m6PFBk6Ch6"
   },
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB')\n",
    "        self.out = nn.Linear(768, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self, question, context, target, tokenizer, max_len):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.target = target\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        question= str(self.question[item])\n",
    "        context = str(self.context[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        padding_len = self.max_len - len(ids)\n",
    "        \n",
    "        ids = ids[:self.max_len] + ([0] * padding_len) \n",
    "        token_type_ids = token_type_ids[:self.max_len] + ([0] * padding_len)\n",
    "        mask = mask[:self.max_len] + ([0] * padding_len)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.target[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def loss_fn(outputs, target):\n",
    "    return nn.CrossEntropyLoss()(outputs, target)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        target = d['target']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        target = target.to(device, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 10 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            \n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            ids = d['ids'].to(device, dtype=torch.long)\n",
    "            mask = d['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "            target = d['target'].to(device, dtype=torch.long)\n",
    "          \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "          \n",
    "            fin_targets.append(target.cpu().detach().numpy())\n",
    "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.hstack(fin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A56vzcaR6Ch8",
    "outputId": "48204d53-3848-4e25-9374-778cfaa95364",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', vocab_size=30500, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.7468404769897461\n",
      "bi=10, loss=0.8125408291816711\n",
      "bi=20, loss=0.5632827877998352\n",
      "bi=30, loss=0.43850085139274597\n",
      "bi=40, loss=0.6881063580513\n",
      "bi=50, loss=0.4384167790412903\n",
      "bi=60, loss=0.43840497732162476\n",
      "bi=70, loss=0.4384075701236725\n",
      "bi=80, loss=0.438368558883667\n",
      "bi=90, loss=0.43836554884910583\n",
      "epoch: 0, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.5632815361022949\n",
      "bi=10, loss=0.6881755590438843\n",
      "bi=20, loss=0.5632649064064026\n",
      "bi=30, loss=0.563271701335907\n",
      "bi=40, loss=0.31340697407722473\n",
      "bi=50, loss=0.4383338987827301\n",
      "bi=60, loss=0.4383391737937927\n",
      "bi=70, loss=0.6881988644599915\n",
      "bi=80, loss=0.31339719891548157\n",
      "bi=90, loss=0.8131579160690308\n",
      "epoch: 1, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.6882166266441345\n",
      "bi=10, loss=0.8131458759307861\n",
      "bi=20, loss=0.43832942843437195\n",
      "bi=30, loss=0.6882184743881226\n",
      "bi=40, loss=0.31337735056877136\n",
      "bi=50, loss=0.43832165002822876\n",
      "bi=60, loss=0.5632729530334473\n",
      "bi=70, loss=0.4383278489112854\n",
      "bi=80, loss=0.4383200705051422\n",
      "bi=90, loss=0.3133712410926819\n",
      "epoch: 2, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.4383048117160797\n",
      "bi=10, loss=0.4383155107498169\n",
      "bi=20, loss=0.438313364982605\n",
      "bi=30, loss=0.4383147954940796\n",
      "bi=40, loss=0.5632556080818176\n",
      "bi=50, loss=0.43832096457481384\n",
      "bi=60, loss=0.5632621645927429\n",
      "bi=70, loss=0.43831968307495117\n",
      "bi=80, loss=0.5632675886154175\n",
      "bi=90, loss=0.6882258653640747\n",
      "epoch: 3, acc: 0.6714285714285714\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 4\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 3e-5\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "print(tokenizer)\n",
    "model = BERTBaseUncased().to(device)\n",
    "\n",
    "train_df = process_data(training_text)\n",
    "test_df = process_data(test_text)\n",
    "\n",
    "train_dataset = BERTDatasetTraining(\n",
    "    question=train_df.question.values,\n",
    "    context=train_df.context.values,\n",
    "    target=train_df.target.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = BERTDatasetTraining(\n",
    "    question=test_df.question.values,\n",
    "    context=test_df.context.values,\n",
    "    target=test_df.target.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "    output, target = eval_loop_fn(test_data_loader, model, device)\n",
    "    acc = (output.argmax(1) == target).sum() / len(target)\n",
    "    print(f'epoch: {epoch}, acc: {acc}')\n",
    "    # print(output.tolist())\n",
    "    # print(target.tolist())\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, target = eval_loop_fn(test_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1sImHvP6-fmM"
   },
   "outputs": [],
   "source": [
    "acc = (output.argmax(1) == target).sum() / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6714285714285714"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning_QA_BioASQ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
