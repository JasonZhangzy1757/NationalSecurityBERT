{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ViMPaSuC6Cht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzDFqyLS6Chz"
   },
   "source": [
    "### Change the path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L-WVJtbi6Ch1"
   },
   "outputs": [],
   "source": [
    "training_filepath = '../../Data/BioASQ_data/BioASQ-training7b/trainining7b.json'\n",
    "test_directory = '../../Data/BioASQ_data/Task7BGoldenEnriched/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oAkv4Hzo6Ch2"
   },
   "outputs": [],
   "source": [
    "training_text = []\n",
    "test_text = []\n",
    "\n",
    "# Filter out the training data\n",
    "with open (training_filepath, \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "    texts = data['questions']\n",
    "    for text in texts:\n",
    "        if 'exact_answer' in text.keys():\n",
    "            if text['exact_answer'] == 'yes' or text['exact_answer'] == 'no':\n",
    "                training_text.append(text)\n",
    "\n",
    "# Filter out the text data\n",
    "directory = test_directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open (directory+filename, \"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        texts = data['questions']\n",
    "        for text in texts:\n",
    "            if 'exact_answer' in text.keys():\n",
    "                if text['exact_answer'] == 'yes' or text['exact_answer'] == 'no':\n",
    "                    test_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R2Sk43KZ6Ch3"
   },
   "outputs": [],
   "source": [
    "def process_data(texts):\n",
    "    question_list = []\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    for text in texts:\n",
    "        question_list.append(text['body'])\n",
    "        context_list.append(' '. join([x['text'] for x in text['snippets']]))\n",
    "        target_list.append(1 if text['exact_answer'] == 'yes' else 0)\n",
    "    df = pd.DataFrame(zip(question_list, context_list, target_list), columns=['question', 'context', 'target'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R2m6PFBk6Ch6"
   },
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.out = nn.Linear(768, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self, question, context, target, tokenizer, max_len):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.target = target\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        question= str(self.question[item])\n",
    "        context = str(self.context[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        padding_len = self.max_len - len(ids)\n",
    "        \n",
    "        ids = ids[:self.max_len] + ([0] * padding_len) \n",
    "        token_type_ids = token_type_ids[:self.max_len] + ([0] * padding_len)\n",
    "        mask = mask[:self.max_len] + ([0] * padding_len)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.target[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def loss_fn(outputs, target):\n",
    "    return nn.CrossEntropyLoss()(outputs, target)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        target = d['target']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        target = target.to(device, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 20 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            \n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            ids = d['ids'].to(device, dtype=torch.long)\n",
    "            mask = d['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "            target = d['target'].to(device, dtype=torch.long)\n",
    "          \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "          \n",
    "            fin_targets.append(target.cpu().detach().numpy())\n",
    "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.hstack(fin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A56vzcaR6Ch8",
    "outputId": "48204d53-3848-4e25-9374-778cfaa95364"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=0.6000276803970337\n",
      "bi=20, loss=0.6239089369773865\n",
      "bi=40, loss=0.5026217699050903\n",
      "epoch: 0, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.5018689036369324\n",
      "bi=20, loss=0.4391683340072632\n",
      "bi=40, loss=0.4387642443180084\n",
      "epoch: 1, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.625644326210022\n",
      "bi=20, loss=0.5630885362625122\n",
      "bi=40, loss=0.5008623003959656\n",
      "epoch: 2, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.5632413625717163\n",
      "bi=20, loss=0.438466340303421\n",
      "bi=40, loss=0.5633181929588318\n",
      "epoch: 3, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.37593668699264526\n",
      "bi=20, loss=0.4383500814437866\n",
      "bi=40, loss=0.6254837512969971\n",
      "epoch: 4, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.37587085366249084\n",
      "bi=20, loss=0.5630156397819519\n",
      "bi=40, loss=0.5008226633071899\n",
      "epoch: 5, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.562738299369812\n",
      "bi=20, loss=0.3137112855911255\n",
      "bi=40, loss=0.501624584197998\n",
      "epoch: 6, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.5008941888809204\n",
      "bi=20, loss=0.5106114745140076\n",
      "bi=40, loss=0.3277122378349304\n",
      "epoch: 7, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.4382147192955017\n",
      "bi=20, loss=0.3761994540691376\n",
      "bi=40, loss=0.38352689146995544\n",
      "epoch: 8, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.3763241469860077\n",
      "bi=20, loss=0.43850937485694885\n",
      "bi=40, loss=0.4563370645046234\n",
      "epoch: 9, acc: 0.6857142857142857\n",
      "---------------------\n",
      "bi=0, loss=0.4797515571117401\n",
      "bi=20, loss=0.46967437863349915\n",
      "bi=40, loss=0.6164017915725708\n",
      "epoch: 10, acc: 0.6785714285714286\n",
      "---------------------\n",
      "bi=0, loss=0.5009040236473083\n",
      "bi=20, loss=0.4790712296962738\n",
      "bi=40, loss=0.3758537471294403\n",
      "epoch: 11, acc: 0.7071428571428572\n",
      "---------------------\n",
      "bi=0, loss=0.561983048915863\n",
      "bi=20, loss=0.376653254032135\n",
      "bi=40, loss=0.3258517384529114\n",
      "epoch: 12, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.3759995400905609\n",
      "bi=20, loss=0.377633273601532\n",
      "bi=40, loss=0.31349778175354004\n",
      "epoch: 13, acc: 0.6714285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.4384945034980774\n",
      "bi=20, loss=0.4800095558166504\n",
      "bi=40, loss=0.3758837878704071\n",
      "epoch: 14, acc: 0.7357142857142858\n",
      "---------------------\n",
      "bi=0, loss=0.4330221116542816\n",
      "bi=20, loss=0.37661388516426086\n",
      "bi=40, loss=0.3134670853614807\n",
      "epoch: 15, acc: 0.7142857142857143\n",
      "---------------------\n",
      "bi=0, loss=0.37595054507255554\n",
      "bi=20, loss=0.43629512190818787\n",
      "bi=40, loss=0.3821256458759308\n",
      "epoch: 16, acc: 0.7214285714285714\n",
      "---------------------\n",
      "bi=0, loss=0.4385843575000763\n",
      "bi=20, loss=0.4962468147277832\n",
      "bi=40, loss=0.4384715259075165\n",
      "epoch: 17, acc: 0.7285714285714285\n",
      "---------------------\n",
      "bi=0, loss=0.43842843174934387\n",
      "bi=20, loss=0.5479727983474731\n",
      "bi=40, loss=0.43916428089141846\n",
      "epoch: 18, acc: 0.7285714285714285\n",
      "---------------------\n",
      "bi=0, loss=0.3134186565876007\n",
      "bi=20, loss=0.375907838344574\n",
      "bi=40, loss=0.3762422800064087\n",
      "epoch: 19, acc: 0.7428571428571429\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 4\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-5\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BERTBaseUncased().to(device)\n",
    "\n",
    "train_df = process_data(training_text)\n",
    "test_df = process_data(test_text)\n",
    "\n",
    "train_dataset = BERTDatasetTraining(\n",
    "    question=train_df.question.values,\n",
    "    context=train_df.context.values,\n",
    "    target=train_df.target.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = BERTDatasetTraining(\n",
    "    question=test_df.question.values,\n",
    "    context=test_df.context.values,\n",
    "    target=test_df.target.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "    output, target = eval_loop_fn(test_data_loader, model, device)\n",
    "    acc = (output.argmax(1) == target).sum() / len(target)\n",
    "    print(f'epoch: {epoch}, acc: {acc}')\n",
    "    # print(output.tolist())\n",
    "    # print(target.tolist())\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1sImHvP6-fmM"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning_QA_BioASQ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
