{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UP_o6WUTwZYT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.parallel import DataParallel\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import json\n",
    "import os\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t95gTIGewZYY"
   },
   "outputs": [],
   "source": [
    "class ChemProtDataset:\n",
    "    def __init__(self, tokenizer, sentence, label, max_len, subj, obj):\n",
    "        self.sentence = sentence\n",
    "        self.subj = subj\n",
    "        self.obj = obj\n",
    "        self.label = label\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentence[item])\n",
    "        subj = str(self.subj[item])\n",
    "        obj = str(self.obj[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            subj + \" \" + obj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'label': torch.tensor(self.label[item], dtype=torch.long),\n",
    "\n",
    "        } \n",
    "    \n",
    "class REModel(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(REModel, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(model_path)\n",
    "        self.hidden_size = 768\n",
    "        self.out = nn.Linear(self.hidden_size, 5)\n",
    "        self.softmax = nn.Softmax(dim=1)    \n",
    "\n",
    "            \n",
    "    def forward(self, ids, mask):\n",
    "        _, outputs = self.bert(ids, attention_mask=mask, return_dict=False)\n",
    "        return self.softmax(outputs)\n",
    "    \n",
    "    \n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        #token_type_ids = d['token_type_ids']\n",
    "        label = d['label']\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        #token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        label = label.to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask)\n",
    "\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 50 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "\n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_labels = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            ids = d['ids'].to(device, dtype=torch.long)\n",
    "            mask = d['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "            label = d['label'].to(device, dtype=torch.long)\n",
    "          \n",
    "            outputs = model(ids, mask)\n",
    "          \n",
    "            fin_labels.append(label.cpu().detach().numpy())\n",
    "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.hstack(fin_labels)\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        result = []\n",
    "        for line in f:\n",
    "            res = json.loads(line)\n",
    "            text = res['text']\n",
    "            subj = text[text.find(\"<<\")+2:text.find(\">>\")]\n",
    "            obj = text[text.find(\"[[\")+12:text.find(\"]]\")]\n",
    "            \n",
    "            res['subj'], res['obj'] = subj, obj\n",
    "            res['label'] = LABEL_DICT[res['label']]\n",
    "            result.append(res)\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFpuqTvNwZYe",
    "outputId": "a9feac09-e954-4442-deb0-a497634108a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these GPUs: [0, 1], with this model: bert-base-uncased\n",
      "bi=0, loss=6.644195556640625\n",
      "bi=50, loss=6.639502048492432\n",
      "bi=100, loss=6.637100696563721\n",
      "bi=150, loss=6.636380672454834\n",
      "bi=200, loss=6.63617467880249\n",
      "bi=250, loss=6.636139392852783\n",
      "bi=0, loss=6.636024475097656\n",
      "bi=50, loss=6.635951519012451\n",
      "bi=100, loss=6.6359100341796875\n",
      "bi=150, loss=6.6358747482299805\n",
      "bi=200, loss=6.635826587677002\n",
      "bi=250, loss=6.635806560516357\n",
      "bi=0, loss=6.635817050933838\n",
      "bi=50, loss=6.635818958282471\n",
      "bi=100, loss=6.635760307312012\n",
      "bi=150, loss=6.635757923126221\n",
      "bi=200, loss=6.635786056518555\n",
      "bi=250, loss=6.635754108428955\n",
      "bi=0, loss=6.635740280151367\n",
      "bi=50, loss=6.635761737823486\n",
      "bi=100, loss=6.635734558105469\n",
      "bi=150, loss=6.635743618011475\n",
      "bi=200, loss=6.635753631591797\n",
      "bi=250, loss=6.635738849639893\n",
      "bi=0, loss=6.635749340057373\n",
      "bi=50, loss=6.635726451873779\n",
      "bi=100, loss=6.635745525360107\n",
      "bi=150, loss=6.635745048522949\n",
      "bi=200, loss=6.635732173919678\n",
      "bi=250, loss=6.6357197761535645\n",
      "bi=0, loss=6.635720252990723\n",
      "bi=50, loss=6.6357316970825195\n",
      "bi=100, loss=6.635753631591797\n",
      "bi=150, loss=6.635739803314209\n",
      "bi=200, loss=6.635729789733887\n",
      "bi=250, loss=6.635740280151367\n",
      "bi=0, loss=6.63572883605957\n",
      "bi=50, loss=6.635727882385254\n",
      "bi=100, loss=6.6357269287109375\n",
      "bi=150, loss=6.635739326477051\n",
      "bi=200, loss=6.635725975036621\n",
      "bi=250, loss=6.635714054107666\n",
      "bi=0, loss=6.635737419128418\n",
      "bi=50, loss=6.635713577270508\n",
      "bi=100, loss=6.63573694229126\n",
      "bi=150, loss=6.635734558105469\n",
      "bi=200, loss=6.635734558105469\n",
      "bi=250, loss=6.635712146759033\n",
      "bi=0, loss=6.635745048522949\n",
      "bi=50, loss=6.635712146759033\n",
      "bi=100, loss=6.635756015777588\n",
      "bi=150, loss=6.635722637176514\n",
      "bi=200, loss=6.635712146759033\n",
      "bi=250, loss=6.635722637176514\n",
      "bi=0, loss=6.635743618011475\n",
      "bi=50, loss=6.6357221603393555\n",
      "bi=100, loss=6.6357221603393555\n",
      "bi=150, loss=6.635753154754639\n",
      "bi=200, loss=6.635711669921875\n",
      "bi=250, loss=6.635743141174316\n",
      "bi=0, loss=6.635711669921875\n",
      "bi=50, loss=6.635711193084717\n",
      "bi=100, loss=6.635732173919678\n",
      "bi=150, loss=6.6357316970825195\n",
      "bi=200, loss=6.6357526779174805\n",
      "bi=250, loss=6.635711669921875\n",
      "bi=0, loss=6.635753631591797\n",
      "bi=50, loss=6.635752201080322\n",
      "bi=100, loss=6.6357421875\n",
      "bi=150, loss=6.635725021362305\n",
      "bi=200, loss=6.635723114013672\n",
      "bi=250, loss=6.635730743408203\n"
     ]
    }
   ],
   "source": [
    "LABEL_DICT = {'UPREGULATOR': 0, 'ACTIVATOR': 0, 'INDIRECT-UPREGULATOR': 0,\n",
    "              'DOWNREGULATOR': 1, 'INHIBITOR': 1, 'INDIRECT-DOWNREGULATOR': 1,\n",
    "              'AGONIST': 2,'AGONIST-ACTIVATOR': 2,'AGONIST-INHIBITOR': 2,\n",
    "              'ANTAGONIST': 3, 'SUBSTRATE': 4, 'PRODUCT-OF': 4, 'SUBSTRATE_PRODUCT-OF': 4}\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EPOCHS = 12\n",
    "SEED = 20\n",
    "LEARNING_RATE = 3e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = REModel(model_path)\n",
    "model = DataParallel(model)\n",
    "model.to(device)\n",
    "print(f'Using these GPUs: {model.device_ids}, with this model: {model_path}')\n",
    "\n",
    "df_train = read_data('./data/chemprot/train.txt')\n",
    "df_test = read_data('./data/chemprot/test.txt')\n",
    "\n",
    "train_dataset = ChemProtDataset(\n",
    "    sentence=df_train.text.values,\n",
    "    label=df_train.label.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    subj = df_train.subj.values, \n",
    "    obj = df_train.obj.values\n",
    ")\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "test_dataset = ChemProtDataset(\n",
    "    sentence=df_test.text.values,\n",
    "    label=df_test.label.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    subj = df_test.subj.values, \n",
    "    obj = df_test.obj.values\n",
    ")\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "outputs, labels = eval_loop_fn(test_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPTHrG9VwZYg",
    "outputId": "5af34fad-2d63-4ed4-e65f-52c65fb225d5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "f1_score(np.argmax(outputs, axis=1), labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.49      0.66      3418\n",
      "           2       0.24      0.94      0.39        51\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.49      3469\n",
      "   macro avg       0.25      0.29      0.21      3469\n",
      "weighted avg       0.99      0.49      0.65      3469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(outputs, axis=1), labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5ewEH3Lvjzqx"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  8 13:49:53 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    38W / 250W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    36W / 250W |      0MiB / 16384MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [test_dataset[i]['label'].detach().cpu().tolist() for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1667\n",
       "0     667\n",
       "4     644\n",
       "3     293\n",
       "2     198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3462\n",
       "2       7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(outputs, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'mask', 'token_type_ids', 'label'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.encode_plus("
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "finetuning_RE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
