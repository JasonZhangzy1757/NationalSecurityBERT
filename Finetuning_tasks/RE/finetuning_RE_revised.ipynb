{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ndk8g2AweNa",
        "outputId": "c79e94f3-59fe-4cf5-dea6-64bc3f4d0937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCXTf-LqwtiI",
        "outputId": "c94eaf2c-2e86-452a-e04d-2653f283e14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 26.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 58.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UP_o6WUTwZYT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "t95gTIGewZYY"
      },
      "outputs": [],
      "source": [
        "class ChemProtDataset:\n",
        "    def __init__(self, tokenizer, sentence, label, max_len, subj, obj):\n",
        "        self.sentence = sentence\n",
        "        self.subj = subj\n",
        "        self.obj = obj\n",
        "        self.label = label\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "        \n",
        "    def __getitem__(self, item):\n",
        "        sentence = str(self.sentence[item])\n",
        "        subj = str(self.subj[item])\n",
        "        obj = str(self.obj[item])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            subj + \" \" + obj,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "        )\n",
        "        \n",
        "        ids = inputs['input_ids']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'label': torch.tensor(self.label[item], dtype=torch.long),\n",
        "\n",
        "        } \n",
        "    \n",
        "class REModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(REModel, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.hidden_size = 768\n",
        "        self.out = nn.Linear(self.hidden_size, 5)\n",
        "        self.softmax = nn.Softmax(dim=1)    \n",
        "\n",
        "            \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, outputs = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        return self.softmax(outputs)\n",
        "    \n",
        "    \n",
        "def loss_fn(outputs, targets):\n",
        "    return nn.CrossEntropyLoss()(outputs, targets)\n",
        "\n",
        "\n",
        "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        ids = d['ids']\n",
        "        mask = d['mask']\n",
        "        token_type_ids = d['token_type_ids']\n",
        "        label = d['label']\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        label = label.to(device, dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        loss = loss_fn(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        if bi % 50 == 0:\n",
        "            print(f'bi={bi}, loss={loss}')\n",
        "\n",
        "\n",
        "def eval_loop_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_labels = []\n",
        "    fin_outputs = []\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        with torch.no_grad():\n",
        "            ids = d['ids'].to(device, dtype=torch.long)\n",
        "            mask = d['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
        "            label = d['label'].to(device, dtype=torch.long)\n",
        "          \n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "          \n",
        "            fin_labels.append(label.cpu().detach().numpy())\n",
        "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "\n",
        "    return np.vstack(fin_outputs), np.hstack(fin_labels)\n",
        "\n",
        "\n",
        "def read_data(path):\n",
        "    with open(path) as f:\n",
        "        result = []\n",
        "        for line in f:\n",
        "            res = json.loads(line)\n",
        "            text = res['text']\n",
        "            subj = text[text.find(\"<<\")+2:text.find(\">>\")]\n",
        "            obj = text[text.find(\"[[\")+12:text.find(\"]]\")]\n",
        "            \n",
        "            res['subj'], res['obj'] = subj, obj\n",
        "            res['label'] = LABEL_DICT[res['label']]\n",
        "            result.append(res)\n",
        "    return pd.DataFrame(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFpuqTvNwZYe",
        "outputId": "a9feac09-e954-4442-deb0-a497634108a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi=0, loss=6.644169807434082\n",
            "bi=50, loss=6.636192798614502\n",
            "bi=100, loss=6.635920524597168\n",
            "bi=150, loss=6.635893821716309\n",
            "bi=200, loss=6.635881423950195\n",
            "bi=250, loss=6.6358747482299805\n",
            "bi=300, loss=6.635870933532715\n",
            "bi=350, loss=6.635867595672607\n",
            "bi=400, loss=6.635865688323975\n",
            "bi=450, loss=6.6358642578125\n",
            "bi=500, loss=6.635863304138184\n",
            "bi=0, loss=6.635863304138184\n",
            "bi=50, loss=6.635861873626709\n",
            "bi=100, loss=6.635860919952393\n",
            "bi=150, loss=6.635860443115234\n",
            "bi=200, loss=6.635860443115234\n",
            "bi=250, loss=6.635859966278076\n",
            "bi=300, loss=6.635859489440918\n",
            "bi=350, loss=6.635859489440918\n",
            "bi=400, loss=6.635857582092285\n",
            "bi=450, loss=6.635857582092285\n",
            "bi=500, loss=6.635857582092285\n",
            "bi=0, loss=6.635857582092285\n",
            "bi=50, loss=6.635857582092285\n",
            "bi=100, loss=6.635857582092285\n",
            "bi=150, loss=6.635857582092285\n",
            "bi=200, loss=6.635857105255127\n",
            "bi=250, loss=6.635857105255127\n",
            "bi=300, loss=6.635857105255127\n",
            "bi=350, loss=6.635857105255127\n",
            "bi=400, loss=6.635857105255127\n",
            "bi=450, loss=6.635857105255127\n",
            "bi=500, loss=6.635857105255127\n",
            "bi=0, loss=6.635857105255127\n",
            "bi=50, loss=6.635857105255127\n",
            "bi=100, loss=6.635857105255127\n",
            "bi=150, loss=6.635857105255127\n",
            "bi=200, loss=6.635857105255127\n",
            "bi=250, loss=6.635857105255127\n",
            "bi=300, loss=6.635857105255127\n",
            "bi=350, loss=6.635856628417969\n",
            "bi=400, loss=6.635856628417969\n",
            "bi=450, loss=6.635856628417969\n",
            "bi=500, loss=6.635857105255127\n"
          ]
        }
      ],
      "source": [
        "LABEL_DICT = {'UPREGULATOR': 0, 'ACTIVATOR': 0, 'INDIRECT-UPREGULATOR': 0,\n",
        "              'DOWNREGULATOR': 1, 'INHIBITOR': 1, 'INDIRECT-DOWNREGULATOR': 1,\n",
        "              'AGONIST': 2,'AGONIST-ACTIVATOR': 2,'AGONIST-INHIBITOR': 2,\n",
        "              'ANTAGONIST': 3, 'SUBSTRATE': 4, 'PRODUCT-OF': 4, 'SUBSTRATE_PRODUCT-OF': 4}\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "SEED = 20\n",
        "LEARNING_RATE = 3e-5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = REModel().to(device)\n",
        "\n",
        "df_train = read_data('./drive/MyDrive/RE/data/chemprot/train.txt')\n",
        "df_test = read_data('./drive/MyDrive/RE/data/chemprot/test.txt')\n",
        "\n",
        "train_dataset = ChemProtDataset(\n",
        "    sentence=df_train.text.values,\n",
        "    label=df_train.label.values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    subj = df_train.subj.values, \n",
        "    obj = df_train.obj.values\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataset = ChemProtDataset(\n",
        "    sentence=df_test.text.values,\n",
        "    label=df_test.label.values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    subj = df_test.subj.values, \n",
        "    obj = df_test.obj.values\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "num_training_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "outputs, labels = eval_loop_fn(test_data_loader, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPTHrG9VwZYg",
        "outputId": "5af34fad-2d63-4ed4-e65f-52c65fb225d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4806805074971165"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(np.argmax(outputs, axis=1), labels, average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5ewEH3Lvjzqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finetuning_RE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}