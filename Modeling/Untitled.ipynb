{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddd7e49-acd7-4e3a-9554-456c801f0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import os, time, json, datetime, pytz\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from multiprocessing import Pool\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertTokenizer, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from scipy import stats\n",
    "\n",
    "#distributed imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, DataParallel\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "import deepspeed\n",
    "\n",
    "#tokenizers and datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "\n",
    "#transformer imports\n",
    "from transformers import BertTokenizer, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling, BertConfig\n",
    "from transformers import BertModel,BertForMaskedLM, BertConfig, AdamW, TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa80ebe-cb02-46b6-b3e6-fed581bf5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/test_save_pretrained/model-trained-14000.pt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b6ed0c-a1fc-4bf3-b97c-9acfadf7b7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66e2ef3-ce6d-4a84-9e2c-8d0653b072b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrainedBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_path)\n",
    "        self.out = nn.Linear(768, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc93bb56-2862-429e-8ac6-fad7e151ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = BertModel(config=BertConfig(vocab_size=30500))\n",
    "        self.out = nn.Linear(768, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self, question, context, targets, tokenizer, max_len):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        question= str(self.question[item])\n",
    "        context = str(self.context[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        padding_len = self.max_len - len(ids)\n",
    "        \n",
    "        ids = ids[:self.max_len] + ([0] * padding_len) \n",
    "        token_type_ids = token_type_ids[:self.max_len] + ([0] * padding_len)\n",
    "        mask = mask[:self.max_len] + ([0] * padding_len)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        targets = d['targets']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 50 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            \n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "          ids = d['ids'].to(device, dtype=torch.long)\n",
    "          mask = d['mask'].to(device, dtype=torch.long)\n",
    "          token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "          targets = d['targets'].to(device, dtype=torch.long)\n",
    "        \n",
    "          outputs = model(ids, mask, token_type_ids)\n",
    "          #loss = loss_fn(outputs, targets)\n",
    "        \n",
    "          fin_targets.append(targets.cpu().detach().numpy())\n",
    "          fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.hstack(fin_targets)\n",
    "\n",
    "\n",
    "def run():\n",
    "    MAX_LEN = 512\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    EPOCHS = 4\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "    df_train = pd.read_json('./data/pqal_fold0/train_set.json', orient='index')\n",
    "    df_valid = pd.read_json('./data/pqal_fold0/dev_set.json', orient='index')\n",
    "    \n",
    "\n",
    "    target_cols = 'final_decision'\n",
    "    train_targets_str = df_train[target_cols].values\n",
    "    valid_targets_str = df_valid[target_cols].values\n",
    "    \n",
    "    # Transformer the target col to numeric values\n",
    "    target_dict = {'yes': 0, 'maybe': 1, 'no': 2}\n",
    "    train_targets = [target_dict[key] for key in train_targets_str]\n",
    "    valid_targets = [target_dict[key] for key in valid_targets_str]\n",
    "    \n",
    "\n",
    "    tokenizer = alternative_tokenizer \n",
    "    #model = TrainedBERT()\n",
    "    model = BERTBaseUncased()\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = BERTDatasetTraining(\n",
    "        question=df_train.QUESTION.values,\n",
    "        context=df_train.CONTEXTS.values,\n",
    "        targets=train_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    valid_dataset = BERTDatasetTraining(\n",
    "        question=df_valid.QUESTION.values,\n",
    "        context=df_valid.CONTEXTS.values,\n",
    "        targets=valid_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        output, target = eval_loop_fn(valid_data_loader, model, device)\n",
    "        acc = (output.argmax(1) == target).sum() / len(target)\n",
    "        print(f'epoch: {epoch}, acc: {acc}')\n",
    "    \n",
    "    # Inference\n",
    "    TEST_BATCH_SIZE = 8\n",
    "    df_test = pd.read_json('./data/test_set.json', orient='index')\n",
    "    test_targets_str = df_test[target_cols].values\n",
    "    test_targets = [target_dict[key] for key in test_targets_str]\n",
    "\n",
    "\n",
    "    test_dataset = BERTDatasetTraining(\n",
    "        question=df_test.QUESTION.values,\n",
    "        context=df_test.CONTEXTS.values,\n",
    "        targets=test_targets, # This is not used\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TEST_BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "\n",
    "    for bi, d in enumerate(test_data_loader):\n",
    "        ids = d[\"ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids).cpu().detach().numpy()\n",
    "            predictions.append(outputs.argmax(1))\n",
    "    pmids = df_test.index\n",
    "\n",
    "    return pmids, np.hstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d3d92c-299c-4d14-be68-b99970173afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi=0, loss=1.0881009101867676\n",
      "bi=50, loss=1.0495349168777466\n",
      "epoch: 0, acc: 0.5416666666666666\n",
      "bi=0, loss=0.8044472932815552\n",
      "bi=50, loss=0.9325860142707825\n",
      "epoch: 1, acc: 0.5416666666666666\n",
      "bi=0, loss=1.0519938468933105\n",
      "bi=50, loss=0.8075883984565735\n",
      "epoch: 2, acc: 0.5416666666666666\n",
      "bi=0, loss=1.1462160348892212\n",
      "bi=50, loss=1.0988537073135376\n",
      "epoch: 3, acc: 0.5625\n"
     ]
    }
   ],
   "source": [
    "pmids, predictions = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb120cf-1ff3-4bca-9a84-29962750ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "ground_truth = pd.read_json('./data/test_ground_truth.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f1677d-4a8e-40d1-b1de-68e239dcaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_list = ground_truth[0].tolist()\n",
    "target_dict = {'yes': 0, 'maybe': 1, 'no': 2}\n",
    "truth_list = [target_dict[key] for key in truth_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab542175-7d37-41dc-8565-221d7352dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.552"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(truth_list == predictions).sum() / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0445883d-8302-45fd-9d04-b547e59dcd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 25 20:58:01 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    35W / 250W |  16151MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    35W / 250W |  16069MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     19233      C   ...s/py38_pytorch/bin/python    16147MiB |\n",
      "|    1   N/A  N/A     19233      C   ...s/py38_pytorch/bin/python    16065MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c92a1f1e-54b7-4fdd-a21a-6a451ad616da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e06a99-d018-4b65-95d6-24bbfbd7fe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 25 20:58:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   30C    P0    35W / 250W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    35W / 250W |      0MiB / 16384MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc0aefa-4ee6-4414-a536-ad9696480fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/FineTuning\n"
     ]
    }
   ],
   "source": [
    "cd ../Modeling/FineTuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5388513d-773a-45c0-ba8d-e64f032559dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('data/test_ground_truth.json', orient='index').value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632799cc-3165-452c-9178-33931e6bb8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.552"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "276/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0ef4c-82f9-4458-bace-edfe76665ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
