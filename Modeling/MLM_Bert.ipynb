{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494fc40d-0386-4b40-9ac6-1584957f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import os, time, json, datetime, pytz\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from math import floor\n",
    "\n",
    "#distributed imports\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, DataParallel\n",
    "import deepspeed\n",
    "\n",
    "#tokenizers and datasets\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "\n",
    "#transformer imports\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForMaskedLM, BertConfig, AdamW\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b42db8b-50b0-4f36-88eb-a52b4d3d8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "for d in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d72b91c-eb5c-437e-b8a9-f315335900b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 26 04:11:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   40C    P0    67W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   56C    P0    77W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000003:00:00.0 Off |                  Off |\n",
      "| N/A   41C    P0    72W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000004:00:00.0 Off |                  Off |\n",
      "| N/A   45C    P0    64W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000005:00:00.0 Off |                  Off |\n",
      "| N/A   38C    P0    62W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000006:00:00.0 Off |                  Off |\n",
      "| N/A   44C    P0    68W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000007:00:00.0 Off |                  Off |\n",
      "| N/A   40C    P0    66W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000008:00:00.0 Off |                  Off |\n",
      "| N/A   45C    P0    65W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712fb9-5a4f-4dc8-907e-3a4abd1f2b57",
   "metadata": {},
   "source": [
    "#### Set Tokenizer and Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d793b3-6694-4ab6-b76e-1c31718f3317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small_sample_10000.txt',\n",
       " 'english_docs_ad.txt',\n",
       " 'english_docs_ac.txt',\n",
       " 'combined_4Gb.txt',\n",
       " 'english_docs_aa.txt',\n",
       " 'english_docs_ab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "text_data_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/Text/'\n",
    "encodings_data_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/Encodings/encodings_0_395390.pt'\n",
    "working_data =  'combined_4Gb.txt'\n",
    "\n",
    "files = [f for f in os.listdir(text_data_path) if os.path.isfile(os.path.join(text_data_path, f))]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8631b6b-5d24-4637-94bb-2cec5ad36bce",
   "metadata": {},
   "source": [
    "#### Instantiate pretrained tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23d3e2f-b515-4bc3-9dd2-e7d2a87f5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(tokenizer_path, strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbff84-1369-4b4a-90c8-80c276a56264",
   "metadata": {},
   "source": [
    "#### Load data from local\n",
    "Data is a 98,000 line file with each line representing one document of length ~12,000 characters from PubMed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57250a2d-1cbf-4273-8aa1-2881ceb46f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from disk\n",
    "def load_data_from_disk(path: str, sample_size:int=None, min_tokens_per_sent: int=4) -> List[str]:\n",
    "    '''\n",
    "    Utility data loading function that performs the following operations:\n",
    "       1. Loads data from disk into a list. Assumes each doc is one line.\n",
    "       2. Performs sentence splitting on each document.\n",
    "       3. Removes all sentences with tokens < 4 (default).\n",
    "       4. Returns a list of sentences \n",
    "    '''\n",
    "    #load data\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    #split data into sentences\n",
    "    sentences = [split_into_sentences(i) for i in tqdm(lines, 'Sentence Splitter')]\n",
    "    \n",
    "    #remove all sentences with less than 5 tokens\n",
    "    all_sentences = []\n",
    "    for doc in tqdm(sentences, 'Filter Senteces'):\n",
    "        for sentence in doc:\n",
    "            if len(sentence.split()) > 4:\n",
    "                all_sentences.append(sentence)\n",
    "    print(f'Return a list of {len(all_sentences)} sentences')\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7703ca1a-62b2-4ab3-98a1-fd38aad080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfeece3e-eb04-4b8b-a6df-23d80150d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49\n"
     ]
    }
   ],
   "source": [
    "#results = load_data_from_disk(os.path.join(vm_data, ))\n",
    "start = time.perf_counter()\n",
    "results = load_data_seq_512(os.path.join(data_path, file[0]))\n",
    "end = time.perf_counter() - start\n",
    "print(round(end, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a0372d-189a-44e1-a1aa-7ebcc6f86f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b03e9a-ca9a-42fb-a0d4-cf18c39fdbd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# segments = []\n",
    "# temp_holder = []\n",
    "# def create_512_sequences(\n",
    "# for sentence in tqdm(results, 'Tokenizing'):\n",
    "#     tokens = alternative_tokenizer.encode(sentence)\n",
    "#     if len(temp_holder) + len(tokens) < 512:\n",
    "#         temp_holder.extend(tokens[1:-1])\n",
    "#     else:\n",
    "#         temp_holder.insert(0,2)\n",
    "#         segments.append(temp_holder + [3])\n",
    "#         temp_holder = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4bc6f-5c8f-4d24-b4ca-c9d2b7b9f73f",
   "metadata": {},
   "source": [
    "#### Batch encode a chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fc29f8-e7aa-42bc-ba2b-79aad83462f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.72 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(results)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "352e2d26-4384-401d-a130-53b14af0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrease load on memory\n",
    "#del results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568788a-5ffe-4376-a72e-1172141d8c00",
   "metadata": {},
   "source": [
    "#### Create pipeline for random masking of 15% of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9386eae-bc2e-4edb-b158-28e3a71748cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f6988-527c-440f-8ab8-35ac3fbc7ff8",
   "metadata": {},
   "source": [
    "#### Load Encodings from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c4dd28-12ac-41d2-a2e0-ebc2bd68a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodings = mlm_pipe(batch)\n",
    "encodings = torch.load(encodings_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c370c7-4cc3-4fbe-a331-37fdd4acc05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1490)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2baae0cd-f767-438c-87fd-aa2b7da3c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {key : tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745683a2-fb73-4805-a5bd-1cee5bcf47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(encodings)\n",
    "#del batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a1ed71-47e7-43b2-9d3a-63985fce024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3531"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(d, batch_size=112, pin_memory=True, shuffle=True)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79ee7a9-0c6c-4fcc-b6ea-1831cc4147b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=30500,num_hidden_layers=12)\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7775707d-efdc-4564-ab09-20dae9d08b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' \n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02fa006-3f28-47f1-8b47-017aac7de024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f596e014-9b05-4e8f-9e7e-f8c734b5d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 26 04:11:41 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   40C    P0    67W / 300W |   1854MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   55C    P0    76W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000003:00:00.0 Off |                  Off |\n",
      "| N/A   41C    P0    73W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000004:00:00.0 Off |                  Off |\n",
      "| N/A   44C    P0    63W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000005:00:00.0 Off |                  Off |\n",
      "| N/A   38C    P0    62W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000006:00:00.0 Off |                  Off |\n",
      "| N/A   43C    P0    68W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000007:00:00.0 Off |                  Off |\n",
      "| N/A   40C    P0    66W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000008:00:00.0 Off |                  Off |\n",
      "| N/A   44C    P0    65W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     10369      C   ...s/py38_pytorch/bin/python     1851MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8438e07-6676-44f1-ba7a-f1b4994b3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def run():\n",
    "    num_batches = len(loader)\n",
    "    epochs = 50\n",
    "    step = 0\n",
    "    lowest_loss = 10000\n",
    "    tolerance = 0.01\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # for file in files[:2]:\n",
    "        #     # setup loop with TQDM and dataloader\n",
    "        #     results = load_data_from_disk(os.path.join(vm_data, file))\n",
    "        #     batch = tokenizer.encode_batch(results)\n",
    "        #     del results\n",
    "        #     encodings = mlm_pipe(batch)\n",
    "        #     del batch\n",
    "        #     d = Dataset(encodings)\n",
    "        #     del encodings\n",
    "        #     loader = torch.utils.data.DataLoader(d, batch_size=384, pin_memory=True, shuffle=True)\n",
    "\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            step += 1\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optimizer.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.sum().backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # print relevant info to progress barI \n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            \n",
    "            loss_check = floor(num_batches/10)\n",
    "            checkpoint = floor(num_batches/2)\n",
    "            \n",
    "            if step % loss_check == 0:\n",
    "                print(f'Loss: {loss.sum()}')\n",
    "            \n",
    "        model.module.save_pretrained(f'checkpoints/run_4GB_Mar25_1000pm/model-trained-{epoch}-{step}.pt') \n",
    "            \n",
    "        #loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b59c3cc-f918-4c0f-9ac9-33c1f2ed41c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec46b1422fd94bbfb4742e81198f47cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.073324203491211\n",
      "Loss: 8.733743667602539\n",
      "Loss: 8.598007202148438\n",
      "Loss: 8.500348091125488\n",
      "Loss: 8.259737968444824\n",
      "Loss: 8.081645965576172\n",
      "Loss: 7.992838382720947\n",
      "Loss: 8.103448867797852\n",
      "Loss: 7.586031913757324\n",
      "Loss: 6.755363464355469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffaf6c584904155a19a27e6376b633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.1926589012146\n",
      "Loss: 5.519335746765137\n",
      "Loss: 5.149563789367676\n",
      "Loss: 4.832999229431152\n",
      "Loss: 4.671858310699463\n",
      "Loss: 4.3307294845581055\n",
      "Loss: 4.0236992835998535\n",
      "Loss: 3.9969537258148193\n",
      "Loss: 3.7827701568603516\n",
      "Loss: 3.540799617767334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8782db941a47b1b2e2c52a3c98e9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.42087459564209\n",
      "Loss: 3.4510416984558105\n",
      "Loss: 3.3610424995422363\n",
      "Loss: 3.31130051612854\n",
      "Loss: 3.358001947402954\n",
      "Loss: 3.260727882385254\n",
      "Loss: 3.1341209411621094\n",
      "Loss: 3.2493226528167725\n",
      "Loss: 3.076169013977051\n",
      "Loss: 3.1103620529174805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba96cad52d847b2a68f8b25fe1dfeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.8777055740356445\n",
      "Loss: 2.9155995845794678\n",
      "Loss: 3.0150604248046875\n",
      "Loss: 2.802856206893921\n",
      "Loss: 2.9506590366363525\n",
      "Loss: 2.888399600982666\n",
      "Loss: 2.8464808464050293\n",
      "Loss: 2.8243842124938965\n",
      "Loss: 2.799969434738159\n",
      "Loss: 2.7896785736083984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a1738ec9f0439e9f4d16cd2fe05edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.674084186553955\n",
      "Loss: 2.6321210861206055\n",
      "Loss: 2.7832517623901367\n",
      "Loss: 2.648716449737549\n",
      "Loss: 2.5958380699157715\n",
      "Loss: 2.5578980445861816\n",
      "Loss: 2.612293243408203\n",
      "Loss: 2.604933977127075\n",
      "Loss: 2.6298789978027344\n",
      "Loss: 2.562743663787842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2ac09b061c47fcafb6b18a3aca2998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4964590072631836\n",
      "Loss: 2.519127368927002\n",
      "Loss: 2.463134765625\n",
      "Loss: 2.3647708892822266\n",
      "Loss: 2.466353178024292\n",
      "Loss: 2.454939365386963\n",
      "Loss: 2.4895360469818115\n",
      "Loss: 2.3631343841552734\n",
      "Loss: 2.383009910583496\n",
      "Loss: 2.4516940116882324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0238ed2d3b437ead873c39d4fcc2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.341940402984619\n",
      "Loss: 2.398685932159424\n",
      "Loss: 2.374462366104126\n",
      "Loss: 2.2744500637054443\n",
      "Loss: 2.340658664703369\n",
      "Loss: 2.392343044281006\n",
      "Loss: 2.341154098510742\n",
      "Loss: 2.3502023220062256\n",
      "Loss: 2.2401556968688965\n",
      "Loss: 2.4101977348327637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4373895a71334179bcadd0cfad71e28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.232128620147705\n",
      "Loss: 2.2581076622009277\n",
      "Loss: 2.121626138687134\n",
      "Loss: 2.3205044269561768\n",
      "Loss: 2.25950288772583\n",
      "Loss: 2.2426810264587402\n",
      "Loss: 2.282215118408203\n",
      "Loss: 2.2590408325195312\n",
      "Loss: 2.227525234222412\n",
      "Loss: 2.2388219833374023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4489f7bb0972467095c8013aa2a9cf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.166822910308838\n",
      "Loss: 2.127472400665283\n",
      "Loss: 2.0675835609436035\n",
      "Loss: 2.073528289794922\n",
      "Loss: 2.1895177364349365\n",
      "Loss: 2.13639497756958\n",
      "Loss: 2.1755905151367188\n",
      "Loss: 2.10386061668396\n",
      "Loss: 2.1725924015045166\n",
      "Loss: 2.134495735168457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06ea79bfdd94628b0df0b5b352a042b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0913610458374023\n",
      "Loss: 1.9862961769104004\n",
      "Loss: 2.0380358695983887\n",
      "Loss: 2.0738883018493652\n",
      "Loss: 2.0535573959350586\n",
      "Loss: 2.035210609436035\n",
      "Loss: 2.0088183879852295\n",
      "Loss: 2.1173152923583984\n",
      "Loss: 2.055591106414795\n",
      "Loss: 2.090090036392212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b584d29994b46a0172c72c30013af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9205766916275024\n",
      "Loss: 1.8831324577331543\n",
      "Loss: 2.0016884803771973\n",
      "Loss: 1.9727044105529785\n",
      "Loss: 1.9709312915802002\n",
      "Loss: 1.9382593631744385\n",
      "Loss: 1.9573805332183838\n",
      "Loss: 1.945777177810669\n",
      "Loss: 2.1248462200164795\n",
      "Loss: 2.0124595165252686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c0a6db792c434d8637d5cf98f8afbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.8893709182739258\n",
      "Loss: 1.8925068378448486\n",
      "Loss: 1.9285356998443604\n",
      "Loss: 1.9657697677612305\n",
      "Loss: 1.8860622644424438\n",
      "Loss: 1.890089750289917\n",
      "Loss: 1.8555983304977417\n",
      "Loss: 1.9392744302749634\n",
      "Loss: 2.013216733932495\n",
      "Loss: 1.9524872303009033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343548cf37874beba5b6a776d89c7e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.7949687242507935\n",
      "Loss: 1.8288542032241821\n",
      "Loss: 1.914140224456787\n",
      "Loss: 1.9128665924072266\n",
      "Loss: 1.8424830436706543\n",
      "Loss: 1.855072021484375\n",
      "Loss: 2.0316755771636963\n",
      "Loss: 1.8875417709350586\n",
      "Loss: 1.8139156103134155\n",
      "Loss: 1.8863837718963623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fd2e1bee9c48a2888b137dffde8bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.778273582458496\n",
      "Loss: 1.7716835737228394\n",
      "Loss: 1.8085905313491821\n",
      "Loss: 1.8155686855316162\n",
      "Loss: 1.833611011505127\n",
      "Loss: 1.8564906120300293\n",
      "Loss: 1.8313688039779663\n",
      "Loss: 1.8647329807281494\n",
      "Loss: 1.8676187992095947\n",
      "Loss: 1.875718355178833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b383948425b7472ba86408e5784e961e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6393511295318604\n",
      "Loss: 1.6845227479934692\n",
      "Loss: 1.7729411125183105\n",
      "Loss: 1.7545100450515747\n",
      "Loss: 1.842043399810791\n",
      "Loss: 1.7828893661499023\n",
      "Loss: 1.807100534439087\n",
      "Loss: 1.7937674522399902\n",
      "Loss: 1.8165805339813232\n",
      "Loss: 1.788452386856079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c846bd35a5d4b7f857b0fd70236a033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.611527919769287\n",
      "Loss: 1.6836025714874268\n",
      "Loss: 1.6616649627685547\n",
      "Loss: 1.6664878129959106\n",
      "Loss: 1.678371787071228\n",
      "Loss: 1.6317059993743896\n",
      "Loss: 1.7852702140808105\n",
      "Loss: 1.7612553834915161\n",
      "Loss: 1.7241921424865723\n",
      "Loss: 1.7680954933166504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c0b488d6b4097ad773bb6c4c8a5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6179929971694946\n",
      "Loss: 1.6242566108703613\n",
      "Loss: 1.7293473482131958\n",
      "Loss: 1.5948189496994019\n",
      "Loss: 1.6286014318466187\n",
      "Loss: 1.7457890510559082\n",
      "Loss: 1.6680102348327637\n",
      "Loss: 1.8103713989257812\n",
      "Loss: 1.7303638458251953\n",
      "Loss: 1.6913769245147705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afebe2f50714d1ea10ab782ab5d9abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.571327567100525\n",
      "Loss: 1.6406667232513428\n",
      "Loss: 1.5922194719314575\n",
      "Loss: 1.6698628664016724\n",
      "Loss: 1.6480906009674072\n",
      "Loss: 1.757521629333496\n",
      "Loss: 1.6177732944488525\n",
      "Loss: 1.6686928272247314\n",
      "Loss: 1.6199958324432373\n",
      "Loss: 1.712738275527954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d561b32f96cb49e5b7fea59df6bcf86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5014357566833496\n",
      "Loss: 1.588231086730957\n",
      "Loss: 1.6354713439941406\n",
      "Loss: 1.6255316734313965\n",
      "Loss: 1.5052427053451538\n",
      "Loss: 1.6267383098602295\n",
      "Loss: 1.599442481994629\n",
      "Loss: 1.6242671012878418\n",
      "Loss: 1.584991216659546\n",
      "Loss: 1.640555739402771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c7955879064039a80fc4746822fd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5263729095458984\n",
      "Loss: 1.5712683200836182\n",
      "Loss: 1.5655875205993652\n",
      "Loss: 1.5478483438491821\n",
      "Loss: 1.5355360507965088\n",
      "Loss: 1.5883703231811523\n",
      "Loss: 1.6170179843902588\n",
      "Loss: 1.5031996965408325\n",
      "Loss: 1.5571300983428955\n",
      "Loss: 1.6782453060150146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51454680757345c4b0eef473b2e09d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4451026916503906\n",
      "Loss: 1.5222151279449463\n",
      "Loss: 1.4926536083221436\n",
      "Loss: 1.4744653701782227\n",
      "Loss: 1.5241048336029053\n",
      "Loss: 1.5071446895599365\n",
      "Loss: 1.5825474262237549\n",
      "Loss: 1.555250644683838\n",
      "Loss: 1.5118706226348877\n",
      "Loss: 1.5728669166564941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526f009698c649b19516ad741dbf907c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4022725820541382\n",
      "Loss: 1.3848152160644531\n",
      "Loss: 1.5074362754821777\n",
      "Loss: 1.4717872142791748\n",
      "Loss: 1.4626399278640747\n",
      "Loss: 1.5455236434936523\n",
      "Loss: 1.4378373622894287\n",
      "Loss: 1.491405963897705\n",
      "Loss: 1.539149522781372\n",
      "Loss: 1.614062786102295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c269b4acde4e459018c8bc6ea20737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3933844566345215\n",
      "Loss: 1.3798444271087646\n",
      "Loss: 1.4734952449798584\n",
      "Loss: 1.5280817747116089\n",
      "Loss: 1.5072996616363525\n",
      "Loss: 1.5143115520477295\n",
      "Loss: 1.4600484371185303\n",
      "Loss: 1.602672815322876\n",
      "Loss: 1.457524061203003\n",
      "Loss: 1.5011769533157349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85786d78bd894e7da2a6346e34a3708f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3895177841186523\n",
      "Loss: 1.3385181427001953\n",
      "Loss: 1.4840412139892578\n",
      "Loss: 1.4749860763549805\n",
      "Loss: 1.5414146184921265\n",
      "Loss: 1.4399067163467407\n",
      "Loss: 1.486703872680664\n",
      "Loss: 1.5375373363494873\n",
      "Loss: 1.556780219078064\n",
      "Loss: 1.5649681091308594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3da2879acc4f8cac674c4a8150f282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3717694282531738\n",
      "Loss: 1.3807822465896606\n",
      "Loss: 1.4413783550262451\n",
      "Loss: 1.48172926902771\n",
      "Loss: 1.478743076324463\n",
      "Loss: 1.440948724746704\n",
      "Loss: 1.4966752529144287\n",
      "Loss: 1.490187406539917\n",
      "Loss: 1.5089738368988037\n",
      "Loss: 1.5168275833129883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370edaf17407483093f0333cfc3c514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.288806676864624\n",
      "Loss: 1.3833585977554321\n",
      "Loss: 1.4018089771270752\n",
      "Loss: 1.4117186069488525\n",
      "Loss: 1.3656630516052246\n",
      "Loss: 1.3667019605636597\n",
      "Loss: 1.4583852291107178\n",
      "Loss: 1.4071767330169678\n",
      "Loss: 1.4655795097351074\n",
      "Loss: 1.431588888168335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922d9dd40e74436895ba2c369f0ae95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2984907627105713\n",
      "Loss: 1.3579798936843872\n",
      "Loss: 1.3793426752090454\n",
      "Loss: 1.4298369884490967\n",
      "Loss: 1.3553822040557861\n",
      "Loss: 1.3851230144500732\n",
      "Loss: 1.5256638526916504\n",
      "Loss: 1.4466924667358398\n",
      "Loss: 1.3731142282485962\n",
      "Loss: 1.4804524183273315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e5fee3b909456e870a4af0d183b66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3200339078903198\n",
      "Loss: 1.3258605003356934\n",
      "Loss: 1.3825438022613525\n",
      "Loss: 1.3245118856430054\n",
      "Loss: 1.4039332866668701\n",
      "Loss: 1.4240598678588867\n",
      "Loss: 1.4047459363937378\n",
      "Loss: 1.4422909021377563\n",
      "Loss: 1.4619202613830566\n",
      "Loss: 1.4378271102905273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308f1155ad74484ba85b4e843dbb80fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.198969841003418\n",
      "Loss: 1.3331431150436401\n",
      "Loss: 1.3001501560211182\n",
      "Loss: 1.3124654293060303\n",
      "Loss: 1.346968412399292\n",
      "Loss: 1.3871517181396484\n",
      "Loss: 1.3586418628692627\n",
      "Loss: 1.349528193473816\n",
      "Loss: 1.477169394493103\n",
      "Loss: 1.4011399745941162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6068c3fa19e4d709454b3d6f1ea9d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2586429119110107\n",
      "Loss: 1.2579805850982666\n",
      "Loss: 1.3425045013427734\n",
      "Loss: 1.2826893329620361\n",
      "Loss: 1.3386163711547852\n",
      "Loss: 1.382401943206787\n",
      "Loss: 1.3356181383132935\n",
      "Loss: 1.37186598777771\n",
      "Loss: 1.3913521766662598\n",
      "Loss: 1.4191524982452393\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225f8606b2634bfaae32515747543eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1996655464172363\n",
      "Loss: 1.294020652770996\n",
      "Loss: 1.2561750411987305\n",
      "Loss: 1.2840348482131958\n",
      "Loss: 1.395337462425232\n",
      "Loss: 1.2934131622314453\n",
      "Loss: 1.3360191583633423\n",
      "Loss: 1.393754482269287\n",
      "Loss: 1.3611252307891846\n",
      "Loss: 1.4134647846221924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b9eb208d584a9e92f3f93b401ade27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.16933274269104\n",
      "Loss: 1.245877981185913\n",
      "Loss: 1.287936806678772\n",
      "Loss: 1.336641788482666\n",
      "Loss: 1.3204429149627686\n",
      "Loss: 1.3487217426300049\n",
      "Loss: 1.3597822189331055\n",
      "Loss: 1.3673274517059326\n",
      "Loss: 1.340757966041565\n",
      "Loss: 1.3570289611816406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8d74826cd14156bd461ace8e875c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2214900255203247\n",
      "Loss: 1.2688748836517334\n",
      "Loss: 1.324378252029419\n",
      "Loss: 1.2499945163726807\n",
      "Loss: 1.3282818794250488\n",
      "Loss: 1.2935651540756226\n",
      "Loss: 1.3365156650543213\n",
      "Loss: 1.276237964630127\n",
      "Loss: 1.3232009410858154\n",
      "Loss: 1.3237115144729614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cc073056b24de680186b639dfa9d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1893830299377441\n",
      "Loss: 1.2278039455413818\n",
      "Loss: 1.214604139328003\n",
      "Loss: 1.2282532453536987\n",
      "Loss: 1.2963335514068604\n",
      "Loss: 1.3355989456176758\n",
      "Loss: 1.2786861658096313\n",
      "Loss: 1.2960171699523926\n",
      "Loss: 1.3697643280029297\n",
      "Loss: 1.4528422355651855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fb596d40ad4bb0b8eefbda4b71e25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1777124404907227\n",
      "Loss: 1.2257864475250244\n",
      "Loss: 1.2208645343780518\n",
      "Loss: 1.2261106967926025\n",
      "Loss: 1.1927168369293213\n",
      "Loss: 1.236515760421753\n",
      "Loss: 1.3775222301483154\n",
      "Loss: 1.2557975053787231\n",
      "Loss: 1.3470258712768555\n",
      "Loss: 1.2840032577514648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9937172513994e4aa1938840e81c7032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1584417819976807\n",
      "Loss: 1.1697134971618652\n",
      "Loss: 1.216402530670166\n",
      "Loss: 1.2556936740875244\n",
      "Loss: 1.2777111530303955\n",
      "Loss: 1.2847025394439697\n",
      "Loss: 1.3144195079803467\n",
      "Loss: 1.2188503742218018\n",
      "Loss: 1.2808573246002197\n",
      "Loss: 1.3221169710159302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579da083453541ca956a3b2f002ae618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1405360698699951\n",
      "Loss: 1.131739854812622\n",
      "Loss: 1.1913433074951172\n",
      "Loss: 1.1170084476470947\n",
      "Loss: 1.2482719421386719\n",
      "Loss: 1.2935435771942139\n",
      "Loss: 1.2357840538024902\n",
      "Loss: 1.2813379764556885\n",
      "Loss: 1.1976304054260254\n",
      "Loss: 1.2979164123535156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5aee32535744288948bab5f190ad76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10369/766504495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10369/3857447941.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             outputs = model(input_ids, attention_mask=attention_mask,\n\u001b[0m\u001b[1;32m     36\u001b[0m                             labels=labels)\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# extract loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mparam_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mparam_copies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_coalesced_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mbuffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Use the autograd function to broadcast if not detach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtensor_copies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             return [tensor_copies[i:i + len(tensors)]\n\u001b[1;32m     73\u001b[0m                     for i in range(0, len(tensor_copies), len(tensors))]\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_coalesced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mnon_differentiables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_requires_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/comm.py\u001b[0m in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mdevices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_handle_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broadcast_coalesced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99dcc579-7540-44e6-8456-fea1fd3783d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Driver/library version mismatch\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47a605de-2c66-480b-9b47-64d9916a4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "281c654a-ec72-4bf0-9525-50d2aee5d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Driver/library version mismatch\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af052351-64b6-43bf-abdb-adc84d695e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = alternative_tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce768ed3-ecf3-4794-b123-3b7fb5ff5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(text: str):\n",
    "    config = BertConfig(vocab_size=30500)\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    untrained_pipe = pipeline('fill-mask', model=model, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n",
    "    utresult = untrained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Untrained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in utresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "        \n",
    "    lm = BertForMaskedLM.from_pretrained('checkpoints/test_save_pretrained/model-trained-14000.pt/')\n",
    "    trained_pipe = pipeline('fill-mask', model=lm, tokenizer=alternative_tokenizer)\n",
    "\n",
    "    tresult = trained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Trained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in tresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6751054-f4ad-4f98-8d83-f42ef5bde42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9714ba88104eedb3114fc8c4670790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009145cbbbec4a17a852b07d18310a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430fbdbed1b244b884c2316b9b49aeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4accefdbe35247e1b78f3084db967bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Untrained Results\n",
      "******************************************************************************************************************************************************\n",
      "it is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.7136348485946655\n",
      "insulin is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.014016539789736271\n",
      "amp is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.01275827456265688\n",
      "cox is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.012082782573997974\n",
      "notch is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.01053623203188181\n",
      "\n",
      "Trained Results\n",
      "******************************************************************************************************************************************************\n",
      "it is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.23749612271785736\n",
      "there is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.11453662812709808\n",
      "acetylcholine is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.030285345390439034\n",
      "dopamine is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.029437104240059853\n",
      "neurotransmitter is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.025556039065122604\n"
     ]
    }
   ],
   "source": [
    "show_results(f'{mask} is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7df956ec-5c19-46af-b9e5-ba9cf169a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/subsets/xaasplit_25K') as f:\n",
    "    data = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66048ed-aa14-4d6b-ab36-c305184cf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.encode_batch(['Introduction Under normal physiological conditions, all cells in the body are exposed chronically to oxidants from both endogenous and exogenous sources;', 'This is another sentence.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fe76d4-9eeb-423b-a152-86b7e3fce8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([x.ids for x in test])\n",
    "mask = torch.tensor([x.attention_mask for x in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e74e514-2e72-4821-b9d9-9e76318a1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = new_model(input_ids, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a91f911-d6b9-4725-9b5e-eedd8b85c595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 30500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "784c568d-efb4-4232-ad1f-4f7bb5c16262",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbut = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e966560-6332-4b83-a87a-c81cf9b2c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_pretrained('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524614ae-7c88-44e9-9e6e-46e33c03141e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=30500, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97e0596f-c6d3-4e34-a44b-7a2d17b493de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30500, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(514, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30500, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26f383e-e1f9-40b5-a139-ccba0ea413c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "test = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db2b895-924d-4bff-88d9-2ded744aadde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ffd1e38-30cc-4052-aa32-87df4b346aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertForMaskedLM, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63018eb5-2b6c-4589-a2b0-78d2fdaba938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87b671c-62fe-4a4c-8377-f5aeee357f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed_pretrain_bert import masking_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c641b0-6a23-407d-938b-ca7113bb53f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  2420,\n",
       "  8747,\n",
       "  1641,\n",
       "  1634,\n",
       "  2065,\n",
       "  14042,\n",
       "  1737,\n",
       "  22511,\n",
       "  17,\n",
       "  4,\n",
       "  17,\n",
       "  4,\n",
       "  4,\n",
       "  1696,\n",
       "  1851,\n",
       "  1897,\n",
       "  2290,\n",
       "  3548,\n",
       "  3949,\n",
       "  4,\n",
       "  1831,\n",
       "  4,\n",
       "  1633,\n",
       "  27604,\n",
       "  30,\n",
       "  37,\n",
       "  4,\n",
       "  1811,\n",
       "  4,\n",
       "  18,\n",
       "  6791,\n",
       "  41,\n",
       "  3259,\n",
       "  18,\n",
       "  2976,\n",
       "  11,\n",
       "  4,\n",
       "  11,\n",
       "  1811,\n",
       "  4,\n",
       "  18,\n",
       "  26897,\n",
       "  41,\n",
       "  3259,\n",
       "  18,\n",
       "  5150,\n",
       "  11,\n",
       "  39,\n",
       "  17,\n",
       "  1784,\n",
       "  1687,\n",
       "  4178,\n",
       "  2415,\n",
       "  5993,\n",
       "  1740,\n",
       "  3548,\n",
       "  3949,\n",
       "  4,\n",
       "  1831,\n",
       "  3253,\n",
       "  1633,\n",
       "  27604,\n",
       "  1746,\n",
       "  1634,\n",
       "  14042,\n",
       "  1641,\n",
       "  43,\n",
       "  2065,\n",
       "  6751,\n",
       "  1702,\n",
       "  4,\n",
       "  3792,\n",
       "  1726,\n",
       "  1677,\n",
       "  3609,\n",
       "  8452,\n",
       "  12,\n",
       "  47,\n",
       "  18,\n",
       "  49,\n",
       "  18,\n",
       "  3548,\n",
       "  3949,\n",
       "  43,\n",
       "  22511,\n",
       "  1769,\n",
       "  19560,\n",
       "  6477,\n",
       "  4,\n",
       "  1654,\n",
       "  16904,\n",
       "  2728,\n",
       "  2065,\n",
       "  1746,\n",
       "  43,\n",
       "  22511,\n",
       "  1769,\n",
       "  3241,\n",
       "  16302,\n",
       "  3965,\n",
       "  2065,\n",
       "  13,\n",
       "  18,\n",
       "  17,\n",
       "  1784,\n",
       "  1687,\n",
       "  1851,\n",
       "  4178,\n",
       "  4,\n",
       "  5993,\n",
       "  1740,\n",
       "  3548,\n",
       "  4,\n",
       "  22511,\n",
       "  1831,\n",
       "  3253,\n",
       "  1633,\n",
       "  27604,\n",
       "  4,\n",
       "  1634,\n",
       "  14042,\n",
       "  1641,\n",
       "  43,\n",
       "  2065,\n",
       "  4,\n",
       "  5993,\n",
       "  6547,\n",
       "  1656,\n",
       "  1710,\n",
       "  14045,\n",
       "  6794,\n",
       "  12,\n",
       "  3762,\n",
       "  3949,\n",
       "  43,\n",
       "  22511,\n",
       "  4,\n",
       "  19560,\n",
       "  1879,\n",
       "  15790,\n",
       "  4,\n",
       "  16904,\n",
       "  2728,\n",
       "  2065,\n",
       "  1746,\n",
       "  43,\n",
       "  22511,\n",
       "  1769,\n",
       "  19560,\n",
       "  1879,\n",
       "  15790,\n",
       "  1654,\n",
       "  16904,\n",
       "  2728,\n",
       "  2065,\n",
       "  13,\n",
       "  18,\n",
       "  None],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4893,\n",
       "  0,\n",
       "  5319,\n",
       "  1877,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  22511,\n",
       "  0,\n",
       "  3253,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  1018,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  16,\n",
       "  0,\n",
       "  0,\n",
       "  1018,\n",
       "  0,\n",
       "  6791,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  22511,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3609,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  49,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1879,\n",
       "  15790,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2415,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3949,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1746,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1732,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3548,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1769,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1654,\n",
       "  16904,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking_function(text=\"\"\"\n",
    "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
    "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "\"\"\", \n",
    "                 tokenizer=alternative_tokenizer, \n",
    "                 mask_prob=0.15, \n",
    "                 random_replace_prob=0.1, \n",
    "                 unmask_replace_prob=0.1,\n",
    "                 max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f294b0b-5502-49a9-a8df-f9fdd1cd1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_identifier(length: int = 8) -> str:\n",
    "    \"\"\"Create a unique identifier by choosing `length`\n",
    "    random characters from list of ascii characters and numbers\n",
    "    \"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    uuid = \"\".join(alphabet[ix] for ix in np.random.choice(len(alphabet), length))\n",
    "    return uuid\n",
    "\n",
    "def create_experiment_dir(\n",
    "        checkpoint_dir: pathlib.Path, all_arguments: Dict\n",
    ") -> pathlib.Path:\n",
    "    \"\"\" Create an experiment directory and save all arguments in it.\"\"\"\n",
    "    current_time = datetime.datetime.now(pytz.timezone(\"US/Pacific\"))\n",
    "    expname = f\"bert_pretrain.{current_time.year}.{current_time.month}.{current_time.day}.{current_time.hour}.{current_time.minute}.{current_time.second}.{get_unique_identifier()}\"\n",
    "    exp_dir = checkpoint_dir / expname\n",
    "    exp_dir.mkdir(exist_ok=False)\n",
    "    hparams_file = exp_dir / \"hparams.json\"\n",
    "    with hparams_file.open(\"w\") as handle:\n",
    "        json.dump(obj=all_arguments, fp=handle, indent=2)\n",
    "\n",
    "    # Create the Tensorboard Dir\n",
    "    tb_dir = exp_dir / \"tb_dir\"\n",
    "    tb_dir.mkdir()\n",
    "    return exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1b2784-6a71-4d83-a7f3-a0be367e1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\"train_file\": '',\n",
    "        \"validation_file\": \"\",\n",
    "        \"mask_prob\": 0.15,\n",
    "        \"epoch\": 10,\n",
    "        \"batch_size\":32,\n",
    "        \"checkpoint_every\":1000,\n",
    "        \"learning_rate\":1e-5,\n",
    "        \"weight_decay\":0.001,\n",
    "        \"gradient_accumulation_steps\":1,\n",
    "        \"lr_scheduler_type\":'linear',\n",
    "        \"num_warmup_steps\":1000,\n",
    "        \"seed\":42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38675c4b-b6ba-4469-b5e9-cd7f11714bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hparams.json', 'w') as f:\n",
    "    f.write(json.dumps(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce778d-58a8-4ae9-9bec-840bbe091017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "\n",
    "def save_model(path: './', multiple_gpu: bool=True):\n",
    "    if multiple_gpu:\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "              f'{path}model_{step}.pt')\n",
    "    else:\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "              f'{path}model_{step}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e6e31a4-18c8-4290-ad06-10d68a3a7ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2592"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36*72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd9addc-9173-4cf5-8b9d-e7bb0b943c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04404019-b614-4898-9df0-a093e4973ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
