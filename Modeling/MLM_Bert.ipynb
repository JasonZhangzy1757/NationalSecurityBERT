{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494fc40d-0386-4b40-9ac6-1584957f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import os, time, json, datetime, pytz\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from multiprocessing import Pool\n",
    "import string\n",
    "import numpy as np\n",
    "from IPython import get_ipython\n",
    "\n",
    "#distributed imports\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, DataParallel\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "import deepspeed\n",
    "from transformers.deepspeed import HfDeepSpeedConfig\n",
    "\n",
    "#tokenizers and datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "\n",
    "#transformer imports\n",
    "from transformers import BertTokenizer, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling\n",
    "from transformers import BertForMaskedLM, BertConfig, AdamW, TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b42db8b-50b0-4f36-88eb-a52b4d3d8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla V100-PCIE-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "for d in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712fb9-5a4f-4dc8-907e-3a4abd1f2b57",
   "metadata": {},
   "source": [
    "#### Set Tokenizer and Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d793b3-6694-4ab6-b76e-1c31718f3317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xac']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_tok_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "vm_data = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/'\n",
    "checkpoint_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/'\n",
    "files = [f for f in os.listdir(vm_data) if os.path.isfile(os.path.join(vm_data, f))]\n",
    "files\n",
    "#local paths\n",
    "# local_tok_path = '/Users/americanthinker1/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "# local_data = '/Users/americanthinker1/aws_data/processed_data/processed_chunks/english_docs_aa.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8631b6b-5d24-4637-94bb-2cec5ad36bce",
   "metadata": {},
   "source": [
    "#### Instantiate pretrained tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23d3e2f-b515-4bc3-9dd2-e7d2a87f5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbff84-1369-4b4a-90c8-80c276a56264",
   "metadata": {},
   "source": [
    "#### Load data from local\n",
    "Data is a 98,000 line file with each line representing one document of length ~12,000 characters from PubMed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57250a2d-1cbf-4273-8aa1-2881ceb46f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from disk\n",
    "def load_data_from_disk(path: str, sample_size:int=None, min_tokens_per_sent: int=4) -> List[str]:\n",
    "    '''\n",
    "    Utility data loading function that performs the following operations:\n",
    "       1. Loads data from disk into a list. Assumes each doc is one line.\n",
    "       2. Performs sentence splitting on each document.\n",
    "       3. Removes all sentences with tokens < 4 (default).\n",
    "       4. Returns a list of sentences \n",
    "    '''\n",
    "    #load data\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    #split data into sentences\n",
    "    sentences = [split_into_sentences(i) for i in tqdm(lines, 'Sentence Splitter')]\n",
    "    \n",
    "    #remove all sentences with less than 5 tokens\n",
    "    all_sentences = []\n",
    "    for doc in tqdm(sentences, 'Filter Senteces'):\n",
    "        for sentence in doc:\n",
    "            if len(sentence.split()) > 4:\n",
    "                all_sentences.append(sentence)\n",
    "    print(f'Return a list of {len(all_sentences)} sentences')\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7703ca1a-62b2-4ab3-98a1-fd38aad080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfeece3e-eb04-4b8b-a6df-23d80150d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n"
     ]
    }
   ],
   "source": [
    "#results = load_data_from_disk(os.path.join(vm_data, ))\n",
    "start = time.perf_counter()\n",
    "results = load_data_seq_512(os.path.join(vm_data, files[0]))\n",
    "end = time.perf_counter() - start\n",
    "print(round(end, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a0372d-189a-44e1-a1aa-7ebcc6f86f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29b03e9a-ca9a-42fb-a0d4-cf18c39fdbd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# segments = []\n",
    "# temp_holder = []\n",
    "# def create_512_sequences(\n",
    "# for sentence in tqdm(results, 'Tokenizing'):\n",
    "#     tokens = alternative_tokenizer.encode(sentence)\n",
    "#     if len(temp_holder) + len(tokens) < 512:\n",
    "#         temp_holder.extend(tokens[1:-1])\n",
    "#     else:\n",
    "#         temp_holder.insert(0,2)\n",
    "#         segments.append(temp_holder + [3])\n",
    "#         temp_holder = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4bc6f-5c8f-4d24-b4ca-c9d2b7b9f73f",
   "metadata": {},
   "source": [
    "#### Batch encode a chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fc29f8-e7aa-42bc-ba2b-79aad83462f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.88 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(results)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "352e2d26-4384-401d-a130-53b14af0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrease load on memory\n",
    "del results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568788a-5ffe-4376-a72e-1172141d8c00",
   "metadata": {},
   "source": [
    "#### Create pipeline for random masking of 15% of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9386eae-bc2e-4edb-b158-28e3a71748cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c4dd28-12ac-41d2-a2e0-ebc2bd68a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b330d8bad0d440e830c5f0c288b9c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c1104de6a84e5cb8c42bb944dbe2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attention Mask:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765b5446c246409ca4fc2b0a7bc9afa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking Words:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodings = mlm_pipe(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c370c7-4cc3-4fbe-a331-37fdd4acc05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1496)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2baae0cd-f767-438c-87fd-aa2b7da3c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {key : tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "745683a2-fb73-4805-a5bd-1cee5bcf47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(encodings)\n",
    "del batch\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a1ed71-47e7-43b2-9d3a-63985fce024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(d, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b099a0-8432-4d46-ae55-27bc73e100ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True,\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 32,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "          \"lr\": 0.001,\n",
    "          \"betas\": [\n",
    "            0.8,\n",
    "            0.999\n",
    "          ],\n",
    "          \"eps\": 1e-8,\n",
    "          \"weight_decay\": 3e-7\n",
    "                    }\n",
    "    },\n",
    "\n",
    "     \"scheduler\": {\n",
    "      \"type\": \"WarmupLR\",\n",
    "      \"params\": {\n",
    "          \"warmup_min_lr\": 0,\n",
    "          \"warmup_max_lr\": 0.001,\n",
    "          \"warmup_num_steps\": 1000\n",
    "                }\n",
    "    },\n",
    "    \n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"train_batch_size\": BATCH_SIZE\n",
    "    #\"train_micro_batch_size_per_gpu\":BATCH_SIZE\n",
    "    }\n",
    "hfds_config = HfDeepSpeedConfig(ds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d79ee7a9-0c6c-4fcc-b6ea-1831cc4147b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=30500,num_hidden_layers=12)\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7775707d-efdc-4564-ab09-20dae9d08b03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' \n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02fa006-3f28-47f1-8b47-017aac7de024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-03-26 19:21:56,991] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.0, git-hash=unknown, git-branch=unknown\n",
      "[2022-03-26 19:21:56,993] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...\n",
      "--------------------------------------------------------------------------\n",
      "[[44922,1],0]: A high-performance Open MPI point-to-point messaging module\n",
      "was unable to find any relevant network interfaces:\n",
      "\n",
      "Module: OpenFabrics (openib)\n",
      "  Host: bert-pretraining-vm\n",
      "\n",
      "Another transport will be used instead, although this may result in\n",
      "lower performance.\n",
      "\n",
      "NOTE: You can disable this warning by setting the MCA parameter\n",
      "btl_base_warn_component_unused to 0.\n",
      "--------------------------------------------------------------------------\n",
      "[2022-03-26 19:21:57,232] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.2.0.5, master_port=29500\n",
      "[2022-03-26 19:21:57,234] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl\n",
      "[2022-03-26 19:22:00,398] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/americanthinker/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/americanthinker/.cache/torch_extensions/py38_cu111/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.745271682739258 seconds\n",
      "[2022-03-26 19:22:04,981] [INFO] [engine.py:1064:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2022-03-26 19:22:04,989] [INFO] [engine.py:1072:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2022-03-26 19:22:04,990] [INFO] [utils.py:48:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2022-03-26 19:22:04,990] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
      "[2022-03-26 19:22:04,991] [INFO] [stage_1_and_2.py:125:__init__] Reduce bucket size 200000000.0\n",
      "[2022-03-26 19:22:04,991] [INFO] [stage_1_and_2.py:126:__init__] Allgather bucket size 200000000.0\n",
      "[2022-03-26 19:22:04,992] [INFO] [stage_1_and_2.py:127:__init__] CPU Offload: True\n",
      "[2022-03-26 19:22:04,992] [INFO] [stage_1_and_2.py:128:__init__] Round robin gradient partitioning: False\n",
      "Using /home/americanthinker/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/americanthinker/.cache/torch_extensions/py38_cu111/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.31125402450561523 seconds\n",
      "Rank: 0 partition count [1] and sizes[(109497380, False)] \n",
      "[2022-03-26 19:22:05,824] [INFO] [utils.py:824:see_memory_usage] Before initializing optimizer states\n",
      "[2022-03-26 19:22:05,826] [INFO] [utils.py:825:see_memory_usage] MA 0.25 GB         Max_MA 0.25 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2022-03-26 19:22:05,827] [INFO] [utils.py:833:see_memory_usage] CPU Virtual Memory:  used = 7.56 GB, percent = 3.4%\n",
      "[2022-03-26 19:22:06,281] [INFO] [utils.py:824:see_memory_usage] After initializing optimizer states\n",
      "[2022-03-26 19:22:06,283] [INFO] [utils.py:825:see_memory_usage] MA 0.25 GB         Max_MA 0.25 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2022-03-26 19:22:06,284] [INFO] [utils.py:833:see_memory_usage] CPU Virtual Memory:  used = 8.79 GB, percent = 4.0%\n",
      "[2022-03-26 19:22:06,284] [INFO] [stage_1_and_2.py:497:__init__] optimizer state initialized\n",
      "[2022-03-26 19:22:06,407] [INFO] [utils.py:824:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2022-03-26 19:22:06,408] [INFO] [utils.py:825:see_memory_usage] MA 0.25 GB         Max_MA 0.25 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2022-03-26 19:22:06,410] [INFO] [utils.py:833:see_memory_usage] CPU Virtual Memory:  used = 8.79 GB, percent = 4.0%\n",
      "[2022-03-26 19:22:06,410] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2022-03-26 19:22:06,411] [INFO] [engine.py:776:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2022-03-26 19:22:06,411] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fd6201807c0>\n",
      "[2022-03-26 19:22:06,412] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2022-03-26 19:22:06,412] [INFO] [config.py:1058:print] DeepSpeedEngine configuration:\n",
      "[2022-03-26 19:22:06,413] [INFO] [config.py:1062:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-03-26 19:22:06,414] [INFO] [config.py:1062:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-03-26 19:22:06,414] [INFO] [config.py:1062:print]   amp_enabled .................. False\n",
      "[2022-03-26 19:22:06,415] [INFO] [config.py:1062:print]   amp_params ................... False\n",
      "[2022-03-26 19:22:06,416] [INFO] [config.py:1062:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-03-26 19:22:06,416] [INFO] [config.py:1062:print]   bfloat16_enabled ............. False\n",
      "[2022-03-26 19:22:06,416] [INFO] [config.py:1062:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-03-26 19:22:06,417] [INFO] [config.py:1062:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-03-26 19:22:06,417] [INFO] [config.py:1062:print]   communication_data_type ...... None\n",
      "[2022-03-26 19:22:06,418] [INFO] [config.py:1062:print]   curriculum_enabled ........... False\n",
      "[2022-03-26 19:22:06,418] [INFO] [config.py:1062:print]   curriculum_params ............ False\n",
      "[2022-03-26 19:22:06,418] [INFO] [config.py:1062:print]   dataloader_drop_last ......... False\n",
      "[2022-03-26 19:22:06,419] [INFO] [config.py:1062:print]   disable_allgather ............ False\n",
      "[2022-03-26 19:22:06,419] [INFO] [config.py:1062:print]   dump_state ................... False\n",
      "[2022-03-26 19:22:06,423] [INFO] [config.py:1062:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
      "[2022-03-26 19:22:06,423] [INFO] [config.py:1062:print]   eigenvalue_enabled ........... False\n",
      "[2022-03-26 19:22:06,424] [INFO] [config.py:1062:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-03-26 19:22:06,424] [INFO] [config.py:1062:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-03-26 19:22:06,425] [INFO] [config.py:1062:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-03-26 19:22:06,425] [INFO] [config.py:1062:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-03-26 19:22:06,425] [INFO] [config.py:1062:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-03-26 19:22:06,426] [INFO] [config.py:1062:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-03-26 19:22:06,426] [INFO] [config.py:1062:print]   eigenvalue_verbose ........... False\n",
      "[2022-03-26 19:22:06,427] [INFO] [config.py:1062:print]   elasticity_enabled ........... False\n",
      "[2022-03-26 19:22:06,428] [INFO] [config.py:1062:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-03-26 19:22:06,429] [INFO] [config.py:1062:print]   fp16_enabled ................. True\n",
      "[2022-03-26 19:22:06,429] [INFO] [config.py:1062:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-03-26 19:22:06,430] [INFO] [config.py:1062:print]   fp16_mixed_quantize .......... False\n",
      "[2022-03-26 19:22:06,430] [INFO] [config.py:1062:print]   global_rank .................. 0\n",
      "[2022-03-26 19:22:06,430] [INFO] [config.py:1062:print]   gradient_accumulation_steps .. 1\n",
      "[2022-03-26 19:22:06,431] [INFO] [config.py:1062:print]   gradient_clipping ............ 1.0\n",
      "[2022-03-26 19:22:06,431] [INFO] [config.py:1062:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-03-26 19:22:06,432] [INFO] [config.py:1062:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-03-26 19:22:06,432] [INFO] [config.py:1062:print]   loss_scale ................... 0\n",
      "[2022-03-26 19:22:06,432] [INFO] [config.py:1062:print]   memory_breakdown ............. False\n",
      "[2022-03-26 19:22:06,433] [INFO] [config.py:1062:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-03-26 19:22:06,433] [INFO] [config.py:1062:print]   optimizer_name ............... adam\n",
      "[2022-03-26 19:22:06,435] [INFO] [config.py:1062:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2022-03-26 19:22:06,436] [INFO] [config.py:1062:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-03-26 19:22:06,436] [INFO] [config.py:1062:print]   pld_enabled .................. False\n",
      "[2022-03-26 19:22:06,436] [INFO] [config.py:1062:print]   pld_params ................... False\n",
      "[2022-03-26 19:22:06,437] [INFO] [config.py:1062:print]   prescale_gradients ........... False\n",
      "[2022-03-26 19:22:06,437] [INFO] [config.py:1062:print]   quantize_change_rate ......... 0.001\n",
      "[2022-03-26 19:22:06,438] [INFO] [config.py:1062:print]   quantize_groups .............. 1\n",
      "[2022-03-26 19:22:06,438] [INFO] [config.py:1062:print]   quantize_offset .............. 1000\n",
      "[2022-03-26 19:22:06,438] [INFO] [config.py:1062:print]   quantize_period .............. 1000\n",
      "[2022-03-26 19:22:06,439] [INFO] [config.py:1062:print]   quantize_rounding ............ 0\n",
      "[2022-03-26 19:22:06,439] [INFO] [config.py:1062:print]   quantize_start_bits .......... 16\n",
      "[2022-03-26 19:22:06,440] [INFO] [config.py:1062:print]   quantize_target_bits ......... 8\n",
      "[2022-03-26 19:22:06,440] [INFO] [config.py:1062:print]   quantize_training_enabled .... False\n",
      "[2022-03-26 19:22:06,440] [INFO] [config.py:1062:print]   quantize_type ................ 0\n",
      "[2022-03-26 19:22:06,441] [INFO] [config.py:1062:print]   quantize_verbose ............. False\n",
      "[2022-03-26 19:22:06,441] [INFO] [config.py:1062:print]   scheduler_name ............... WarmupLR\n",
      "[2022-03-26 19:22:06,442] [INFO] [config.py:1062:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
      "[2022-03-26 19:22:06,442] [INFO] [config.py:1062:print]   sparse_attention ............. None\n",
      "[2022-03-26 19:22:06,442] [INFO] [config.py:1062:print]   sparse_gradients_enabled ..... False\n",
      "[2022-03-26 19:22:06,443] [INFO] [config.py:1062:print]   steps_per_print .............. 10\n",
      "[2022-03-26 19:22:06,443] [INFO] [config.py:1062:print]   tensorboard_enabled .......... False\n",
      "[2022-03-26 19:22:06,444] [INFO] [config.py:1062:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-03-26 19:22:06,444] [INFO] [config.py:1062:print]   tensorboard_output_path ...... \n",
      "[2022-03-26 19:22:06,444] [INFO] [config.py:1062:print]   train_batch_size ............. 20\n",
      "[2022-03-26 19:22:06,445] [INFO] [config.py:1062:print]   train_micro_batch_size_per_gpu  20\n",
      "[2022-03-26 19:22:06,445] [INFO] [config.py:1062:print]   use_quantizer_kernel ......... False\n",
      "[2022-03-26 19:22:06,446] [INFO] [config.py:1062:print]   wall_clock_breakdown ......... False\n",
      "[2022-03-26 19:22:06,446] [INFO] [config.py:1062:print]   world_size ................... 1\n",
      "[2022-03-26 19:22:06,446] [INFO] [config.py:1062:print]   zero_allow_untested_optimizer  False\n",
      "[2022-03-26 19:22:06,447] [INFO] [config.py:1062:print]   zero_config .................. {\n",
      "    \"stage\": 2, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 2.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 2.000000e+08, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": true, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-03-26 19:22:06,451] [INFO] [config.py:1062:print]   zero_enabled ................. True\n",
      "[2022-03-26 19:22:06,452] [INFO] [config.py:1062:print]   zero_optimization_stage ...... 2\n",
      "[2022-03-26 19:22:06,452] [INFO] [config.py:1064:print]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 32, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"train_batch_size\": 20\n",
      "}\n",
      "Using /home/americanthinker/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0015246868133544922 seconds\n"
     ]
    }
   ],
   "source": [
    "engine = deepspeed.initialize(model=model,\n",
    "                              model_parameters=model.parameters(), \n",
    "                              config=ds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa84c0ff-3f6a-4771-9be1-7147aa8d2466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = engine[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e979b297-c9e3-4c9c-9b29-a5027503d11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-03-26 19:29:22,721] [INFO] [engine.py:3112:save_16bit_model] Saving model weights to ./checkpoints/pytorch_model_test_deepspeed.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save_16bit_model('./checkpoints/', save_filename='pytorch_model_test_deepspeed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f596e014-9b05-4e8f-9e7e-f8c734b5d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 25 17:18:29 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    39W / 250W |   1983MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    39W / 250W |   1947MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     1979MiB |\n",
      "|    1   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     1943MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f41bbc9a-1282-42b2-a1ee-cc729427cc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_counts = []\n",
    "\n",
    "def early_stopping(total_loss, tol: float = 0.01, early_stopping: int=10):\n",
    "    if total_loss < lowest_loss:\n",
    "        lowest_loss = total_loss\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8438e07-6676-44f1-ba7a-f1b4994b3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def run():\n",
    "    num_batches = len(loader)\n",
    "    epochs = 3\n",
    "    step = 0\n",
    "    lowest_loss = 10000\n",
    "    tolerance = 0.01\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # for file in files[:2]:\n",
    "        #     # setup loop with TQDM and dataloader\n",
    "        #     results = load_data_from_disk(os.path.join(vm_data, file))\n",
    "        #     batch = tokenizer.encode_batch(results)\n",
    "        #     del results\n",
    "        #     encodings = mlm_pipe(batch)\n",
    "        #     del batch\n",
    "        #     d = Dataset(encodings)\n",
    "        #     del encodings\n",
    "        #     loader = torch.utils.data.DataLoader(d, batch_size=384, pin_memory=True, shuffle=True)\n",
    "\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            step += 1\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optimizer.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.sum().backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # print relevant info to progress barI \n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                print(f'Loss: {loss.sum()}')\n",
    "                \n",
    "        model.module.save_pretrained(f'checkpoints/test_save_pretrained/model-trained-{step}.pt') \n",
    "            \n",
    "        #loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b59c3cc-f918-4c0f-9ac9-33c1f2ed41c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1615476608276367\n",
      "Loss: 2.114736557006836\n",
      "Loss: 2.025595188140869\n",
      "Loss: 2.143763780593872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9414606094360352\n",
      "Loss: 1.926030158996582\n",
      "Loss: 1.8185341358184814\n",
      "Loss: 1.5966253280639648\n",
      "Loss: 1.2994054555892944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.251365303993225\n",
      "Loss: 1.2248347997665405\n",
      "Loss: 1.126413106918335\n",
      "Loss: 1.0757315158843994\n",
      "Loss: 0.9106765985488892\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99dcc579-7540-44e6-8456-fea1fd3783d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 25 20:00:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   30C    P0    35W / 250W |  15641MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    35W / 250W |  13213MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python    15637MiB |\n",
      "|    1   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python    13209MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47a605de-2c66-480b-9b47-64d9916a4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "281c654a-ec72-4bf0-9525-50d2aee5d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 25 20:01:03 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    39W / 250W |   3701MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    39W / 250W |   1519MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     3697MiB |\n",
      "|    1   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     1515MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af052351-64b6-43bf-abdb-adc84d695e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = alternative_tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce768ed3-ecf3-4794-b123-3b7fb5ff5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(text: str):\n",
    "    config = BertConfig(vocab_size=30500)\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    untrained_pipe = pipeline('fill-mask', model=model, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n",
    "    utresult = untrained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Untrained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in utresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "        \n",
    "    lm = BertForMaskedLM.from_pretrained('checkpoints/test_save_pretrained/model-trained-14000.pt/')\n",
    "    trained_pipe = pipeline('fill-mask', model=lm, tokenizer=alternative_tokenizer)\n",
    "\n",
    "    tresult = trained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Trained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in tresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6751054-f4ad-4f98-8d83-f42ef5bde42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Untrained Results\n",
      "******************************************************************************************************************************************************\n",
      "it is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.7136348485946655\n",
      "insulin is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.014016539789736271\n",
      "amp is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.01275827456265688\n",
      "cox is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.012082782573997974\n",
      "notch is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.01053623203188181\n",
      "\n",
      "Trained Results\n",
      "******************************************************************************************************************************************************\n",
      "it is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.23749612271785736\n",
      "there is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.11453662812709808\n",
      "acetylcholine is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.030285345390439034\n",
      "dopamine is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.029437104240059853\n",
      "neurotransmitter is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.025556039065122604\n"
     ]
    }
   ],
   "source": [
    "show_results(f'{mask} is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7df956ec-5c19-46af-b9e5-ba9cf169a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/subsets/xaasplit_25K') as f:\n",
    "    data = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66048ed-aa14-4d6b-ab36-c305184cf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.encode_batch(['Introduction Under normal physiological conditions, all cells in the body are exposed chronically to oxidants from both endogenous and exogenous sources;', 'This is another sentence.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fe76d4-9eeb-423b-a152-86b7e3fce8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([x.ids for x in test])\n",
    "mask = torch.tensor([x.attention_mask for x in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e74e514-2e72-4821-b9d9-9e76318a1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = new_model(input_ids, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a91f911-d6b9-4725-9b5e-eedd8b85c595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 30500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "784c568d-efb4-4232-ad1f-4f7bb5c16262",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbut = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e966560-6332-4b83-a87a-c81cf9b2c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_pretrained('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524614ae-7c88-44e9-9e6e-46e33c03141e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=30500, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9218ed6e-7b0a-475d-9dc9-6d9850bbf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c5b2725-4486-4dac-bdb5-a05ce1cd1d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./checkpoints/deepspeed_model/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./checkpoints/deepspeed_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('./checkpoints/deepspeed_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbf57ba9-e54b-4ea4-8a9c-a51bef9afaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30500, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15b50f-ee64-4808-b923-6f0efd9f7bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
