{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494fc40d-0386-4b40-9ac6-1584957f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import os, time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "#distributed imports\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, DataParallel\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "\n",
    "#tokenizers and datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "\n",
    "#transformer imports\n",
    "from transformers import BertTokenizer, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling\n",
    "from transformers import BertForMaskedLM, BertConfig, AdamW, TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b42db8b-50b0-4f36-88eb-a52b4d3d8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n"
     ]
    }
   ],
   "source": [
    "for d in range(4):\n",
    "    print(torch.cuda.get_device_properties(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265b9168-bbcf-4eaa-85b2-4f32ece9b3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712fb9-5a4f-4dc8-907e-3a4abd1f2b57",
   "metadata": {},
   "source": [
    "#### Set Tokenizer and Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d793b3-6694-4ab6-b76e-1c31718f3317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xabsplit_25K', 'xadsplit_25K', 'xacsplit_25K', 'xaasplit_25K']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_tok_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "vm_data = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/subsets/'\n",
    "checkpoint_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/'\n",
    "files = [f for f in os.listdir(vm_data) if f.endswith('25K')]\n",
    "files\n",
    "#local paths\n",
    "# local_tok_path = '/Users/americanthinker1/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "# local_data = '/Users/americanthinker1/aws_data/processed_data/processed_chunks/english_docs_aa.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8631b6b-5d24-4637-94bb-2cec5ad36bce",
   "metadata": {},
   "source": [
    "#### Instantiate pretrained tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23d3e2f-b515-4bc3-9dd2-e7d2a87f5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1618: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=50)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e5dd6e-8616-4403-86f3-25b2a039a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_model('../Preprocessing/Tokenization/', prefix='BWPTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbff84-1369-4b4a-90c8-80c276a56264",
   "metadata": {},
   "source": [
    "#### Load data from local\n",
    "Data is a 98,000 line file with each line representing one document of length ~12,000 characters from PubMed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57250a2d-1cbf-4273-8aa1-2881ceb46f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from disk\n",
    "def load_data_from_disk(path: str, sample_size=None, min_tokens_per_sent: int=4) -> List[str]:\n",
    "    '''\n",
    "    Utility data loading function that performs the following operations:\n",
    "       1. Loads data from disk into a list. Assumes each doc is one line.\n",
    "       2. Performs sentence splitting on each document.\n",
    "       3. Removes all sentences with tokens < 4 (default).\n",
    "       4. Returns a list of sentences \n",
    "    '''\n",
    "    #load data\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    #split data into sentences\n",
    "    sentences = [split_into_sentences(i) for i in tqdm(lines, 'Sentence Splitter')]\n",
    "    \n",
    "    #remove all sentences with less than 5 tokens\n",
    "    all_sentences = []\n",
    "    for doc in tqdm(sentences, 'Filter Senteces'):\n",
    "        for sentence in doc:\n",
    "            if len(sentence.split()) > 4:\n",
    "                all_sentences.append(sentence)\n",
    "    print(f'Return a list of {len(all_sentences)} sentences')\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b9c2b26-8066-47c8-86df-3a1cd048d428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97ebcb47e76428db6d78accd0e770dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence Splitter:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9a7121b974697896e2d02750fee1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter Senteces:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a list of 71680 sentences\n"
     ]
    }
   ],
   "source": [
    "results = load_data_from_disk(os.path.join(vm_data, files[0]), sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4bc6f-5c8f-4d24-b4ca-c9d2b7b9f73f",
   "metadata": {},
   "source": [
    "#### Batch encode a chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5fc29f8-e7aa-42bc-ba2b-79aad83462f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(results)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "352e2d26-4384-401d-a130-53b14af0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrease load on memory\n",
    "del results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568788a-5ffe-4376-a72e-1172141d8c00",
   "metadata": {},
   "source": [
    "#### Create pipeline for random masking of 15% of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9386eae-bc2e-4edb-b158-28e3a71748cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.20) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')], dtype=torch.int16)\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')], dtype=torch.int16)\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2c4dd28-12ac-41d2-a2e0-ebc2bd68a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3acd904f72c482cbc84786ac1b04af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/71680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427e2f857c4749788cab46538897a08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attention Mask:   0%|          | 0/71680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538719ffe8354b73a69789c4aeae6776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking Words:   0%|          | 0/71680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodings = mlm_pipe(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e6c370c7-4cc3-4fbe-a331-37fdd4acc05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1184)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2baae0cd-f767-438c-87fd-aa2b7da3c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {key : tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "745683a2-fb73-4805-a5bd-1cee5bcf47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(encodings)\n",
    "del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "73a1ed71-47e7-43b2-9d3a-63985fce024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(d, batch_size=384, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79ee7a9-0c6c-4fcc-b6ea-1831cc4147b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=30500, max_position_embeddings=514, num_hidden_layers=12)\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7775707d-efdc-4564-ab09-20dae9d08b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c02fa006-3f28-47f1-8b47-017aac7de024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f41bbc9a-1282-42b2-a1ee-cc729427cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def save_model(path: './', multiple_gpu: bool=True):\n",
    "    if multiple_gpu:\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "              f'{path}model_{step}.pt')\n",
    "    else:\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "              f'{path}model_{step}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8438e07-6676-44f1-ba7a-f1b4994b3151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d99e5f9d39447619d76c553e6bb7875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "num_batches = len(loader)\n",
    "epochs = 2\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        step += 1\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.sum().backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress barI \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        \n",
    "        if step % floor(num_batches/10) == 0:\n",
    "            save_model()\n",
    "            \n",
    "        #loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b59c3cc-f918-4c0f-9ac9-33c1f2ed41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99dcc579-7540-44e6-8456-fea1fd3783d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Mar 23 01:19:19 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    26W / 250W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    25W / 250W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  Off  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    25W / 250W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  Off  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    25W / 250W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618960e-ec74-4892-a2b3-c66fc8dbff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OOP way to train model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=2,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=8, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=1,  # accumulating the gradients before updating the weight\n",
    "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=500,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "755f2f5a-629a-4a6b-a33f-27dcdff08889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_: Union[str, pathlib.Path], parallel=False):\n",
    "    checkpoint = torch.load(checkpoint_)\n",
    "    model_state = checkpoint['model_state_dict']\n",
    "    opt_state = checkpoint['optimizer_state_dict']\n",
    "    model_loss = checkpoint['loss']\n",
    "    model.load_state_dict(model_state)\n",
    "    if parallel:\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = DataParallel(model)\n",
    "    return model, opt_state, model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af052351-64b6-43bf-abdb-adc84d695e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = alternative_tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce768ed3-ecf3-4794-b123-3b7fb5ff5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(text: str):\n",
    "    config = BertConfig(vocab_size=30500, max_position_embeddings=514, num_hidden_layers=12)\n",
    "    model = BertForMaskedLM(config)\n",
    "    untrained_pipe = pipeline('fill-mask', model=model, tokenizer=alternative_tokenizer)\n",
    "    utresult = untrained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Untrained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in utresult:\n",
    "        print(result)\n",
    "        \n",
    "    trained_model, opt, model_loss = load_checkpoint(model, 'checkpoints/final_model_18710.pt')\n",
    "    trained_pipe = pipeline('fill-mask', model=trained_model, tokenizer=alternative_tokenizer)\n",
    "\n",
    "    tresult = trained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Trained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in tresult:\n",
    "        print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6751054-f4ad-4f98-8d83-f42ef5bde42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Untrained Results\n",
      "******************************************************************************************************************************************************\n",
      "{'sequence': 'pomegranate is a popular fruit grown inaked with a large annual production rate.', 'score': 0.00029260278097353876, 'token': 12730, 'token_str': '# # a k e d'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown inouracil with a large annual production rate.', 'score': 0.0002666855289135128, 'token': 20742, 'token_str': '# # o u r a c i l'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in paradig with a large annual production rate.', 'score': 0.0002603462780825794, 'token': 8810, 'token_str': 'p a r a d i g'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in −x with a large annual production rate.', 'score': 0.0002488254103809595, 'token': 27163, 'token_str': '− x'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in specially with a large annual production rate.', 'score': 0.00023313777637667954, 'token': 21916, 'token_str': 's p e c i a l l y'}\n",
      "\n",
      "Trained Results\n",
      "******************************************************************************************************************************************************\n",
      "{'sequence': 'pomegranate is a popular fruit grown in china with a large annual production rate.', 'score': 0.19407474994659424, 'token': 3588, 'token_str': 'c h i n a'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in europe with a large annual production rate.', 'score': 0.04890244081616402, 'token': 4345, 'token_str': 'e u r o p e'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in australia with a large annual production rate.', 'score': 0.03206990286707878, 'token': 7312, 'token_str': 'a u s t r a l i a'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in countries with a large annual production rate.', 'score': 0.030651113018393517, 'token': 3753, 'token_str': 'c o u n t r i e s'}\n",
      "{'sequence': 'pomegranate is a popular fruit grown in areas with a large annual production rate.', 'score': 0.02832319214940071, 'token': 3484, 'token_str': 'a r e a s'}\n"
     ]
    }
   ],
   "source": [
    "show_results(f'Pomegranate is a popular fruit grown in {mask} with a large annual production rate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7df956ec-5c19-46af-b9e5-ba9cf169a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/subsets/xaasplit_25K') as f:\n",
    "    data = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad05d663-b579-49bf-8716-10b0669f6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction Under normal physiological conditions, all cells in the body are exposed chronically to oxidants from both endogenous and exogenous sources; yet the intracellular “redox buffer” mechanism provides significant protection mainly by the antioxidant network [1]. Disturbance in the prooxidant- antioxidant balance in favor of the former leads to what is known as oxidative stress [2]. This oxidative stress and reactive oxygen species (ROS) can cause damage to DNA, proteins and lipids an d end up with an epidemic of non communicable chronic human diseases [3–5]. The prevalence of NCD are at escalating in Egypt due to activation of 64 genes involved in inflammation [6, 7] and other modifiable risk factors [8]. Medical and pharmacologic chemotherapeutic agents were reported to reduce cardio vascular mortality among individuals at risk, but they may induce oxidative stress, which increases to an invasive stage with disease progression [9]. Plant polyphenols possess the ideal chemical structure for free radical scavenging activity and their in vitro antioxidative activities are more effective than tocopherols and ascorbate [10]. Designing effective preventive strategies using naturally occurring phytonutrients aiming at reducing oxidative stress is one of the cost-effective strategies to move people’s lifestyle toward healthier behaviors [11]. Pomegranate is a popular fruit grown in Egypt with annual production of approximately 130000 tons [12]. Natural unprocessed pomegranate juice (PGJ) is superior to commercial juices in their polyphenol (PP) contents with mean levels of 421 and 382\\xa0mg per 100\\xa0ml, respectively. Another investigation reported respective (PP) concentrations of 139\\xa0mg gallic acid equivalent (GAE) per 100\\xa0ml juice and may reach over 200\\xa0mg GAE/ 100\\xa0ml, when the other phenolic compounds; anthocyamin, ellagitannins, and tannin punicalagin were included [13]. Dietary pomegranate intake in human trials elevated urinary excretion of the above mentioned phenolic metabolites, which are the bioactive constituents responsible for more than >50\\xa0% of the antioxidative capacity activity of the juice [14, 15] and are the biomarkers linked to health promotion [16]. Measurement of GST activity had been recommended for the evaluation of protective treatment in trials considering antioxidant strategies. The expression of the phase II hepatic glutathione S-transferase was activated in the liver cells of animals following feeding pomegranate anthocyanins flavonoids. The molecular mechanism was related to activation of antioxidant response element (ARE) upstream of genes that regulate the expression of GST [17]. Specific strains of lactic acid bacteria (LAB) possess also antioxidative ability, scavenge reactive oxygen species and chelate metal ions, which provide protection against oxidative stress and lipid peroxidation [18, 19]. The regular intake of LAB fermented foods such as sourdough [20], water kefir [21, 22] and sour sobya [23] containing specific strains of (LAB) with antioxidative capacity activity, contribute to their health effects. Furthermore, the oral administration of some strains of LAB (L plantarum) to humans was capable to break down phenolic acids and hydrolysable tannins into phenolic metabolites that are more easily absorbed in the body and enhancing the gut antioxidative effects [24]. In vivo and ex vivo functional biomarkers such as thiobarbituric acid reactive substances (TBARS) are developed for assessing exposure to antioxidant and to lipid oxidation and oxidative stress status [25]. The objective of the present study is to assess the effectiveness of dietary intervention with apricots (AP), pomegranate juice (PGJ), LAB fermented sour sobya (FS) or mixtures of PGJ and FS by healthy adults on biomarkers of antioxidative capacity activity (AEAC), oxidative stress (TBARS) and phase II glutathione-S-transferase enzyme activity [E- GST]. Subjects and methods A Randomized controlled trial (RCT) was conducted on 35 healthy Egyptian adults (25 men and 10 women) between the ages of 21– 35\\xa0years (mean age 27. 8\\u2009±\\u20090. 96\\xa0years). The study received the approval from the research ethical committee (REC) decision No 12-2013-9, Ministry of Health & population, Egypt, which complied with the Declaration of Helsinki guidelines (2004). The protocol was fully explained to all subjects and written informed consent was obtained before their participation in the trial. Inclusion criteria were healthy adults aging 21 – 35\\xa0years. Exclusion criteria at the time of the screening were: History of diabetes, hypertension, heart disease, endocrine disorders, abnormal blood chemistry profile, fasting LDL-cholesterol concentration\\u2009>\\u20093. 37\\xa0mmol/L, fasting triacylglycerol concentration\\u2009>\\u20093. 39\\xa0mmol/L, smoking, use of antioxidant, vitamin supplements or having taken part in any dietary intervention trials. Female subjects were neither pregnant nor lactating. The volunteers were instructed to continue to eat their normal diet and not to alter their usual dietary or fluid intake with the exception of the foods and beverages that are high in dietary flavonoids, such as purple grapes, cocoa and chocolate during the entire three week dietary intervention trial. Also As a Muslim country red wine drinking is unknown to almost all of the youths, because its drinking is prohibited by religion. Also Egypt is not a coffee drinking country. Volunteers who met the initial inclusion/exclusion criteria took part in the study and there were no withdrawals. Participant characteristics are summarized in Table\\xa01. Supplements Apricots (AP) (family Rosaceae; Prunus Armenia) and pomegranate (family Punicaceae; Punica granatum L. ) were purchased in batches from El Obour market. AP were stored immediately in packages each containing 200\\xa0g net (AP) and were saved in the refrigerator at 4 °C. Pomegranates were picked by hand, washed and stored in tanks. The fruit was crushed and squeezed in an electric juicer (Brand, Germany) and the juice (PGJ) (approximately 30\\xa0% of the fruit weight) was packed immediately in 250\\xa0ml or 100\\xa0ml portions in air tight polyethylene bottle with screw caps and frozen at -200 °C. Lactic acid bacteria fermented sour sobya (FS) was purchased from the retail market (El Rahmany brand) once weekly and saved in the refrigerator. Table 1 Baseline characteristics of the study subjects  Parameter\\tUnit\\tx̄ ±SE\\tMedian\\tQ1\\tQ3\\t Age\\tyears\\t27. 8±0. 96\\t27. 00\\t24. 00\\t29. 75\\t BMI\\tKg. m-1 \\t23. 7±0. 58\\t23. 57\\t21. 48\\t25. 98\\t Estimated energy intake\\tKcal/d\\t1969±138. 78\\t1903. 0\\t1555\\t2262\\t Plasma Antioxidant activity\\tAEAC/100ml\\t3. 79±0. 33\\t3. 37\\t2. 92\\t4. 27\\t Erythrocyte GST Activity\\tIU/g Hb\\t4. 65±0. 46\\t4. 09\\t2. 73\\t6. 23\\t Urinary polyphenols\\tGAE/mg creat\\t11. 45±2. 57\\t6. 96\\t4. 89\\t12. 59\\t Urinary Antioxidant activity\\tAEAC/mg creat*\\t7. 21±0. 84\\t6. 46\\t4. 04\\t8. 55\\t Urinary TBA\\tμg/mg creat\\t114. 75±17. 37\\t83. 30\\t47. 76\\t141. 71\\t x̄ ±SE: Mean ± standard Error, * : mmol ascorbic acid equivalent antioxidant capacity / mg creatinine),  GAE : gallic acid equivalent     Design of the RCT The volunteers were divided into five groups of equal size and each group consisted of 5 men and 2 women. The first group served as control and received no supplements; the second group received fresh (AP) fruits (200\\xa0g); the third (PGJ) (250\\xa0g), the fourth a mixture of PGJ (150\\xa0g) and FS (140\\xa0g) and the fifth group received (FS) (170\\xa0g). The portions were reasonable dose to eat or to drink over the course of a day and the volunteers were given instructions to consume the portion between 5 – 7\\xa0pm. Dietary assessment Three-day food diaries (including two weekdays and one weekend day) were completed by the participants to monitor the subjects’ dietary compliances and assess their nutrient intakes. Dietary intake were calculated using a computer-aided nutritional analysis program (Nutri survey, Seoul, Korea). Venous blood collection One day before starting the trial and on day 22, Blood was taken into vacutainer tubes containing citrate, under quality control. The plasma was separated from blood by centrifugation in a cooling centrifuged at 3500\\xa0rpm for 10\\xa0min. Collected plasma were stored at -200 °C until analysis. The red blood cells were washed with cold physiological saline and the packed red blood cells were stored at – 200C. Urine samples were collected from all volunteers at day zero and day 22 and saved frozen at -20\\xa0°C. Blood and urine collection and storage Overnight fasted blood samples (3\\xa0ml) were collected one day before starting the dietary intervention and after 3\\xa0weeks of the dietary intervention (Day 22). All blood samples were collected from the left antecubital vein between 8:00 and 9:00\\xa0am in tubes containing sodium citrate. The plasma was separated from blood cells by centrifugation at 3000\\xa0rpm for 15\\xa0min at 4o C and the separated plasma was stored at -700 C for later biochemical analysis. The red blood cells (RBCs) were washed using cold physiological saline solution and stored at -200C. Urine samples were collected in the early morning after the subjects had fasted 10–12\\xa0h both on −1, and +21 d dietary intervention and aliquots (2\\xa0mL) were immediately frozen at −20\\xa0°C for biochemical analysis. Laboratory investigations Determination of total polyphenols The solid-phase extraction (SPE) Cartridge (60\\xa0mg) Strata-C18 (Phenomenex,Torrance,CA, USA); Folin Ciocalteu reagent (Catalog No47641; Sigma, USA). MilliQ ultrapure water (Millipore Iberica, Madrid, Spain). All of the analytical steps were carried out in dim light. The PPs of AP and PGJ were extracted vigorously from 4\\xa0g AP or 10\\xa0g PGJ with 5\\xa0ml distilled water then with 20\\xa0ml absolute MetOH. After centrifugation in the cold, the supernatant was aspirated and concentrated in vaccuo to 4\\xa0ml [26]. Clean up step on Waters 3\\xa0ml HLB (60\\xa0mg) cartridges were equilibrated with the successive addition of absolute methanol (1\\xa0ml) and H2O (1\\xa0ml). Aliquots of the methanolic fruit extracts (2\\xa0ml) were loaded on the activated SPE; washing with the successive addition of H2O (10\\xa0ml) and 5\\xa0% aqueous MetOH (1\\xa0ml). The PP containing fraction was eluted after the addition of 5\\xa0ml absolute MetOH. The elute was concentrated under a stream of nitrogen to 2\\xa0ml. Aliquots (20\\xa0μl (were taken for the analysis of PP using the Folin Ciocalteau colorimetric method [27] and the results were expressed as gallic acid equivalent (GAE) per g apricot or g PGJ. Measurement of total polyphenolic compounds in urine Total polyphenols in the urine samples were assayed after the clean up step by the solid-phase extraction (SPE) as described earlier [28, 29]. Aliquots of the centrifuged urine samples (1\\xa0ml) were acidified with normal hydrochloric acid (17\\xa0μl) and were loaded onto an activated Waters Cartridge, preconditioned with 1\\xa0ml methanol and equilibrated with 1\\xa0ml of 1. 5\\xa0mol/l aqueous formic acid. The SPE cartridge was washed twice with 2\\xa0ml of the same formic acid and 2\\xa0ml of water-methanol (95:5 by volume) to elute the impurities. Elution of the (PP) was completed with a mixture containing 1\\xa0ml of methanol containing 1\\xa0ml/l of formic acid (2\\xa0%)- methanol- H2O (2/60/38\\xa0v/v/v). Aliquots of the elute (500\\xa0μl) were pipetted into Eppendorf vials for the colorimetric determination of the (PP). Successive addition of Folin_Ciocalteu reagent (50\\xa0μl), followed by 20\\xa0% sodium carbonate (600\\xa0μl). The tubes were left in the dark for 60\\xa0min and then the absorbance of the developed color was measured at 765\\xa0nm against a blank. A blank and standard working solutions of gallic acid (0. 215\\xa0mg/ml) were run in parallel. Urinary (PP) excretion was expressed as mg GAE per g of creatinine. Plasma antioxidative activity ascorbic acid equivalents (AEAC) The assay is based on the measurement of the scavenging ability of antioxidants towards the stable DPPH (1,1-diphenyl-2-picrylhydrazyl) radical compound [30].'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb908d-02ea-48f1-bde1-4df2a3f9f4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
