{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494fc40d-0386-4b40-9ac6-1584957f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import os, time, json, datetime, pytz\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from multiprocessing import Pool\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#distributed imports\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, DataParallel\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "import deepspeed\n",
    "\n",
    "#tokenizers and datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "\n",
    "#transformer imports\n",
    "from transformers import BertTokenizer, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling\n",
    "from transformers import BertForMaskedLM, BertConfig, AdamW, TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b42db8b-50b0-4f36-88eb-a52b4d3d8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla V100-PCIE-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "for d in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d72b91c-eb5c-437e-b8a9-f315335900b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 25 20:28:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    34W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    35W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712fb9-5a4f-4dc8-907e-3a4abd1f2b57",
   "metadata": {},
   "source": [
    "#### Set Tokenizer and Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4628a365-0f3a-4f62-bc00-f91b03376271",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as j:\n",
    "    data = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d793b3-6694-4ab6-b76e-1c31718f3317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english_docs_aa.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_tok_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "vm_data = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/'\n",
    "checkpoint_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/'\n",
    "files = [f for f in os.listdir(vm_data) if os.path.isfile(os.path.join(vm_data, f))]\n",
    "files\n",
    "#local paths\n",
    "# local_tok_path = '/Users/americanthinker1/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "# local_data = '/Users/americanthinker1/aws_data/processed_data/processed_chunks/english_docs_aa.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8631b6b-5d24-4637-94bb-2cec5ad36bce",
   "metadata": {},
   "source": [
    "#### Instantiate pretrained tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23d3e2f-b515-4bc3-9dd2-e7d2a87f5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b289eaa-72ae-461e-b5c2-eaede4b2b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_tokenizer.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbff84-1369-4b4a-90c8-80c276a56264",
   "metadata": {},
   "source": [
    "#### Load data from local\n",
    "Data is a 98,000 line file with each line representing one document of length ~12,000 characters from PubMed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57250a2d-1cbf-4273-8aa1-2881ceb46f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from disk\n",
    "def load_data_from_disk(path: str, sample_size:int=None, min_tokens_per_sent: int=4) -> List[str]:\n",
    "    '''\n",
    "    Utility data loading function that performs the following operations:\n",
    "       1. Loads data from disk into a list. Assumes each doc is one line.\n",
    "       2. Performs sentence splitting on each document.\n",
    "       3. Removes all sentences with tokens < 4 (default).\n",
    "       4. Returns a list of sentences \n",
    "    '''\n",
    "    #load data\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    #split data into sentences\n",
    "    sentences = [split_into_sentences(i) for i in tqdm(lines, 'Sentence Splitter')]\n",
    "    \n",
    "    #remove all sentences with less than 5 tokens\n",
    "    all_sentences = []\n",
    "    for doc in tqdm(sentences, 'Filter Senteces'):\n",
    "        for sentence in doc:\n",
    "            if len(sentence.split()) > 4:\n",
    "                all_sentences.append(sentence)\n",
    "    print(f'Return a list of {len(all_sentences)} sentences')\n",
    "    \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7703ca1a-62b2-4ab3-98a1-fd38aad080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfeece3e-eb04-4b8b-a6df-23d80150d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.02\n"
     ]
    }
   ],
   "source": [
    "#results = load_data_from_disk(os.path.join(vm_data, ))\n",
    "start = time.perf_counter()\n",
    "results = load_data_seq_512(os.path.join(vm_data, 'english_docs_aa.txt'))\n",
    "end = time.perf_counter() - start\n",
    "print(round(end, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43a0372d-189a-44e1-a1aa-7ebcc6f86f90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2623/2591398107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29b03e9a-ca9a-42fb-a0d4-cf18c39fdbd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# segments = []\n",
    "# temp_holder = []\n",
    "# def create_512_sequences(\n",
    "# for sentence in tqdm(results, 'Tokenizing'):\n",
    "#     tokens = alternative_tokenizer.encode(sentence)\n",
    "#     if len(temp_holder) + len(tokens) < 512:\n",
    "#         temp_holder.extend(tokens[1:-1])\n",
    "#     else:\n",
    "#         temp_holder.insert(0,2)\n",
    "#         segments.append(temp_holder + [3])\n",
    "#         temp_holder = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4bc6f-5c8f-4d24-b4ca-c9d2b7b9f73f",
   "metadata": {},
   "source": [
    "#### Batch encode a chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5fc29f8-e7aa-42bc-ba2b-79aad83462f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.23 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(results)\n",
    "e = time.perf_counter() - s\n",
    "print(round(e,2), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "352e2d26-4384-401d-a130-53b14af0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrease load on memory\n",
    "del results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568788a-5ffe-4376-a72e-1172141d8c00",
   "metadata": {},
   "source": [
    "#### Create pipeline for random masking of 15% of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9386eae-bc2e-4edb-b158-28e3a71748cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2c4dd28-12ac-41d2-a2e0-ebc2bd68a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6df1bac2d54452a1fd6ceaf90e4cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d386cd035bfe4137b88c9010c02a1871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attention Mask:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af00f9ced964bc8b3882c6dfddd6d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking Words:   0%|          | 0/98862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodings = mlm_pipe(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6c370c7-4cc3-4fbe-a331-37fdd4acc05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1490)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(encodings['input_ids'] == 4)) / sum(sum(encodings['labels'] != 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2baae0cd-f767-438c-87fd-aa2b7da3c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {key : tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "745683a2-fb73-4805-a5bd-1cee5bcf47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(encodings)\n",
    "del batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73a1ed71-47e7-43b2-9d3a-63985fce024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4944"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(d, batch_size=20, pin_memory=True, shuffle=True)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d79ee7a9-0c6c-4fcc-b6ea-1831cc4147b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=30500,num_hidden_layers=12)\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7775707d-efdc-4564-ab09-20dae9d08b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' \n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c02fa006-3f28-47f1-8b47-017aac7de024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f596e014-9b05-4e8f-9e7e-f8c734b5d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 25 17:18:29 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    39W / 250W |   1983MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    39W / 250W |   1947MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     1979MiB |\n",
      "|    1   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     1943MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f41bbc9a-1282-42b2-a1ee-cc729427cc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_counts = []\n",
    "\n",
    "def early_stopping(total_loss, tol: float = 0.01, early_stopping: int=10):\n",
    "    if total_loss < lowest_loss:\n",
    "        lowest_loss = total_loss\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8438e07-6676-44f1-ba7a-f1b4994b3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def run():\n",
    "    num_batches = len(loader)\n",
    "    epochs = 3\n",
    "    step = 0\n",
    "    lowest_loss = 10000\n",
    "    tolerance = 0.01\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # for file in files[:2]:\n",
    "        #     # setup loop with TQDM and dataloader\n",
    "        #     results = load_data_from_disk(os.path.join(vm_data, file))\n",
    "        #     batch = tokenizer.encode_batch(results)\n",
    "        #     del results\n",
    "        #     encodings = mlm_pipe(batch)\n",
    "        #     del batch\n",
    "        #     d = Dataset(encodings)\n",
    "        #     del encodings\n",
    "        #     loader = torch.utils.data.DataLoader(d, batch_size=384, pin_memory=True, shuffle=True)\n",
    "\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            step += 1\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optimizer.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.sum().backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # print relevant info to progress barI \n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            if loss.sum() < lowest_loss:\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                print(f'Loss: {loss.sum()}')\n",
    "                model.module.save_pretrained(f'checkpoints/test_save_pretrained/model-trained-{step}.pt') \n",
    "            \n",
    "        #loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b59c3cc-f918-4c0f-9ac9-33c1f2ed41c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91fcd68b37b4be68731e8b107bcc6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1615476608276367\n",
      "Loss: 2.114736557006836\n",
      "Loss: 2.025595188140869\n",
      "Loss: 2.143763780593872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56622affe9b34b65a3144082d0e7f360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9414606094360352\n",
      "Loss: 1.926030158996582\n",
      "Loss: 1.8185341358184814\n",
      "Loss: 1.5966253280639648\n",
      "Loss: 1.2994054555892944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0c7889cf5d4a0abec62600e55fa363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.251365303993225\n",
      "Loss: 1.2248347997665405\n",
      "Loss: 1.126413106918335\n",
      "Loss: 1.0757315158843994\n",
      "Loss: 0.9106765985488892\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99dcc579-7540-44e6-8456-fea1fd3783d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 25 20:00:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   30C    P0    35W / 250W |  15641MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    35W / 250W |  13213MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python    15637MiB |\n",
      "|    1   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python    13209MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47a605de-2c66-480b-9b47-64d9916a4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "281c654a-ec72-4bf0-9525-50d2aee5d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 25 20:01:03 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    39W / 250W |   3701MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000002:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    39W / 250W |   1519MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     3697MiB |\n",
      "|    1   N/A  N/A      2623      C   ...s/py38_pytorch/bin/python     1515MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af052351-64b6-43bf-abdb-adc84d695e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = alternative_tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce768ed3-ecf3-4794-b123-3b7fb5ff5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(text: str):\n",
    "    config = BertConfig(vocab_size=30500)\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    untrained_pipe = pipeline('fill-mask', model=model, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n",
    "    utresult = untrained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Untrained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in utresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "        \n",
    "    lm = BertForMaskedLM.from_pretrained('checkpoints/test_save_pretrained/model-trained-14000.pt/')\n",
    "    trained_pipe = pipeline('fill-mask', model=lm, tokenizer=alternative_tokenizer)\n",
    "\n",
    "    tresult = trained_pipe(text)\n",
    "    \n",
    "    print()\n",
    "    print(\"Trained Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in tresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6751054-f4ad-4f98-8d83-f42ef5bde42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9714ba88104eedb3114fc8c4670790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009145cbbbec4a17a852b07d18310a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430fbdbed1b244b884c2316b9b49aeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4accefdbe35247e1b78f3084db967bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Untrained Results\n",
      "******************************************************************************************************************************************************\n",
      "it is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.7136348485946655\n",
      "insulin is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.014016539789736271\n",
      "amp is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.01275827456265688\n",
      "cox is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.012082782573997974\n",
      "notch is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.01053623203188181\n",
      "\n",
      "Trained Results\n",
      "******************************************************************************************************************************************************\n",
      "it is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.23749612271785736\n",
      "there is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.11453662812709808\n",
      "acetylcholine is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.030285345390439034\n",
      "dopamine is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.029437104240059853\n",
      "neurotransmitter is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves. 0.025556039065122604\n"
     ]
    }
   ],
   "source": [
    "show_results(f'{mask} is a cholinergic enzyme primarily found at postsynaptic neuromuscular junctions, especially in muscles and nerves.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7df956ec-5c19-46af-b9e5-ba9cf169a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/subsets/xaasplit_25K') as f:\n",
    "    data = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66048ed-aa14-4d6b-ab36-c305184cf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.encode_batch(['Introduction Under normal physiological conditions, all cells in the body are exposed chronically to oxidants from both endogenous and exogenous sources;', 'This is another sentence.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fe76d4-9eeb-423b-a152-86b7e3fce8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([x.ids for x in test])\n",
    "mask = torch.tensor([x.attention_mask for x in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e74e514-2e72-4821-b9d9-9e76318a1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = new_model(input_ids, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a91f911-d6b9-4725-9b5e-eedd8b85c595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 30500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "784c568d-efb4-4232-ad1f-4f7bb5c16262",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbut = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e966560-6332-4b83-a87a-c81cf9b2c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_pretrained('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524614ae-7c88-44e9-9e6e-46e33c03141e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=30500, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97e0596f-c6d3-4e34-a44b-7a2d17b493de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30500, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(514, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30500, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26f383e-e1f9-40b5-a139-ccba0ea413c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "test = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db2b895-924d-4bff-88d9-2ded744aadde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ffd1e38-30cc-4052-aa32-87df4b346aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertForMaskedLM, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63018eb5-2b6c-4589-a2b0-78d2fdaba938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87b671c-62fe-4a4c-8377-f5aeee357f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed_pretrain_bert import masking_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c641b0-6a23-407d-938b-ca7113bb53f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  2420,\n",
       "  8747,\n",
       "  1641,\n",
       "  1634,\n",
       "  2065,\n",
       "  14042,\n",
       "  1737,\n",
       "  22511,\n",
       "  17,\n",
       "  4,\n",
       "  17,\n",
       "  4,\n",
       "  4,\n",
       "  1696,\n",
       "  1851,\n",
       "  1897,\n",
       "  2290,\n",
       "  3548,\n",
       "  3949,\n",
       "  4,\n",
       "  1831,\n",
       "  4,\n",
       "  1633,\n",
       "  27604,\n",
       "  30,\n",
       "  37,\n",
       "  4,\n",
       "  1811,\n",
       "  4,\n",
       "  18,\n",
       "  6791,\n",
       "  41,\n",
       "  3259,\n",
       "  18,\n",
       "  2976,\n",
       "  11,\n",
       "  4,\n",
       "  11,\n",
       "  1811,\n",
       "  4,\n",
       "  18,\n",
       "  26897,\n",
       "  41,\n",
       "  3259,\n",
       "  18,\n",
       "  5150,\n",
       "  11,\n",
       "  39,\n",
       "  17,\n",
       "  1784,\n",
       "  1687,\n",
       "  4178,\n",
       "  2415,\n",
       "  5993,\n",
       "  1740,\n",
       "  3548,\n",
       "  3949,\n",
       "  4,\n",
       "  1831,\n",
       "  3253,\n",
       "  1633,\n",
       "  27604,\n",
       "  1746,\n",
       "  1634,\n",
       "  14042,\n",
       "  1641,\n",
       "  43,\n",
       "  2065,\n",
       "  6751,\n",
       "  1702,\n",
       "  4,\n",
       "  3792,\n",
       "  1726,\n",
       "  1677,\n",
       "  3609,\n",
       "  8452,\n",
       "  12,\n",
       "  47,\n",
       "  18,\n",
       "  49,\n",
       "  18,\n",
       "  3548,\n",
       "  3949,\n",
       "  43,\n",
       "  22511,\n",
       "  1769,\n",
       "  19560,\n",
       "  6477,\n",
       "  4,\n",
       "  1654,\n",
       "  16904,\n",
       "  2728,\n",
       "  2065,\n",
       "  1746,\n",
       "  43,\n",
       "  22511,\n",
       "  1769,\n",
       "  3241,\n",
       "  16302,\n",
       "  3965,\n",
       "  2065,\n",
       "  13,\n",
       "  18,\n",
       "  17,\n",
       "  1784,\n",
       "  1687,\n",
       "  1851,\n",
       "  4178,\n",
       "  4,\n",
       "  5993,\n",
       "  1740,\n",
       "  3548,\n",
       "  4,\n",
       "  22511,\n",
       "  1831,\n",
       "  3253,\n",
       "  1633,\n",
       "  27604,\n",
       "  4,\n",
       "  1634,\n",
       "  14042,\n",
       "  1641,\n",
       "  43,\n",
       "  2065,\n",
       "  4,\n",
       "  5993,\n",
       "  6547,\n",
       "  1656,\n",
       "  1710,\n",
       "  14045,\n",
       "  6794,\n",
       "  12,\n",
       "  3762,\n",
       "  3949,\n",
       "  43,\n",
       "  22511,\n",
       "  4,\n",
       "  19560,\n",
       "  1879,\n",
       "  15790,\n",
       "  4,\n",
       "  16904,\n",
       "  2728,\n",
       "  2065,\n",
       "  1746,\n",
       "  43,\n",
       "  22511,\n",
       "  1769,\n",
       "  19560,\n",
       "  1879,\n",
       "  15790,\n",
       "  1654,\n",
       "  16904,\n",
       "  2728,\n",
       "  2065,\n",
       "  13,\n",
       "  18,\n",
       "  None],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4893,\n",
       "  0,\n",
       "  5319,\n",
       "  1877,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  22511,\n",
       "  0,\n",
       "  3253,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  1018,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  16,\n",
       "  0,\n",
       "  0,\n",
       "  1018,\n",
       "  0,\n",
       "  6791,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  22511,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3609,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  49,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1879,\n",
       "  15790,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2415,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3949,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1746,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1732,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3548,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1769,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1654,\n",
       "  16904,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking_function(text=\"\"\"\n",
    "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
    "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "\"\"\", \n",
    "                 tokenizer=alternative_tokenizer, \n",
    "                 mask_prob=0.15, \n",
    "                 random_replace_prob=0.1, \n",
    "                 unmask_replace_prob=0.1,\n",
    "                 max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f294b0b-5502-49a9-a8df-f9fdd1cd1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_identifier(length: int = 8) -> str:\n",
    "    \"\"\"Create a unique identifier by choosing `length`\n",
    "    random characters from list of ascii characters and numbers\n",
    "    \"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    uuid = \"\".join(alphabet[ix] for ix in np.random.choice(len(alphabet), length))\n",
    "    return uuid\n",
    "\n",
    "def create_experiment_dir(\n",
    "        checkpoint_dir: pathlib.Path, all_arguments: Dict\n",
    ") -> pathlib.Path:\n",
    "    \"\"\" Create an experiment directory and save all arguments in it.\"\"\"\n",
    "    current_time = datetime.datetime.now(pytz.timezone(\"US/Pacific\"))\n",
    "    expname = f\"bert_pretrain.{current_time.year}.{current_time.month}.{current_time.day}.{current_time.hour}.{current_time.minute}.{current_time.second}.{get_unique_identifier()}\"\n",
    "    exp_dir = checkpoint_dir / expname\n",
    "    exp_dir.mkdir(exist_ok=False)\n",
    "    hparams_file = exp_dir / \"hparams.json\"\n",
    "    with hparams_file.open(\"w\") as handle:\n",
    "        json.dump(obj=all_arguments, fp=handle, indent=2)\n",
    "\n",
    "    # Create the Tensorboard Dir\n",
    "    tb_dir = exp_dir / \"tb_dir\"\n",
    "    tb_dir.mkdir()\n",
    "    return exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1b2784-6a71-4d83-a7f3-a0be367e1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\"train_file\": '',\n",
    "        \"validation_file\": \"\",\n",
    "        \"mask_prob\": 0.15,\n",
    "        \"epoch\": 10,\n",
    "        \"batch_size\":32,\n",
    "        \"checkpoint_every\":1000,\n",
    "        \"learning_rate\":1e-5,\n",
    "        \"weight_decay\":0.001,\n",
    "        \"gradient_accumulation_steps\":1,\n",
    "        \"lr_scheduler_type\":'linear',\n",
    "        \"num_warmup_steps\":1000,\n",
    "        \"seed\":42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38675c4b-b6ba-4469-b5e9-cd7f11714bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hparams.json', 'w') as f:\n",
    "    f.write(json.dumps(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce778d-58a8-4ae9-9bec-840bbe091017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "\n",
    "def save_model(path: './', multiple_gpu: bool=True):\n",
    "    if multiple_gpu:\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "              f'{path}model_{step}.pt')\n",
    "    else:\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "              f'{path}model_{step}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
