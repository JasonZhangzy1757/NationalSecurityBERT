{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddd7e49-acd7-4e3a-9554-456c801f0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import os, time, json, datetime, pytz\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union, Dict\n",
    "from re_sent_splitter import split_into_sentences\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from multiprocessing import Pool\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "#distributed imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, DataParallel\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "import deepspeed\n",
    "\n",
    "#tokenizers and datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "\n",
    "#transformer imports\n",
    "from transformers import BertTokenizer, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling\n",
    "from transformers import BertModel,BertForMaskedLM, BertConfig, AdamW, TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b6ed0c-a1fc-4bf3-b97c-9acfadf7b7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt', strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66e2ef3-ce6d-4a84-9e2c-8d0653b072b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrainedBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('../Modeling/checkpoints/test_save_pretrained/model-trained-14000.pt/')\n",
    "        self.out = nn.Linear(768, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93bb56-2862-429e-8ac6-fad7e151ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.out = nn.Linear(768, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self, question, context, targets, tokenizer, max_len):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        question= str(self.question[item])\n",
    "        context = str(self.context[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        padding_len = self.max_len - len(ids)\n",
    "        \n",
    "        ids = ids[:self.max_len] + ([0] * padding_len) \n",
    "        token_type_ids = token_type_ids[:self.max_len] + ([0] * padding_len)\n",
    "        mask = mask[:self.max_len] + ([0] * padding_len)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        targets = d['targets']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 50 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            \n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "          ids = d['ids'].to(device, dtype=torch.long)\n",
    "          mask = d['mask'].to(device, dtype=torch.long)\n",
    "          token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "          targets = d['targets'].to(device, dtype=torch.long)\n",
    "        \n",
    "          outputs = model(ids, mask, token_type_ids)\n",
    "          #loss = loss_fn(outputs, targets)\n",
    "        \n",
    "          fin_targets.append(targets.cpu().detach().numpy())\n",
    "          fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.hstack(fin_targets)\n",
    "\n",
    "\n",
    "def run():\n",
    "    MAX_LEN = 512\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    EPOCHS = 4\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "    df_train = pd.read_json('./data/pqal_fold0/train_set.json', orient='index')\n",
    "    df_valid = pd.read_json('./data/pqal_fold0/dev_set.json', orient='index')\n",
    "    \n",
    "\n",
    "    target_cols = 'final_decision'\n",
    "    train_targets_str = df_train[target_cols].values\n",
    "    valid_targets_str = df_valid[target_cols].values\n",
    "    \n",
    "    # Transformer the target col to numeric values\n",
    "    target_dict = {'yes': 0, 'maybe': 1, 'no': 2}\n",
    "    train_targets = [target_dict[key] for key in train_targets_str]\n",
    "    valid_targets = [target_dict[key] for key in valid_targets_str]\n",
    "    \n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = TrainedBERT()\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = BERTDatasetTraining(\n",
    "        question=df_train.QUESTION.values,\n",
    "        context=df_train.CONTEXTS.values,\n",
    "        targets=train_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    valid_dataset = BERTDatasetTraining(\n",
    "        question=df_valid.QUESTION.values,\n",
    "        context=df_valid.CONTEXTS.values,\n",
    "        targets=valid_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        output, target = eval_loop_fn(valid_data_loader, model, device)\n",
    "        acc = (output.argmax(1) == target).sum() / len(target)\n",
    "        print(f'epoch: {epoch}, acc: {acc}')\n",
    "    \n",
    "    # Inference\n",
    "    TEST_BATCH_SIZE = 16\n",
    "    df_test = pd.read_json('./drive/MyDrive/qa_finetuning/data/test_set.json', orient='index')\n",
    "    test_targets_str = df_test[target_cols].values\n",
    "    test_targets = [target_dict[key] for key in test_targets_str]\n",
    "\n",
    "\n",
    "    test_dataset = BERTDatasetTraining(\n",
    "        question=df_test.QUESTION.values,\n",
    "        context=df_test.CONTEXTS.values,\n",
    "        targets=test_targets, # This is not used\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TEST_BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "\n",
    "    for bi, d in enumerate(test_data_loader):\n",
    "        ids = d[\"ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids).cpu().detach().numpy()\n",
    "            predictions.append(outputs.argmax(1))\n",
    "    pmids = df_test.index\n",
    "\n",
    "    return pmids, np.hstack(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
