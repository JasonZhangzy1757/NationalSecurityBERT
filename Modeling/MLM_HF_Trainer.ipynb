{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6983bf1-b4e8-4f3d-a4f9-c599def83bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertForMaskedLM, BertConfig, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "import tokenizers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from typing import List\n",
    "import deepspeed\n",
    "import loguru\n",
    "import nvgpu\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47dab9c0-ba7c-4e75-bc28-3b04e0babf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 01:22:30.348 | INFO     | __main__:<module>:4 - {'index': '0', 'type': 'Tesla V100-PCIE-16GB', 'uuid': 'GPU-a977becb-feef-b927-7929-3a141db5315d', 'mem_used': 4, 'mem_total': 16384, 'mem_used_percent': 0.0244140625}\n",
      "2022-03-30 01:22:30.349 | INFO     | __main__:<module>:4 - {'index': '1', 'type': 'Tesla V100-PCIE-16GB', 'uuid': 'GPU-90baff9b-3e43-3db5-b771-b26da4945020', 'mem_used': 4, 'mem_total': 16384, 'mem_used_percent': 0.0244140625}\n",
      "2022-03-30 01:22:30.351 | INFO     | __main__:<module>:16 - ['xac']\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logger = loguru.logger\n",
    "\n",
    "for gpu in nvgpu.gpu_info():\n",
    "    logger.info(gpu)\n",
    "    \n",
    "local_rank = 0\n",
    "device = (\n",
    "        torch.device(\"cuda\", local_rank)\n",
    "        if (local_rank > -1) and torch.cuda.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "tokenizer_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "text_data_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Data/'\n",
    "\n",
    "files = [f for f in os.listdir(text_data_path) if os.path.isfile(os.path.join(text_data_path, f))]\n",
    "logger.info(f'{files}')\n",
    "\n",
    "# #### Instantiate pretrained tokenizer from file\n",
    "alternative_tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(tokenizer_path, strip_accents=True, lowercase=True)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8b350f6-c92e-4a8d-b684-46add3ae81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_seq_512(path: str, sample_size:int=None) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        if sample_size:\n",
    "            lines = [line.strip() for line in f.readlines()[:sample_size]]\n",
    "        else:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def mlm_pipe(batch: List[tokenizers.Encoding], mlm_prob=0.15) -> dict:\n",
    "    '''\n",
    "    Given a single instance from a batch of encodings, return masked inputs and associated arrays.\n",
    "    Converts tokenizer.Encoding into a pytorch tensor.\n",
    "    '''\n",
    "    \n",
    "    labels = torch.tensor([x.ids for x in tqdm(batch, 'Labels')])\n",
    "    mask = torch.tensor([x.attention_mask for x in tqdm(batch, 'Attention Mask')])\n",
    "    input_ids = labels.detach().clone()\n",
    "    \n",
    "    #default masking prob = 15%, don't mask special tokens \n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mlm_prob) * (input_ids > 4)\n",
    "    for i in tqdm(range(input_ids.shape[0]), 'Masking Words'):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        input_ids[i, selection] = 4\n",
    "        \n",
    "    # temp = input_ids.flatten()\n",
    "    # percent = sum(temp == 4)/sum(labels.flatten() != 4)\n",
    "    # print(percent)\n",
    "    encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dfe1677-ccda-4db7-9609-0a748f16352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 01:22:31.705 | INFO     | __main__:<module>:1 - Loading data from disk into memory...\n",
      "2022-03-30 01:22:31.957 | INFO     | __main__:<module>:5 - Loading completed: 0.25 seconds to load 5000 lines/documents.\n",
      "2022-03-30 01:22:31.958 | INFO     | __main__:<module>:7 - Batch encoding data...\n",
      "2022-03-30 01:22:35.098 | INFO     | __main__:<module>:11 - Batch encoding completed, took 3.14 seconds to complete\n",
      "2022-03-30 01:22:35.103 | INFO     | __main__:<module>:15 - Masking tokens\n",
      "Labels: 100%|██████████| 5000/5000 [00:00<00:00, 38148.97it/s]\n",
      "Attention Mask: 100%|██████████| 5000/5000 [00:00<00:00, 125266.67it/s]\n",
      "Masking Words: 100%|██████████| 5000/5000 [00:00<00:00, 30068.42it/s]\n",
      "2022-03-30 01:22:35.740 | INFO     | __main__:<module>:17 - Masking tokens completed\n",
      "2022-03-30 01:22:36.300 | INFO     | __main__:<module>:23 - Total of 14.92% of tokens are masked.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading data from disk into memory...')\n",
    "start = time.perf_counter()\n",
    "results = load_data_seq_512(os.path.join(text_data_path, files[0]))\n",
    "end = time.perf_counter() - start\n",
    "logger.info(f'Loading completed: {round(end, 2)} seconds to load {len(results)} lines/documents.')\n",
    "\n",
    "logger.info('Batch encoding data...')\n",
    "s = time.perf_counter()\n",
    "batch = tokenizer.encode_batch(results)\n",
    "e = time.perf_counter() - s\n",
    "logger.info(f'Batch encoding completed, took {round(e,2)} seconds to complete')\n",
    "\n",
    "del results\n",
    "\n",
    "logger.info('Masking tokens')\n",
    "encodings = mlm_pipe(batch)\n",
    "logger.info('Masking tokens completed')\n",
    "#encodings = torch.load(encodings_data_path)\n",
    "\n",
    "del batch\n",
    "\n",
    "percent = sum(sum(encodings['input_ids'].detach().numpy() == 4)) / sum(sum(encodings['labels'].detach().numpy() != 4))\n",
    "logger.info(f'Total of {round(percent * 100,2)}% of tokens are masked.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eeba1488-35ae-48b3-8083-0ecb9c5432ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {key : tensor[i] for key, tensor in self.encodings.items()}\n",
    "\n",
    "\n",
    "d = Dataset(encodings)\n",
    "BATCH_SIZE = 16\n",
    "loader = torch.utils.data.DataLoader(d, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c227fed7-08c0-4eeb-97a0-b8953148ba03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fp16': {'enabled': 'auto',\n",
       "  'loss_scale': 0,\n",
       "  'loss_scale_window': 1000,\n",
       "  'initial_scale_power': 16,\n",
       "  'hysteresis': 2,\n",
       "  'min_loss_scale': 1},\n",
       " 'optimizer': {'type': 'AdamW',\n",
       "  'params': {'lr': 'auto',\n",
       "   'betas': 'auto',\n",
       "   'eps': 'auto',\n",
       "   'weight_decay': 'auto'}},\n",
       " 'scheduler': {'type': 'WarmupLR',\n",
       "  'params': {'warmup_min_lr': 'auto',\n",
       "   'warmup_max_lr': 'auto',\n",
       "   'warmup_num_steps': 'auto'}},\n",
       " 'zero_optimization': {'stage': 2,\n",
       "  'offload_optimizer': {'device': 'cpu', 'pin_memory': True},\n",
       "  'allgather_partitions': True,\n",
       "  'allgather_bucket_size': 200000000.0,\n",
       "  'overlap_comm': True,\n",
       "  'reduce_scatter': True,\n",
       "  'reduce_bucket_size': 200000000.0,\n",
       "  'contiguous_gradients': True},\n",
       " 'gradient_accumulation_steps': 'auto',\n",
       " 'gradient_clipping': 'auto',\n",
       " 'steps_per_print': 2000,\n",
       " 'train_batch_size': 'auto',\n",
       " 'train_micro_batch_size_per_gpu': 'auto',\n",
       " 'wall_clock_breakdown': False}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('zero2_config.json') as f:\n",
    "    ds_config = json.loads(f.read())\n",
    "ds_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b92bb1d-b45e-4431-bfe3-5ad71df5a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig(vocab_size=30500)\n",
    "model = BertForMaskedLM(config=bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68b234d7-e2bc-4654-8e4b-6b45a743f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-03-30 01:47:31,359] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--------------------------------------------------------------------------\n",
      "[[54709,1],0]: A high-performance Open MPI point-to-point messaging module\n",
      "was unable to find any relevant network interfaces:\n",
      "\n",
      "Module: OpenFabrics (openib)\n",
      "  Host: bert-pretraining-vm\n",
      "\n",
      "Another transport will be used instead, although this may result in\n",
      "lower performance.\n",
      "\n",
      "NOTE: You can disable this warning by setting the MCA parameter\n",
      "btl_base_warn_component_unused to 0.\n",
      "--------------------------------------------------------------------------\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2022-03-30 01:47:33,045] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.2.0.5, master_port=29500\n",
      "[2022-03-30 01:47:33,046] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/\",\n",
    "    do_train=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=36,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    seed=0,\n",
    "    local_rank=0,\n",
    "    deepspeed=ds_config,\n",
    "    dataloader_num_workers=os.cpu_count(),\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=d,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "483f19d8-1d46-4185-b5b8-f07ab89c5a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-03-30 01:48:05,666] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.0, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18155/607914459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m             deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m             )\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/deepspeed.py\u001b[0m in \u001b[0;36mdeepspeed_init\u001b[0;34m(trainer, num_training_steps, resume_from_checkpoint, inference)\u001b[0m\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mdeepspeed_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# stash kwargs to enabled a later deepspeed_reinit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/deepspeed/__init__.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_params)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipelineModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         engine = DeepSpeedEngine(args=args,\n\u001b[0m\u001b[1;32m    120\u001b[0m                                  \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                  \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_params, dont_change_device)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# Configure distributed model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_distributed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36m_configure_distributed_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdont_change_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# MoE related initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train pre-trained model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3caa1b-2e27-45c9-b5da-7399ab653659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
