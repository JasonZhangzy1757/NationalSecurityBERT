{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839b9be1-2d87-41da-999b-340dd1ff462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForMaskedLM\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420bdb13-b4d5-4873-9376-fae5f190ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Preprocessing/Tokenization/wp-vocab-30500-vocab.txt'\n",
    "checkpoint_folder = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/run_4GB_Mar25_1000pm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87334ac5-3273-4d50-8ff0-72fee661e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "mask = alternative_tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91b98c85-fb79-43e1-a040-624916ef141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(text: str, masked_word: str, other_model='bert-base-uncased'):\n",
    "    config = BertConfig(vocab_size=30500)\n",
    "    model = BertForMaskedLM.from_pretrained(other_model)\n",
    "    untrained_pipe = pipeline('fill-mask', model=model, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n",
    "    utresult = untrained_pipe(text)\n",
    "    print()\n",
    "    print(f\"Original Text: ['MASK'] = {masked_word}\")\n",
    "    print(\"*\" * 150)\n",
    "    print(text)\n",
    "    print()\n",
    "    print(f\"{other_model} Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in utresult:\n",
    "        print(result['sequence'], result['score'])\n",
    "\n",
    "    lm = BertForMaskedLM.from_pretrained(os.path.join(checkpoint_folder, 'model-trained-31-112992.pt'))\n",
    "    trained_pipe = pipeline('fill-mask', model=lm, tokenizer=alternative_tokenizer)\n",
    "\n",
    "    tresult = trained_pipe(text)\n",
    "\n",
    "    print()\n",
    "    print(\"Trained Model Results\")\n",
    "    print(\"*\" * 150)\n",
    "    for result in tresult:\n",
    "        print(result['sequence'], result['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "351a772d-7c02-4359-88a5-35e942b5078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: ['MASK'] = multinomial\n",
      "******************************************************************************************************************************************************\n",
      "Given D documents containing T topics expressed over W unique words, we can represent P(w|z) with a set of T [MASK] distributions\n",
      "\n",
      "bert-base-uncased Results\n",
      "******************************************************************************************************************************************************\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t - distributions 0.46534988284111023\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of táµ¢ distributions 0.02118883840739727\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t unique distributions 0.017121976241469383\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t'distributions 0.009133614599704742\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t parameter distributions 0.008222663775086403\n",
      "\n",
      "Trained Model Results\n",
      "******************************************************************************************************************************************************\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t topic distributions 0.20734535157680511\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t flu distributions 0.10268118232488632\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t field distributions 0.08412953466176987\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t topics distributions 0.06150899827480316\n",
      "given d documents containing t topics expressed over w unique words, we can represent p ( w | z ) with a set of t belt distributions 0.025198839604854584\n"
     ]
    }
   ],
   "source": [
    "show_results(f'Given D documents containing T topics expressed over W unique words, we can represent P(w|z) with a set of T {mask} distributions', 'multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dcdad-493b-4c5d-9747-6c6d498a3af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
